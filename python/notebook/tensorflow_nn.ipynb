{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import contextlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "image_size = 28 * 28\n",
    "num_classes = 10\n",
    "assert mnist.train.images.shape[1] == image_size\n",
    "assert mnist.train.labels.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Logistic Regression\n",
    "References\n",
    "- [MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html)\n",
    "- [L2 Reguralization example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py)\n",
    "  - How is `tf.nn.l2_loss` different from square and reduce_sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RunLogsticRegression(learning_rate = 0.05,\n",
    "                         batch_size = 8,\n",
    "                         steps = 200 * 1000,\n",
    "                         sample = 1000,\n",
    "                         l2_regularization_strength = 0.0):\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    step_records = []\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "        weights = tf.Variable(tf.truncated_normal([image_size, num_classes], stddev=1), name='a')\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(inputs, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        \n",
    "        loss = cross_entropy\n",
    "        if l2_regularization_strength > 0:\n",
    "            regularizer = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "            loss += l2_regularization_strength * regularizer\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        init_variables = tf.global_variables_initializer()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label})\n",
    "            if step % sample == 0:\n",
    "                batch_entropy, batch_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: batch_input, labels: batch_label})\n",
    "                train_entropy, train_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: mnist.train.images, labels: mnist.train.labels})\n",
    "                validation_entropy, validation_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                                   {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "                print 'step: %d, batch loss: %f, train loss: %f, train accuracy: %.2f%%, validation loss: %f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if step > 10000:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 11.254118, train loss: 13.565357, train accuracy: 13.01%, validation loss: 13.613159, validation accuracy: 13.44%\n",
      "step: 1000, batch loss: 0.621442, train loss: 1.223060, train accuracy: 74.63%, validation loss: 1.185761, validation accuracy: 75.52%\n",
      "step: 2000, batch loss: 1.580356, train loss: 0.874641, train accuracy: 80.84%, validation loss: 0.845006, validation accuracy: 81.90%\n",
      "step: 3000, batch loss: 0.109550, train loss: 0.749208, train accuracy: 83.27%, validation loss: 0.725033, validation accuracy: 84.34%\n",
      "step: 4000, batch loss: 0.018759, train loss: 0.669638, train accuracy: 84.61%, validation loss: 0.648239, validation accuracy: 85.62%\n",
      "step: 5000, batch loss: 0.157296, train loss: 0.636471, train accuracy: 85.16%, validation loss: 0.627879, validation accuracy: 85.90%\n",
      "step: 6000, batch loss: 0.222711, train loss: 0.576403, train accuracy: 86.45%, validation loss: 0.564561, validation accuracy: 87.14%\n",
      "step: 7000, batch loss: 0.004111, train loss: 0.550616, train accuracy: 86.75%, validation loss: 0.537017, validation accuracy: 87.60%\n",
      "step: 8000, batch loss: 0.423387, train loss: 0.532951, train accuracy: 87.02%, validation loss: 0.523349, validation accuracy: 87.62%\n",
      "step: 9000, batch loss: 0.423324, train loss: 0.518161, train accuracy: 87.41%, validation loss: 0.513922, validation accuracy: 88.06%\n",
      "step: 10000, batch loss: 0.613856, train loss: 0.500118, train accuracy: 87.74%, validation loss: 0.490074, validation accuracy: 88.44%\n",
      "step: 11000, batch loss: 0.852913, train loss: 0.483562, train accuracy: 88.24%, validation loss: 0.480234, validation accuracy: 88.46%\n",
      "step: 12000, batch loss: 0.002141, train loss: 0.469207, train accuracy: 88.57%, validation loss: 0.463211, validation accuracy: 88.78%\n",
      "step: 13000, batch loss: 0.368328, train loss: 0.456349, train accuracy: 88.74%, validation loss: 0.444951, validation accuracy: 89.18%\n",
      "step: 14000, batch loss: 1.321756, train loss: 0.446184, train accuracy: 88.93%, validation loss: 0.441142, validation accuracy: 89.20%\n",
      "step: 15000, batch loss: 0.009084, train loss: 0.444720, train accuracy: 88.91%, validation loss: 0.431982, validation accuracy: 89.52%\n",
      "step: 16000, batch loss: 0.080867, train loss: 0.432835, train accuracy: 89.24%, validation loss: 0.424540, validation accuracy: 89.36%\n",
      "step: 17000, batch loss: 0.018990, train loss: 0.439560, train accuracy: 89.04%, validation loss: 0.437284, validation accuracy: 89.64%\n",
      "step: 18000, batch loss: 0.063630, train loss: 0.420008, train accuracy: 89.48%, validation loss: 0.413156, validation accuracy: 89.62%\n",
      "step: 19000, batch loss: 0.643436, train loss: 0.427216, train accuracy: 89.22%, validation loss: 0.423284, validation accuracy: 89.58%\n",
      "step: 20000, batch loss: 0.069524, train loss: 0.411177, train accuracy: 89.52%, validation loss: 0.403037, validation accuracy: 89.68%\n",
      "step: 21000, batch loss: 0.114521, train loss: 0.410872, train accuracy: 89.41%, validation loss: 0.409534, validation accuracy: 89.44%\n",
      "step: 22000, batch loss: 0.405678, train loss: 0.402867, train accuracy: 89.70%, validation loss: 0.402887, validation accuracy: 89.56%\n",
      "step: 23000, batch loss: 0.010366, train loss: 0.398408, train accuracy: 89.83%, validation loss: 0.400309, validation accuracy: 89.88%\n",
      "step: 24000, batch loss: 0.084034, train loss: 0.396696, train accuracy: 89.89%, validation loss: 0.392308, validation accuracy: 89.96%\n",
      "step: 25000, batch loss: 1.285297, train loss: 0.392155, train accuracy: 89.89%, validation loss: 0.387876, validation accuracy: 90.38%\n",
      "step: 26000, batch loss: 0.146015, train loss: 0.381373, train accuracy: 90.22%, validation loss: 0.374865, validation accuracy: 90.42%\n",
      "step: 27000, batch loss: 0.154232, train loss: 0.376062, train accuracy: 90.32%, validation loss: 0.374280, validation accuracy: 90.52%\n",
      "step: 28000, batch loss: 0.519642, train loss: 0.375627, train accuracy: 90.15%, validation loss: 0.372168, validation accuracy: 90.52%\n",
      "step: 29000, batch loss: 0.159237, train loss: 0.375037, train accuracy: 90.27%, validation loss: 0.374266, validation accuracy: 90.34%\n",
      "step: 30000, batch loss: 0.240512, train loss: 0.371218, train accuracy: 90.29%, validation loss: 0.377737, validation accuracy: 90.14%\n",
      "step: 31000, batch loss: 0.054334, train loss: 0.368177, train accuracy: 90.29%, validation loss: 0.369300, validation accuracy: 90.38%\n",
      "step: 32000, batch loss: 0.016584, train loss: 0.362691, train accuracy: 90.56%, validation loss: 0.358549, validation accuracy: 90.62%\n",
      "step: 33000, batch loss: 0.186438, train loss: 0.359998, train accuracy: 90.62%, validation loss: 0.356537, validation accuracy: 90.66%\n",
      "step: 34000, batch loss: 0.130186, train loss: 0.358056, train accuracy: 90.66%, validation loss: 0.366436, validation accuracy: 90.56%\n",
      "step: 35000, batch loss: 0.039503, train loss: 0.361197, train accuracy: 90.50%, validation loss: 0.362986, validation accuracy: 90.42%\n",
      "step: 36000, batch loss: 0.751788, train loss: 0.355432, train accuracy: 90.75%, validation loss: 0.355438, validation accuracy: 90.72%\n",
      "step: 37000, batch loss: 0.020824, train loss: 0.351816, train accuracy: 90.65%, validation loss: 0.353626, validation accuracy: 91.02%\n",
      "step: 38000, batch loss: 0.100878, train loss: 0.348846, train accuracy: 90.85%, validation loss: 0.351654, validation accuracy: 90.94%\n",
      "step: 39000, batch loss: 0.026633, train loss: 0.358792, train accuracy: 90.49%, validation loss: 0.365093, validation accuracy: 90.22%\n",
      "step: 40000, batch loss: 0.146799, train loss: 0.338961, train accuracy: 91.08%, validation loss: 0.345020, validation accuracy: 90.66%\n",
      "step: 41000, batch loss: 1.675161, train loss: 0.343182, train accuracy: 90.95%, validation loss: 0.348867, validation accuracy: 91.06%\n",
      "step: 42000, batch loss: 0.030978, train loss: 0.345145, train accuracy: 90.81%, validation loss: 0.348474, validation accuracy: 90.54%\n",
      "step: 43000, batch loss: 0.008320, train loss: 0.345481, train accuracy: 90.86%, validation loss: 0.352268, validation accuracy: 90.70%\n",
      "step: 44000, batch loss: 0.040485, train loss: 0.342754, train accuracy: 91.13%, validation loss: 0.345634, validation accuracy: 91.08%\n",
      "step: 45000, batch loss: 0.044173, train loss: 0.330583, train accuracy: 91.29%, validation loss: 0.338043, validation accuracy: 91.00%\n",
      "step: 46000, batch loss: 0.499466, train loss: 0.332496, train accuracy: 91.21%, validation loss: 0.338848, validation accuracy: 90.90%\n",
      "step: 47000, batch loss: 0.090761, train loss: 0.328065, train accuracy: 91.27%, validation loss: 0.335768, validation accuracy: 91.12%\n",
      "step: 48000, batch loss: 0.011678, train loss: 0.328218, train accuracy: 91.29%, validation loss: 0.329465, validation accuracy: 91.36%\n",
      "step: 49000, batch loss: 0.028808, train loss: 0.337550, train accuracy: 90.97%, validation loss: 0.342138, validation accuracy: 91.08%\n",
      "step: 50000, batch loss: 0.016884, train loss: 0.333078, train accuracy: 91.01%, validation loss: 0.341604, validation accuracy: 90.86%\n",
      "step: 51000, batch loss: 1.643863, train loss: 0.326563, train accuracy: 91.27%, validation loss: 0.332157, validation accuracy: 91.42%\n",
      "step: 52000, batch loss: 0.798115, train loss: 0.323953, train accuracy: 91.34%, validation loss: 0.332965, validation accuracy: 91.58%\n",
      "step: 53000, batch loss: 0.008590, train loss: 0.330337, train accuracy: 91.18%, validation loss: 0.333658, validation accuracy: 91.52%\n",
      "step: 54000, batch loss: 0.176532, train loss: 0.319609, train accuracy: 91.41%, validation loss: 0.324997, validation accuracy: 91.38%\n",
      "step: 55000, batch loss: 0.023098, train loss: 0.319721, train accuracy: 91.44%, validation loss: 0.329345, validation accuracy: 91.12%\n",
      "step: 56000, batch loss: 0.123465, train loss: 0.322748, train accuracy: 91.23%, validation loss: 0.334986, validation accuracy: 90.92%\n",
      "step: 57000, batch loss: 0.320318, train loss: 0.326293, train accuracy: 91.08%, validation loss: 0.336728, validation accuracy: 90.98%\n",
      "step: 58000, batch loss: 0.154654, train loss: 0.327475, train accuracy: 91.12%, validation loss: 0.342352, validation accuracy: 90.70%\n",
      "step: 59000, batch loss: 0.362497, train loss: 0.334052, train accuracy: 91.07%, validation loss: 0.349046, validation accuracy: 90.96%\n",
      "step: 60000, batch loss: 0.050968, train loss: 0.321285, train accuracy: 91.35%, validation loss: 0.331703, validation accuracy: 91.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61000, batch loss: 0.217509, train loss: 0.322566, train accuracy: 91.44%, validation loss: 0.327494, validation accuracy: 91.68%\n",
      "step: 62000, batch loss: 0.105746, train loss: 0.314012, train accuracy: 91.46%, validation loss: 0.318922, validation accuracy: 91.32%\n",
      "step: 63000, batch loss: 0.147563, train loss: 0.320096, train accuracy: 91.27%, validation loss: 0.328540, validation accuracy: 91.06%\n",
      "step: 64000, batch loss: 0.026468, train loss: 0.317391, train accuracy: 91.25%, validation loss: 0.327503, validation accuracy: 91.08%\n",
      "step: 65000, batch loss: 0.051114, train loss: 0.318475, train accuracy: 91.27%, validation loss: 0.328799, validation accuracy: 91.04%\n",
      "step: 66000, batch loss: 0.248636, train loss: 0.307029, train accuracy: 91.68%, validation loss: 0.320655, validation accuracy: 91.42%\n",
      "step: 67000, batch loss: 0.162854, train loss: 0.305333, train accuracy: 91.77%, validation loss: 0.317318, validation accuracy: 91.44%\n",
      "step: 68000, batch loss: 0.090153, train loss: 0.303444, train accuracy: 91.75%, validation loss: 0.315717, validation accuracy: 91.60%\n",
      "step: 69000, batch loss: 0.023887, train loss: 0.307522, train accuracy: 91.77%, validation loss: 0.316861, validation accuracy: 91.76%\n",
      "step: 70000, batch loss: 0.426064, train loss: 0.314194, train accuracy: 91.51%, validation loss: 0.322393, validation accuracy: 91.34%\n",
      "step: 71000, batch loss: 0.014548, train loss: 0.306287, train accuracy: 91.72%, validation loss: 0.320596, validation accuracy: 91.38%\n",
      "step: 72000, batch loss: 0.058486, train loss: 0.304767, train accuracy: 91.71%, validation loss: 0.318609, validation accuracy: 91.48%\n",
      "step: 73000, batch loss: 0.046208, train loss: 0.298623, train accuracy: 91.95%, validation loss: 0.313219, validation accuracy: 91.42%\n",
      "step: 74000, batch loss: 0.350881, train loss: 0.312879, train accuracy: 91.43%, validation loss: 0.326081, validation accuracy: 91.32%\n",
      "step: 75000, batch loss: 0.044131, train loss: 0.302977, train accuracy: 91.81%, validation loss: 0.321083, validation accuracy: 91.32%\n",
      "step: 76000, batch loss: 0.016686, train loss: 0.306273, train accuracy: 91.71%, validation loss: 0.319456, validation accuracy: 91.22%\n",
      "step: 77000, batch loss: 0.064739, train loss: 0.310401, train accuracy: 91.34%, validation loss: 0.325958, validation accuracy: 91.20%\n",
      "step: 78000, batch loss: 0.013180, train loss: 0.301929, train accuracy: 91.91%, validation loss: 0.316619, validation accuracy: 91.38%\n",
      "step: 79000, batch loss: 0.108323, train loss: 0.295845, train accuracy: 92.04%, validation loss: 0.311834, validation accuracy: 91.86%\n",
      "step: 80000, batch loss: 0.947336, train loss: 0.302699, train accuracy: 91.82%, validation loss: 0.316159, validation accuracy: 91.76%\n",
      "step: 81000, batch loss: 0.085233, train loss: 0.298665, train accuracy: 91.96%, validation loss: 0.308103, validation accuracy: 91.90%\n",
      "step: 82000, batch loss: 0.131972, train loss: 0.298575, train accuracy: 91.77%, validation loss: 0.311810, validation accuracy: 91.56%\n",
      "step: 83000, batch loss: 2.353569, train loss: 0.294746, train accuracy: 91.93%, validation loss: 0.306810, validation accuracy: 91.82%\n",
      "step: 84000, batch loss: 0.081554, train loss: 0.299004, train accuracy: 91.79%, validation loss: 0.309291, validation accuracy: 91.76%\n",
      "step: 85000, batch loss: 0.022787, train loss: 0.300610, train accuracy: 91.88%, validation loss: 0.316998, validation accuracy: 91.86%\n",
      "step: 86000, batch loss: 0.183876, train loss: 0.298279, train accuracy: 91.74%, validation loss: 0.317288, validation accuracy: 91.58%\n",
      "step: 87000, batch loss: 0.045696, train loss: 0.290632, train accuracy: 92.09%, validation loss: 0.303312, validation accuracy: 91.68%\n",
      "step: 88000, batch loss: 0.069758, train loss: 0.299778, train accuracy: 91.70%, validation loss: 0.314360, validation accuracy: 91.70%\n",
      "step: 89000, batch loss: 0.019544, train loss: 0.288769, train accuracy: 92.23%, validation loss: 0.306375, validation accuracy: 91.66%\n",
      "step: 90000, batch loss: 0.138276, train loss: 0.292819, train accuracy: 91.96%, validation loss: 0.307272, validation accuracy: 91.90%\n",
      "step: 91000, batch loss: 0.008583, train loss: 0.289125, train accuracy: 92.11%, validation loss: 0.304156, validation accuracy: 91.66%\n",
      "step: 92000, batch loss: 0.357634, train loss: 0.294000, train accuracy: 91.89%, validation loss: 0.309502, validation accuracy: 91.56%\n",
      "step: 93000, batch loss: 0.157925, train loss: 0.292234, train accuracy: 91.89%, validation loss: 0.309278, validation accuracy: 91.68%\n",
      "step: 94000, batch loss: 0.016521, train loss: 0.290393, train accuracy: 92.09%, validation loss: 0.310796, validation accuracy: 91.66%\n",
      "step: 95000, batch loss: 0.953262, train loss: 0.295164, train accuracy: 91.92%, validation loss: 0.314624, validation accuracy: 91.56%\n",
      "step: 96000, batch loss: 0.060199, train loss: 0.290489, train accuracy: 91.92%, validation loss: 0.308575, validation accuracy: 91.84%\n",
      "step: 97000, batch loss: 0.541561, train loss: 0.302871, train accuracy: 91.60%, validation loss: 0.317388, validation accuracy: 91.40%\n",
      "step: 98000, batch loss: 0.286386, train loss: 0.289353, train accuracy: 92.02%, validation loss: 0.310843, validation accuracy: 91.40%\n",
      "step: 99000, batch loss: 0.308223, train loss: 0.289803, train accuracy: 92.11%, validation loss: 0.305765, validation accuracy: 91.62%\n",
      "step: 100000, batch loss: 0.048724, train loss: 0.285688, train accuracy: 92.15%, validation loss: 0.302171, validation accuracy: 91.76%\n",
      "step: 101000, batch loss: 0.437871, train loss: 0.296670, train accuracy: 91.75%, validation loss: 0.320433, validation accuracy: 91.06%\n",
      "step: 102000, batch loss: 0.238312, train loss: 0.282622, train accuracy: 92.30%, validation loss: 0.301809, validation accuracy: 91.48%\n",
      "step: 103000, batch loss: 0.204194, train loss: 0.284517, train accuracy: 92.40%, validation loss: 0.305948, validation accuracy: 92.24%\n",
      "step: 104000, batch loss: 0.052346, train loss: 0.290175, train accuracy: 92.07%, validation loss: 0.311963, validation accuracy: 91.62%\n",
      "step: 105000, batch loss: 0.126326, train loss: 0.283243, train accuracy: 92.21%, validation loss: 0.301461, validation accuracy: 91.78%\n",
      "step: 106000, batch loss: 0.046517, train loss: 0.279609, train accuracy: 92.42%, validation loss: 0.301146, validation accuracy: 91.94%\n",
      "step: 107000, batch loss: 0.116553, train loss: 0.286254, train accuracy: 92.06%, validation loss: 0.308387, validation accuracy: 91.32%\n",
      "step: 108000, batch loss: 0.175653, train loss: 0.280210, train accuracy: 92.33%, validation loss: 0.301408, validation accuracy: 91.76%\n",
      "step: 109000, batch loss: 0.174715, train loss: 0.280832, train accuracy: 92.27%, validation loss: 0.299268, validation accuracy: 91.74%\n",
      "step: 110000, batch loss: 0.359040, train loss: 0.282032, train accuracy: 92.27%, validation loss: 0.302708, validation accuracy: 91.92%\n",
      "step: 111000, batch loss: 0.106743, train loss: 0.280394, train accuracy: 92.36%, validation loss: 0.305160, validation accuracy: 91.70%\n",
      "step: 112000, batch loss: 0.481700, train loss: 0.284992, train accuracy: 92.24%, validation loss: 0.304500, validation accuracy: 91.96%\n",
      "step: 113000, batch loss: 0.135945, train loss: 0.286159, train accuracy: 92.14%, validation loss: 0.310261, validation accuracy: 91.54%\n",
      "step: 114000, batch loss: 0.433520, train loss: 0.279023, train accuracy: 92.34%, validation loss: 0.302015, validation accuracy: 91.78%\n",
      "step: 115000, batch loss: 0.256500, train loss: 0.285528, train accuracy: 92.17%, validation loss: 0.305653, validation accuracy: 91.94%\n",
      "step: 116000, batch loss: 0.159339, train loss: 0.274456, train accuracy: 92.55%, validation loss: 0.296751, validation accuracy: 91.98%\n",
      "step: 117000, batch loss: 0.037473, train loss: 0.288652, train accuracy: 91.93%, validation loss: 0.312686, validation accuracy: 91.38%\n",
      "step: 118000, batch loss: 0.043185, train loss: 0.283249, train accuracy: 92.24%, validation loss: 0.302209, validation accuracy: 91.86%\n",
      "step: 119000, batch loss: 0.172567, train loss: 0.277869, train accuracy: 92.34%, validation loss: 0.300106, validation accuracy: 91.78%\n",
      "step: 120000, batch loss: 0.083923, train loss: 0.286363, train accuracy: 91.99%, validation loss: 0.308716, validation accuracy: 91.58%\n",
      "step: 121000, batch loss: 1.100904, train loss: 0.276572, train accuracy: 92.41%, validation loss: 0.299173, validation accuracy: 91.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 122000, batch loss: 0.138872, train loss: 0.275695, train accuracy: 92.46%, validation loss: 0.295081, validation accuracy: 92.32%\n",
      "step: 123000, batch loss: 0.047253, train loss: 0.287210, train accuracy: 92.03%, validation loss: 0.307942, validation accuracy: 91.58%\n",
      "step: 124000, batch loss: 0.080056, train loss: 0.272656, train accuracy: 92.55%, validation loss: 0.295963, validation accuracy: 92.06%\n",
      "step: 125000, batch loss: 0.054403, train loss: 0.274940, train accuracy: 92.55%, validation loss: 0.296570, validation accuracy: 92.22%\n",
      "step: 126000, batch loss: 0.263473, train loss: 0.279773, train accuracy: 92.32%, validation loss: 0.304697, validation accuracy: 91.66%\n",
      "step: 127000, batch loss: 1.726015, train loss: 0.281657, train accuracy: 92.30%, validation loss: 0.301307, validation accuracy: 91.88%\n",
      "step: 128000, batch loss: 0.461541, train loss: 0.279673, train accuracy: 92.22%, validation loss: 0.301757, validation accuracy: 91.88%\n",
      "step: 129000, batch loss: 0.523133, train loss: 0.271959, train accuracy: 92.59%, validation loss: 0.295078, validation accuracy: 91.90%\n",
      "step: 130000, batch loss: 0.006130, train loss: 0.274324, train accuracy: 92.35%, validation loss: 0.299118, validation accuracy: 91.68%\n",
      "step: 131000, batch loss: 0.096981, train loss: 0.271485, train accuracy: 92.46%, validation loss: 0.292630, validation accuracy: 92.12%\n",
      "step: 132000, batch loss: 0.072909, train loss: 0.272968, train accuracy: 92.46%, validation loss: 0.297216, validation accuracy: 91.82%\n",
      "step: 133000, batch loss: 0.015733, train loss: 0.270558, train accuracy: 92.65%, validation loss: 0.292890, validation accuracy: 92.06%\n",
      "step: 134000, batch loss: 0.168844, train loss: 0.272236, train accuracy: 92.47%, validation loss: 0.298272, validation accuracy: 91.96%\n",
      "step: 135000, batch loss: 0.433694, train loss: 0.281763, train accuracy: 92.21%, validation loss: 0.304823, validation accuracy: 91.44%\n",
      "step: 136000, batch loss: 0.658563, train loss: 0.271700, train accuracy: 92.45%, validation loss: 0.296093, validation accuracy: 91.90%\n",
      "step: 137000, batch loss: 0.037656, train loss: 0.280483, train accuracy: 92.18%, validation loss: 0.304967, validation accuracy: 91.56%\n",
      "step: 138000, batch loss: 0.312402, train loss: 0.276581, train accuracy: 92.33%, validation loss: 0.298626, validation accuracy: 92.10%\n",
      "step: 139000, batch loss: 0.062716, train loss: 0.271089, train accuracy: 92.64%, validation loss: 0.293843, validation accuracy: 92.08%\n",
      "step: 140000, batch loss: 0.129690, train loss: 0.270658, train accuracy: 92.54%, validation loss: 0.292013, validation accuracy: 92.12%\n",
      "step: 141000, batch loss: 0.002326, train loss: 0.273236, train accuracy: 92.43%, validation loss: 0.296652, validation accuracy: 91.76%\n",
      "step: 142000, batch loss: 0.105417, train loss: 0.270292, train accuracy: 92.56%, validation loss: 0.299488, validation accuracy: 91.72%\n",
      "step: 143000, batch loss: 0.751447, train loss: 0.265889, train accuracy: 92.73%, validation loss: 0.294208, validation accuracy: 92.04%\n",
      "step: 144000, batch loss: 0.136004, train loss: 0.277883, train accuracy: 92.33%, validation loss: 0.300016, validation accuracy: 91.86%\n",
      "step: 145000, batch loss: 0.026252, train loss: 0.272408, train accuracy: 92.40%, validation loss: 0.296879, validation accuracy: 91.94%\n",
      "step: 146000, batch loss: 0.394307, train loss: 0.268858, train accuracy: 92.60%, validation loss: 0.293362, validation accuracy: 92.24%\n",
      "step: 147000, batch loss: 0.042915, train loss: 0.266841, train accuracy: 92.61%, validation loss: 0.291528, validation accuracy: 92.00%\n",
      "step: 148000, batch loss: 0.019156, train loss: 0.263996, train accuracy: 92.76%, validation loss: 0.289856, validation accuracy: 91.96%\n",
      "step: 149000, batch loss: 0.070079, train loss: 0.273802, train accuracy: 92.32%, validation loss: 0.295327, validation accuracy: 92.10%\n",
      "step: 150000, batch loss: 0.559847, train loss: 0.269096, train accuracy: 92.61%, validation loss: 0.296959, validation accuracy: 92.04%\n",
      "step: 151000, batch loss: 0.151427, train loss: 0.269885, train accuracy: 92.60%, validation loss: 0.300780, validation accuracy: 91.66%\n",
      "step: 152000, batch loss: 0.183600, train loss: 0.267230, train accuracy: 92.71%, validation loss: 0.293157, validation accuracy: 92.04%\n",
      "step: 153000, batch loss: 0.055101, train loss: 0.270263, train accuracy: 92.59%, validation loss: 0.297653, validation accuracy: 91.84%\n",
      "step: 154000, batch loss: 0.045308, train loss: 0.269188, train accuracy: 92.62%, validation loss: 0.295056, validation accuracy: 92.00%\n",
      "step: 155000, batch loss: 0.006281, train loss: 0.270924, train accuracy: 92.48%, validation loss: 0.295991, validation accuracy: 92.10%\n",
      "step: 156000, batch loss: 0.038689, train loss: 0.263303, train accuracy: 92.70%, validation loss: 0.293436, validation accuracy: 92.10%\n",
      "step: 157000, batch loss: 0.852691, train loss: 0.266498, train accuracy: 92.56%, validation loss: 0.295349, validation accuracy: 92.28%\n",
      "step: 158000, batch loss: 0.396938, train loss: 0.269525, train accuracy: 92.53%, validation loss: 0.299392, validation accuracy: 91.84%\n",
      "step: 159000, batch loss: 0.170361, train loss: 0.275711, train accuracy: 92.26%, validation loss: 0.303031, validation accuracy: 91.60%\n",
      "step: 160000, batch loss: 0.120888, train loss: 0.265925, train accuracy: 92.72%, validation loss: 0.293669, validation accuracy: 92.20%\n",
      "step: 161000, batch loss: 0.044082, train loss: 0.268004, train accuracy: 92.58%, validation loss: 0.296050, validation accuracy: 91.96%\n",
      "step: 162000, batch loss: 0.049723, train loss: 0.263360, train accuracy: 92.72%, validation loss: 0.289571, validation accuracy: 92.12%\n",
      "step: 163000, batch loss: 0.047989, train loss: 0.265969, train accuracy: 92.67%, validation loss: 0.295775, validation accuracy: 92.24%\n",
      "step: 164000, batch loss: 0.056069, train loss: 0.269186, train accuracy: 92.44%, validation loss: 0.300502, validation accuracy: 91.80%\n",
      "step: 165000, batch loss: 0.006354, train loss: 0.280266, train accuracy: 91.99%, validation loss: 0.303870, validation accuracy: 91.66%\n",
      "step: 166000, batch loss: 0.027837, train loss: 0.265252, train accuracy: 92.67%, validation loss: 0.295108, validation accuracy: 91.80%\n",
      "step: 167000, batch loss: 0.328963, train loss: 0.261633, train accuracy: 92.83%, validation loss: 0.290915, validation accuracy: 91.96%\n",
      "step: 168000, batch loss: 0.210268, train loss: 0.262235, train accuracy: 92.81%, validation loss: 0.287597, validation accuracy: 92.24%\n",
      "step: 169000, batch loss: 0.684263, train loss: 0.265335, train accuracy: 92.69%, validation loss: 0.293389, validation accuracy: 92.02%\n",
      "step: 170000, batch loss: 0.048669, train loss: 0.264275, train accuracy: 92.70%, validation loss: 0.294291, validation accuracy: 92.00%\n",
      "step: 171000, batch loss: 0.008387, train loss: 0.259525, train accuracy: 92.81%, validation loss: 0.288222, validation accuracy: 92.00%\n",
      "step: 172000, batch loss: 0.037893, train loss: 0.259818, train accuracy: 92.80%, validation loss: 0.286473, validation accuracy: 92.32%\n",
      "step: 173000, batch loss: 0.008840, train loss: 0.272194, train accuracy: 92.39%, validation loss: 0.301977, validation accuracy: 91.82%\n",
      "step: 174000, batch loss: 2.256224, train loss: 0.272982, train accuracy: 92.29%, validation loss: 0.301275, validation accuracy: 92.06%\n",
      "step: 175000, batch loss: 0.015002, train loss: 0.259711, train accuracy: 92.84%, validation loss: 0.286640, validation accuracy: 92.28%\n",
      "step: 176000, batch loss: 0.052155, train loss: 0.261656, train accuracy: 92.77%, validation loss: 0.288617, validation accuracy: 92.14%\n",
      "step: 177000, batch loss: 0.034439, train loss: 0.259213, train accuracy: 92.91%, validation loss: 0.288377, validation accuracy: 92.18%\n",
      "step: 178000, batch loss: 0.709279, train loss: 0.264804, train accuracy: 92.62%, validation loss: 0.293463, validation accuracy: 92.18%\n",
      "step: 179000, batch loss: 0.214940, train loss: 0.263194, train accuracy: 92.68%, validation loss: 0.294849, validation accuracy: 92.04%\n",
      "step: 180000, batch loss: 0.030605, train loss: 0.260452, train accuracy: 92.80%, validation loss: 0.292554, validation accuracy: 91.96%\n",
      "step: 181000, batch loss: 0.020493, train loss: 0.263678, train accuracy: 92.85%, validation loss: 0.292165, validation accuracy: 92.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 182000, batch loss: 0.021156, train loss: 0.264701, train accuracy: 92.63%, validation loss: 0.290278, validation accuracy: 91.92%\n",
      "step: 183000, batch loss: 0.136540, train loss: 0.265004, train accuracy: 92.57%, validation loss: 0.293252, validation accuracy: 91.90%\n",
      "step: 184000, batch loss: 0.066584, train loss: 0.258710, train accuracy: 92.92%, validation loss: 0.287344, validation accuracy: 92.16%\n",
      "step: 185000, batch loss: 0.039148, train loss: 0.262344, train accuracy: 92.78%, validation loss: 0.290075, validation accuracy: 92.30%\n",
      "step: 186000, batch loss: 0.119558, train loss: 0.259057, train accuracy: 92.92%, validation loss: 0.291717, validation accuracy: 91.94%\n",
      "step: 187000, batch loss: 0.108678, train loss: 0.257457, train accuracy: 92.86%, validation loss: 0.287412, validation accuracy: 92.00%\n",
      "step: 188000, batch loss: 0.181435, train loss: 0.256343, train accuracy: 92.92%, validation loss: 0.285909, validation accuracy: 92.04%\n",
      "step: 189000, batch loss: 0.100014, train loss: 0.269798, train accuracy: 92.51%, validation loss: 0.298670, validation accuracy: 91.74%\n",
      "step: 190000, batch loss: 0.007949, train loss: 0.269211, train accuracy: 92.42%, validation loss: 0.300121, validation accuracy: 91.76%\n",
      "step: 191000, batch loss: 0.004127, train loss: 0.263619, train accuracy: 92.76%, validation loss: 0.289932, validation accuracy: 92.12%\n",
      "step: 192000, batch loss: 0.165136, train loss: 0.262944, train accuracy: 92.64%, validation loss: 0.295394, validation accuracy: 91.90%\n",
      "step: 193000, batch loss: 0.016481, train loss: 0.259562, train accuracy: 92.68%, validation loss: 0.289994, validation accuracy: 92.08%\n",
      "step: 194000, batch loss: 0.153388, train loss: 0.269052, train accuracy: 92.48%, validation loss: 0.294414, validation accuracy: 92.00%\n",
      "step: 195000, batch loss: 0.078405, train loss: 0.261407, train accuracy: 92.71%, validation loss: 0.295328, validation accuracy: 91.94%\n",
      "step: 196000, batch loss: 0.752663, train loss: 0.257892, train accuracy: 92.91%, validation loss: 0.290493, validation accuracy: 92.20%\n",
      "step: 197000, batch loss: 0.057265, train loss: 0.261761, train accuracy: 92.73%, validation loss: 0.294186, validation accuracy: 92.06%\n",
      "step: 198000, batch loss: 0.011576, train loss: 0.268673, train accuracy: 92.45%, validation loss: 0.294732, validation accuracy: 92.08%\n",
      "step: 199000, batch loss: 0.016384, train loss: 0.255700, train accuracy: 92.97%, validation loss: 0.286369, validation accuracy: 92.26%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdb92e8c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 9.173740, train loss: 14.272741, train accuracy: 11.18%, validation loss: 14.067716, validation accuracy: 10.54%\n",
      "step: 1000, batch loss: 3.364270, train loss: 1.952444, train accuracy: 60.69%, validation loss: 1.868599, validation accuracy: 61.32%\n",
      "step: 2000, batch loss: 1.346344, train loss: 1.289300, train accuracy: 72.43%, validation loss: 1.207721, validation accuracy: 73.74%\n",
      "step: 3000, batch loss: 0.458598, train loss: 1.045034, train accuracy: 76.94%, validation loss: 0.965616, validation accuracy: 78.64%\n",
      "step: 4000, batch loss: 1.749797, train loss: 0.922399, train accuracy: 79.38%, validation loss: 0.859973, validation accuracy: 80.58%\n",
      "step: 5000, batch loss: 1.592070, train loss: 0.838615, train accuracy: 81.20%, validation loss: 0.772437, validation accuracy: 82.74%\n",
      "step: 6000, batch loss: 0.648750, train loss: 0.787010, train accuracy: 82.23%, validation loss: 0.730018, validation accuracy: 83.34%\n",
      "step: 7000, batch loss: 0.355700, train loss: 0.738360, train accuracy: 83.04%, validation loss: 0.687989, validation accuracy: 84.50%\n",
      "step: 8000, batch loss: 0.058332, train loss: 0.707204, train accuracy: 83.61%, validation loss: 0.662699, validation accuracy: 84.66%\n",
      "step: 9000, batch loss: 1.218646, train loss: 0.672756, train accuracy: 84.47%, validation loss: 0.628471, validation accuracy: 85.86%\n",
      "step: 10000, batch loss: 0.155481, train loss: 0.649690, train accuracy: 84.85%, validation loss: 0.607878, validation accuracy: 86.24%\n",
      "step: 11000, batch loss: 1.626336, train loss: 0.624923, train accuracy: 85.37%, validation loss: 0.584020, validation accuracy: 86.32%\n",
      "step: 12000, batch loss: 0.582275, train loss: 0.606613, train accuracy: 85.64%, validation loss: 0.575202, validation accuracy: 86.16%\n",
      "step: 13000, batch loss: 0.482172, train loss: 0.593728, train accuracy: 85.95%, validation loss: 0.563486, validation accuracy: 86.70%\n",
      "step: 14000, batch loss: 0.132232, train loss: 0.573650, train accuracy: 86.29%, validation loss: 0.545557, validation accuracy: 86.96%\n",
      "step: 15000, batch loss: 1.006793, train loss: 0.564555, train accuracy: 86.49%, validation loss: 0.535233, validation accuracy: 87.00%\n",
      "step: 16000, batch loss: 1.038442, train loss: 0.548358, train accuracy: 86.80%, validation loss: 0.526519, validation accuracy: 86.96%\n",
      "step: 17000, batch loss: 0.007889, train loss: 0.537375, train accuracy: 87.04%, validation loss: 0.511414, validation accuracy: 87.38%\n",
      "step: 18000, batch loss: 1.638504, train loss: 0.529283, train accuracy: 87.23%, validation loss: 0.504596, validation accuracy: 87.64%\n",
      "step: 19000, batch loss: 0.055521, train loss: 0.517307, train accuracy: 87.41%, validation loss: 0.493811, validation accuracy: 87.94%\n",
      "step: 20000, batch loss: 0.051355, train loss: 0.514925, train accuracy: 87.41%, validation loss: 0.496994, validation accuracy: 87.80%\n",
      "step: 21000, batch loss: 0.015572, train loss: 0.502783, train accuracy: 87.73%, validation loss: 0.483240, validation accuracy: 88.00%\n",
      "step: 22000, batch loss: 0.152573, train loss: 0.503290, train accuracy: 87.57%, validation loss: 0.485756, validation accuracy: 87.76%\n",
      "step: 23000, batch loss: 0.166781, train loss: 0.493253, train accuracy: 87.87%, validation loss: 0.474410, validation accuracy: 87.90%\n",
      "step: 24000, batch loss: 0.442292, train loss: 0.482145, train accuracy: 88.10%, validation loss: 0.467613, validation accuracy: 88.34%\n",
      "step: 25000, batch loss: 0.066784, train loss: 0.476124, train accuracy: 88.15%, validation loss: 0.460821, validation accuracy: 88.50%\n",
      "step: 26000, batch loss: 0.066396, train loss: 0.474526, train accuracy: 88.15%, validation loss: 0.463857, validation accuracy: 88.16%\n",
      "step: 27000, batch loss: 0.004250, train loss: 0.466875, train accuracy: 88.35%, validation loss: 0.455971, validation accuracy: 88.56%\n",
      "step: 28000, batch loss: 0.763726, train loss: 0.463023, train accuracy: 88.59%, validation loss: 0.449947, validation accuracy: 88.86%\n",
      "step: 29000, batch loss: 0.986500, train loss: 0.455557, train accuracy: 88.63%, validation loss: 0.442891, validation accuracy: 88.68%\n",
      "step: 30000, batch loss: 0.621562, train loss: 0.451116, train accuracy: 88.68%, validation loss: 0.445070, validation accuracy: 88.62%\n",
      "step: 31000, batch loss: 0.050011, train loss: 0.448539, train accuracy: 88.73%, validation loss: 0.441530, validation accuracy: 88.84%\n",
      "step: 32000, batch loss: 1.333424, train loss: 0.443148, train accuracy: 88.83%, validation loss: 0.436945, validation accuracy: 89.14%\n",
      "step: 33000, batch loss: 0.533081, train loss: 0.437045, train accuracy: 88.94%, validation loss: 0.427529, validation accuracy: 89.36%\n",
      "step: 34000, batch loss: 0.124353, train loss: 0.438310, train accuracy: 88.94%, validation loss: 0.426776, validation accuracy: 89.20%\n",
      "step: 35000, batch loss: 0.399681, train loss: 0.429756, train accuracy: 89.23%, validation loss: 0.420979, validation accuracy: 89.40%\n",
      "step: 36000, batch loss: 0.120303, train loss: 0.428207, train accuracy: 89.29%, validation loss: 0.419327, validation accuracy: 89.32%\n",
      "step: 37000, batch loss: 0.018946, train loss: 0.432917, train accuracy: 88.99%, validation loss: 0.428236, validation accuracy: 88.98%\n",
      "step: 38000, batch loss: 0.161629, train loss: 0.419832, train accuracy: 89.27%, validation loss: 0.415766, validation accuracy: 89.56%\n",
      "step: 39000, batch loss: 0.220549, train loss: 0.419215, train accuracy: 89.30%, validation loss: 0.415150, validation accuracy: 89.36%\n",
      "step: 40000, batch loss: 0.004868, train loss: 0.413917, train accuracy: 89.44%, validation loss: 0.409761, validation accuracy: 89.54%\n",
      "step: 41000, batch loss: 0.030572, train loss: 0.414017, train accuracy: 89.40%, validation loss: 0.411259, validation accuracy: 89.62%\n",
      "step: 42000, batch loss: 1.023247, train loss: 0.415104, train accuracy: 89.40%, validation loss: 0.409356, validation accuracy: 89.44%\n",
      "step: 43000, batch loss: 0.209174, train loss: 0.409361, train accuracy: 89.52%, validation loss: 0.409160, validation accuracy: 89.38%\n",
      "step: 44000, batch loss: 0.652627, train loss: 0.408693, train accuracy: 89.43%, validation loss: 0.407807, validation accuracy: 89.66%\n",
      "step: 45000, batch loss: 0.111963, train loss: 0.404377, train accuracy: 89.61%, validation loss: 0.402667, validation accuracy: 89.56%\n",
      "step: 46000, batch loss: 0.074412, train loss: 0.398717, train accuracy: 89.70%, validation loss: 0.395290, validation accuracy: 89.84%\n",
      "step: 47000, batch loss: 0.542371, train loss: 0.395963, train accuracy: 89.75%, validation loss: 0.393783, validation accuracy: 89.96%\n",
      "step: 48000, batch loss: 0.622797, train loss: 0.394955, train accuracy: 89.83%, validation loss: 0.395422, validation accuracy: 89.90%\n",
      "step: 49000, batch loss: 1.454483, train loss: 0.395592, train accuracy: 89.81%, validation loss: 0.394655, validation accuracy: 89.96%\n",
      "step: 50000, batch loss: 0.060705, train loss: 0.391939, train accuracy: 89.81%, validation loss: 0.391564, validation accuracy: 90.02%\n",
      "step: 51000, batch loss: 0.064392, train loss: 0.390019, train accuracy: 89.94%, validation loss: 0.388459, validation accuracy: 90.06%\n",
      "step: 52000, batch loss: 0.428319, train loss: 0.388455, train accuracy: 89.94%, validation loss: 0.388967, validation accuracy: 89.94%\n",
      "step: 53000, batch loss: 0.065007, train loss: 0.385652, train accuracy: 90.00%, validation loss: 0.384285, validation accuracy: 89.92%\n",
      "step: 54000, batch loss: 0.186032, train loss: 0.382552, train accuracy: 90.14%, validation loss: 0.381716, validation accuracy: 90.14%\n",
      "step: 55000, batch loss: 0.710626, train loss: 0.383375, train accuracy: 90.01%, validation loss: 0.385570, validation accuracy: 89.92%\n",
      "step: 56000, batch loss: 0.007876, train loss: 0.378570, train accuracy: 90.23%, validation loss: 0.381708, validation accuracy: 90.26%\n",
      "step: 57000, batch loss: 0.179905, train loss: 0.379869, train accuracy: 90.11%, validation loss: 0.384564, validation accuracy: 90.18%\n",
      "step: 58000, batch loss: 0.039124, train loss: 0.378249, train accuracy: 90.12%, validation loss: 0.380745, validation accuracy: 90.10%\n",
      "step: 59000, batch loss: 0.566383, train loss: 0.375893, train accuracy: 90.21%, validation loss: 0.378978, validation accuracy: 90.24%\n",
      "step: 60000, batch loss: 0.046210, train loss: 0.372644, train accuracy: 90.32%, validation loss: 0.376150, validation accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61000, batch loss: 0.450734, train loss: 0.368737, train accuracy: 90.35%, validation loss: 0.372411, validation accuracy: 90.32%\n",
      "step: 62000, batch loss: 0.606158, train loss: 0.369349, train accuracy: 90.27%, validation loss: 0.372124, validation accuracy: 90.36%\n",
      "step: 63000, batch loss: 0.674607, train loss: 0.369890, train accuracy: 90.25%, validation loss: 0.374999, validation accuracy: 90.38%\n",
      "step: 64000, batch loss: 0.024732, train loss: 0.367111, train accuracy: 90.38%, validation loss: 0.373454, validation accuracy: 90.44%\n",
      "step: 65000, batch loss: 0.149730, train loss: 0.366522, train accuracy: 90.50%, validation loss: 0.370854, validation accuracy: 90.40%\n",
      "step: 66000, batch loss: 0.458344, train loss: 0.364090, train accuracy: 90.41%, validation loss: 0.368700, validation accuracy: 90.54%\n",
      "step: 67000, batch loss: 2.155894, train loss: 0.362634, train accuracy: 90.39%, validation loss: 0.368707, validation accuracy: 90.20%\n",
      "step: 68000, batch loss: 0.225126, train loss: 0.363014, train accuracy: 90.49%, validation loss: 0.367077, validation accuracy: 90.42%\n",
      "step: 69000, batch loss: 0.616881, train loss: 0.365509, train accuracy: 90.24%, validation loss: 0.370435, validation accuracy: 90.46%\n",
      "step: 70000, batch loss: 0.399368, train loss: 0.362940, train accuracy: 90.42%, validation loss: 0.367279, validation accuracy: 90.72%\n",
      "step: 71000, batch loss: 0.137499, train loss: 0.359107, train accuracy: 90.66%, validation loss: 0.365256, validation accuracy: 90.64%\n",
      "step: 72000, batch loss: 0.011927, train loss: 0.356826, train accuracy: 90.63%, validation loss: 0.359251, validation accuracy: 90.78%\n",
      "step: 73000, batch loss: 0.138477, train loss: 0.357870, train accuracy: 90.61%, validation loss: 0.366136, validation accuracy: 90.58%\n",
      "step: 74000, batch loss: 0.015488, train loss: 0.356915, train accuracy: 90.43%, validation loss: 0.365405, validation accuracy: 90.40%\n",
      "step: 75000, batch loss: 0.718024, train loss: 0.352455, train accuracy: 90.70%, validation loss: 0.358788, validation accuracy: 90.62%\n",
      "step: 76000, batch loss: 0.016548, train loss: 0.351194, train accuracy: 90.73%, validation loss: 0.356428, validation accuracy: 91.06%\n",
      "step: 77000, batch loss: 0.338247, train loss: 0.350093, train accuracy: 90.85%, validation loss: 0.357043, validation accuracy: 91.02%\n",
      "step: 78000, batch loss: 0.116197, train loss: 0.349773, train accuracy: 90.79%, validation loss: 0.356587, validation accuracy: 91.00%\n",
      "step: 79000, batch loss: 0.129179, train loss: 0.351014, train accuracy: 90.75%, validation loss: 0.357343, validation accuracy: 90.86%\n",
      "step: 80000, batch loss: 0.465419, train loss: 0.346847, train accuracy: 90.85%, validation loss: 0.354424, validation accuracy: 90.96%\n",
      "step: 81000, batch loss: 0.007912, train loss: 0.346075, train accuracy: 90.79%, validation loss: 0.354508, validation accuracy: 90.90%\n",
      "step: 82000, batch loss: 0.467130, train loss: 0.345248, train accuracy: 90.83%, validation loss: 0.355568, validation accuracy: 90.88%\n",
      "step: 83000, batch loss: 0.047226, train loss: 0.343281, train accuracy: 90.97%, validation loss: 0.353239, validation accuracy: 90.84%\n",
      "step: 84000, batch loss: 0.009234, train loss: 0.344184, train accuracy: 90.84%, validation loss: 0.349703, validation accuracy: 91.00%\n",
      "step: 85000, batch loss: 0.296456, train loss: 0.342579, train accuracy: 90.93%, validation loss: 0.348647, validation accuracy: 91.06%\n",
      "step: 86000, batch loss: 0.430802, train loss: 0.347073, train accuracy: 90.71%, validation loss: 0.354974, validation accuracy: 90.88%\n",
      "step: 87000, batch loss: 0.184730, train loss: 0.345120, train accuracy: 90.81%, validation loss: 0.357372, validation accuracy: 90.90%\n",
      "step: 88000, batch loss: 1.187758, train loss: 0.338387, train accuracy: 90.96%, validation loss: 0.348671, validation accuracy: 90.88%\n",
      "step: 89000, batch loss: 0.030870, train loss: 0.341296, train accuracy: 90.87%, validation loss: 0.350576, validation accuracy: 90.88%\n",
      "step: 90000, batch loss: 1.021508, train loss: 0.338577, train accuracy: 91.04%, validation loss: 0.348934, validation accuracy: 91.18%\n",
      "step: 91000, batch loss: 0.567294, train loss: 0.335652, train accuracy: 90.97%, validation loss: 0.348412, validation accuracy: 90.74%\n",
      "step: 92000, batch loss: 0.161386, train loss: 0.337309, train accuracy: 90.97%, validation loss: 0.348141, validation accuracy: 90.86%\n",
      "step: 93000, batch loss: 0.073225, train loss: 0.338624, train accuracy: 90.85%, validation loss: 0.348822, validation accuracy: 90.90%\n",
      "step: 94000, batch loss: 0.293258, train loss: 0.332487, train accuracy: 91.19%, validation loss: 0.341921, validation accuracy: 91.10%\n",
      "step: 95000, batch loss: 0.343567, train loss: 0.333523, train accuracy: 91.06%, validation loss: 0.345469, validation accuracy: 91.00%\n",
      "step: 96000, batch loss: 0.877144, train loss: 0.332695, train accuracy: 91.12%, validation loss: 0.346415, validation accuracy: 90.94%\n",
      "step: 97000, batch loss: 0.324001, train loss: 0.336276, train accuracy: 90.97%, validation loss: 0.349130, validation accuracy: 91.02%\n",
      "step: 98000, batch loss: 0.026916, train loss: 0.333580, train accuracy: 91.09%, validation loss: 0.346064, validation accuracy: 90.86%\n",
      "step: 99000, batch loss: 0.177250, train loss: 0.333285, train accuracy: 91.04%, validation loss: 0.344177, validation accuracy: 91.20%\n",
      "step: 100000, batch loss: 0.105006, train loss: 0.329944, train accuracy: 91.18%, validation loss: 0.341651, validation accuracy: 91.20%\n",
      "step: 101000, batch loss: 1.168964, train loss: 0.332471, train accuracy: 91.03%, validation loss: 0.346180, validation accuracy: 91.14%\n",
      "step: 102000, batch loss: 0.196582, train loss: 0.327770, train accuracy: 91.26%, validation loss: 0.338483, validation accuracy: 91.40%\n",
      "step: 103000, batch loss: 0.193931, train loss: 0.326098, train accuracy: 91.27%, validation loss: 0.339067, validation accuracy: 91.20%\n",
      "step: 104000, batch loss: 0.131737, train loss: 0.327512, train accuracy: 91.16%, validation loss: 0.338478, validation accuracy: 91.14%\n",
      "step: 105000, batch loss: 0.769789, train loss: 0.323879, train accuracy: 91.30%, validation loss: 0.338773, validation accuracy: 91.22%\n",
      "step: 106000, batch loss: 0.006737, train loss: 0.323852, train accuracy: 91.29%, validation loss: 0.337123, validation accuracy: 91.10%\n",
      "step: 107000, batch loss: 0.189625, train loss: 0.324481, train accuracy: 91.22%, validation loss: 0.336839, validation accuracy: 91.50%\n",
      "step: 108000, batch loss: 0.019096, train loss: 0.325243, train accuracy: 91.28%, validation loss: 0.340374, validation accuracy: 91.24%\n",
      "step: 109000, batch loss: 0.269542, train loss: 0.321412, train accuracy: 91.37%, validation loss: 0.335416, validation accuracy: 91.38%\n",
      "step: 110000, batch loss: 0.185387, train loss: 0.321670, train accuracy: 91.30%, validation loss: 0.335901, validation accuracy: 91.42%\n",
      "step: 111000, batch loss: 0.014460, train loss: 0.322520, train accuracy: 91.34%, validation loss: 0.338421, validation accuracy: 91.16%\n",
      "step: 112000, batch loss: 0.027788, train loss: 0.320591, train accuracy: 91.32%, validation loss: 0.335248, validation accuracy: 91.10%\n",
      "step: 113000, batch loss: 0.097982, train loss: 0.320848, train accuracy: 91.31%, validation loss: 0.334337, validation accuracy: 91.34%\n",
      "step: 114000, batch loss: 0.305316, train loss: 0.320656, train accuracy: 91.39%, validation loss: 0.332117, validation accuracy: 91.46%\n",
      "step: 115000, batch loss: 0.011571, train loss: 0.317912, train accuracy: 91.41%, validation loss: 0.329901, validation accuracy: 91.58%\n",
      "step: 116000, batch loss: 0.020545, train loss: 0.318907, train accuracy: 91.45%, validation loss: 0.332638, validation accuracy: 91.30%\n",
      "step: 117000, batch loss: 1.482783, train loss: 0.318138, train accuracy: 91.40%, validation loss: 0.332921, validation accuracy: 91.50%\n",
      "step: 118000, batch loss: 0.916112, train loss: 0.317502, train accuracy: 91.41%, validation loss: 0.330841, validation accuracy: 91.44%\n",
      "step: 119000, batch loss: 0.319206, train loss: 0.316014, train accuracy: 91.47%, validation loss: 0.330722, validation accuracy: 91.56%\n",
      "step: 120000, batch loss: 0.695937, train loss: 0.317762, train accuracy: 91.53%, validation loss: 0.328888, validation accuracy: 91.58%\n",
      "step: 121000, batch loss: 0.279293, train loss: 0.317334, train accuracy: 91.48%, validation loss: 0.334008, validation accuracy: 91.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 122000, batch loss: 0.067478, train loss: 0.316161, train accuracy: 91.51%, validation loss: 0.330340, validation accuracy: 91.50%\n",
      "step: 123000, batch loss: 0.128557, train loss: 0.315661, train accuracy: 91.57%, validation loss: 0.330494, validation accuracy: 91.40%\n",
      "step: 124000, batch loss: 0.861188, train loss: 0.313723, train accuracy: 91.46%, validation loss: 0.330772, validation accuracy: 91.36%\n",
      "step: 125000, batch loss: 0.005349, train loss: 0.316048, train accuracy: 91.48%, validation loss: 0.330437, validation accuracy: 91.50%\n",
      "step: 126000, batch loss: 0.135362, train loss: 0.312361, train accuracy: 91.59%, validation loss: 0.326127, validation accuracy: 91.62%\n",
      "step: 127000, batch loss: 2.064535, train loss: 0.312491, train accuracy: 91.51%, validation loss: 0.330128, validation accuracy: 91.66%\n",
      "step: 128000, batch loss: 0.208197, train loss: 0.312570, train accuracy: 91.58%, validation loss: 0.326194, validation accuracy: 91.56%\n",
      "step: 129000, batch loss: 0.216945, train loss: 0.309722, train accuracy: 91.62%, validation loss: 0.325118, validation accuracy: 91.60%\n",
      "step: 130000, batch loss: 0.224042, train loss: 0.313195, train accuracy: 91.45%, validation loss: 0.332203, validation accuracy: 91.38%\n",
      "step: 131000, batch loss: 0.099102, train loss: 0.311527, train accuracy: 91.53%, validation loss: 0.325729, validation accuracy: 91.64%\n",
      "step: 132000, batch loss: 0.120464, train loss: 0.310492, train accuracy: 91.54%, validation loss: 0.326049, validation accuracy: 91.60%\n",
      "step: 133000, batch loss: 0.290113, train loss: 0.308865, train accuracy: 91.61%, validation loss: 0.326363, validation accuracy: 91.50%\n",
      "step: 134000, batch loss: 0.010026, train loss: 0.307405, train accuracy: 91.64%, validation loss: 0.324155, validation accuracy: 91.62%\n",
      "step: 135000, batch loss: 0.083800, train loss: 0.307492, train accuracy: 91.66%, validation loss: 0.323010, validation accuracy: 91.66%\n",
      "step: 136000, batch loss: 0.421586, train loss: 0.308278, train accuracy: 91.69%, validation loss: 0.326722, validation accuracy: 91.44%\n",
      "step: 137000, batch loss: 0.065162, train loss: 0.307343, train accuracy: 91.69%, validation loss: 0.324456, validation accuracy: 91.46%\n",
      "step: 138000, batch loss: 0.346990, train loss: 0.311388, train accuracy: 91.51%, validation loss: 0.330948, validation accuracy: 91.24%\n",
      "step: 139000, batch loss: 0.065842, train loss: 0.308397, train accuracy: 91.65%, validation loss: 0.327381, validation accuracy: 91.48%\n",
      "step: 140000, batch loss: 0.058159, train loss: 0.307689, train accuracy: 91.64%, validation loss: 0.326383, validation accuracy: 91.26%\n",
      "step: 141000, batch loss: 0.063406, train loss: 0.306216, train accuracy: 91.61%, validation loss: 0.325572, validation accuracy: 91.46%\n",
      "step: 142000, batch loss: 0.129383, train loss: 0.305260, train accuracy: 91.69%, validation loss: 0.321385, validation accuracy: 91.62%\n",
      "step: 143000, batch loss: 0.282117, train loss: 0.304877, train accuracy: 91.69%, validation loss: 0.322359, validation accuracy: 91.64%\n",
      "step: 144000, batch loss: 0.239427, train loss: 0.303307, train accuracy: 91.75%, validation loss: 0.320473, validation accuracy: 91.74%\n",
      "step: 145000, batch loss: 0.201304, train loss: 0.305130, train accuracy: 91.71%, validation loss: 0.323536, validation accuracy: 91.64%\n",
      "step: 146000, batch loss: 0.135637, train loss: 0.305136, train accuracy: 91.64%, validation loss: 0.323391, validation accuracy: 91.60%\n",
      "step: 147000, batch loss: 0.194749, train loss: 0.301886, train accuracy: 91.82%, validation loss: 0.320821, validation accuracy: 91.70%\n",
      "step: 148000, batch loss: 0.007036, train loss: 0.302385, train accuracy: 91.82%, validation loss: 0.320508, validation accuracy: 91.72%\n",
      "step: 149000, batch loss: 0.013280, train loss: 0.305125, train accuracy: 91.71%, validation loss: 0.323363, validation accuracy: 91.30%\n",
      "step: 150000, batch loss: 0.133064, train loss: 0.303773, train accuracy: 91.74%, validation loss: 0.321271, validation accuracy: 91.64%\n",
      "step: 151000, batch loss: 0.211170, train loss: 0.302991, train accuracy: 91.72%, validation loss: 0.321302, validation accuracy: 91.44%\n",
      "step: 152000, batch loss: 0.621790, train loss: 0.301670, train accuracy: 91.76%, validation loss: 0.320536, validation accuracy: 91.68%\n",
      "step: 153000, batch loss: 0.656801, train loss: 0.299307, train accuracy: 91.92%, validation loss: 0.317438, validation accuracy: 91.62%\n",
      "step: 154000, batch loss: 0.404686, train loss: 0.298679, train accuracy: 91.89%, validation loss: 0.318594, validation accuracy: 91.58%\n",
      "step: 155000, batch loss: 0.252718, train loss: 0.300205, train accuracy: 91.75%, validation loss: 0.318664, validation accuracy: 91.62%\n",
      "step: 156000, batch loss: 0.116417, train loss: 0.303553, train accuracy: 91.81%, validation loss: 0.321557, validation accuracy: 91.36%\n",
      "step: 157000, batch loss: 0.229507, train loss: 0.298134, train accuracy: 91.83%, validation loss: 0.316401, validation accuracy: 91.70%\n",
      "step: 158000, batch loss: 0.020635, train loss: 0.300031, train accuracy: 91.89%, validation loss: 0.320279, validation accuracy: 91.84%\n",
      "step: 159000, batch loss: 0.048648, train loss: 0.298336, train accuracy: 91.82%, validation loss: 0.316846, validation accuracy: 91.72%\n",
      "step: 160000, batch loss: 0.077858, train loss: 0.300424, train accuracy: 91.71%, validation loss: 0.319866, validation accuracy: 91.46%\n",
      "step: 161000, batch loss: 0.377557, train loss: 0.305620, train accuracy: 91.69%, validation loss: 0.325450, validation accuracy: 91.34%\n",
      "step: 162000, batch loss: 0.175111, train loss: 0.300951, train accuracy: 91.75%, validation loss: 0.322084, validation accuracy: 91.58%\n",
      "step: 163000, batch loss: 0.795218, train loss: 0.296407, train accuracy: 91.95%, validation loss: 0.316009, validation accuracy: 91.82%\n",
      "step: 164000, batch loss: 0.060843, train loss: 0.295322, train accuracy: 92.03%, validation loss: 0.316501, validation accuracy: 91.70%\n",
      "step: 165000, batch loss: 0.031806, train loss: 0.294891, train accuracy: 91.94%, validation loss: 0.315933, validation accuracy: 91.64%\n",
      "step: 166000, batch loss: 0.017697, train loss: 0.295780, train accuracy: 91.95%, validation loss: 0.317328, validation accuracy: 91.54%\n",
      "step: 167000, batch loss: 0.101345, train loss: 0.297010, train accuracy: 91.81%, validation loss: 0.318168, validation accuracy: 91.68%\n",
      "step: 168000, batch loss: 0.136279, train loss: 0.295401, train accuracy: 92.00%, validation loss: 0.317405, validation accuracy: 91.60%\n",
      "step: 169000, batch loss: 0.003252, train loss: 0.295443, train accuracy: 92.03%, validation loss: 0.318854, validation accuracy: 91.54%\n",
      "step: 170000, batch loss: 0.080708, train loss: 0.294446, train accuracy: 92.01%, validation loss: 0.315320, validation accuracy: 91.56%\n",
      "step: 171000, batch loss: 0.212457, train loss: 0.295520, train accuracy: 91.90%, validation loss: 0.315426, validation accuracy: 91.84%\n",
      "step: 172000, batch loss: 1.194329, train loss: 0.294420, train accuracy: 91.96%, validation loss: 0.316186, validation accuracy: 91.66%\n",
      "step: 173000, batch loss: 0.024518, train loss: 0.294930, train accuracy: 92.01%, validation loss: 0.315266, validation accuracy: 91.68%\n",
      "step: 174000, batch loss: 0.026527, train loss: 0.293259, train accuracy: 91.98%, validation loss: 0.315084, validation accuracy: 91.62%\n",
      "step: 175000, batch loss: 0.593046, train loss: 0.292322, train accuracy: 91.99%, validation loss: 0.312977, validation accuracy: 91.62%\n",
      "step: 176000, batch loss: 0.067039, train loss: 0.292175, train accuracy: 91.96%, validation loss: 0.314088, validation accuracy: 91.62%\n",
      "step: 177000, batch loss: 0.366700, train loss: 0.290740, train accuracy: 92.00%, validation loss: 0.312546, validation accuracy: 91.86%\n",
      "step: 178000, batch loss: 0.166574, train loss: 0.291589, train accuracy: 92.03%, validation loss: 0.311175, validation accuracy: 91.78%\n",
      "step: 179000, batch loss: 0.116860, train loss: 0.293132, train accuracy: 91.98%, validation loss: 0.315144, validation accuracy: 91.62%\n",
      "step: 180000, batch loss: 0.252236, train loss: 0.290818, train accuracy: 92.07%, validation loss: 0.312684, validation accuracy: 91.82%\n",
      "step: 181000, batch loss: 0.045681, train loss: 0.291807, train accuracy: 92.15%, validation loss: 0.316010, validation accuracy: 91.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 182000, batch loss: 0.398313, train loss: 0.294409, train accuracy: 92.02%, validation loss: 0.314818, validation accuracy: 91.68%\n",
      "step: 183000, batch loss: 0.320341, train loss: 0.291647, train accuracy: 92.01%, validation loss: 0.310480, validation accuracy: 91.72%\n",
      "step: 184000, batch loss: 0.176722, train loss: 0.298942, train accuracy: 91.77%, validation loss: 0.317031, validation accuracy: 91.52%\n",
      "step: 185000, batch loss: 0.480806, train loss: 0.289312, train accuracy: 92.08%, validation loss: 0.310238, validation accuracy: 91.90%\n",
      "step: 186000, batch loss: 0.049966, train loss: 0.291392, train accuracy: 91.95%, validation loss: 0.313780, validation accuracy: 91.90%\n",
      "step: 187000, batch loss: 0.146054, train loss: 0.289880, train accuracy: 92.08%, validation loss: 0.311852, validation accuracy: 91.70%\n",
      "step: 188000, batch loss: 0.075179, train loss: 0.294114, train accuracy: 91.88%, validation loss: 0.316922, validation accuracy: 91.66%\n",
      "step: 189000, batch loss: 0.856467, train loss: 0.290053, train accuracy: 92.04%, validation loss: 0.310449, validation accuracy: 91.80%\n",
      "step: 190000, batch loss: 0.079927, train loss: 0.289038, train accuracy: 92.06%, validation loss: 0.309556, validation accuracy: 91.68%\n",
      "step: 191000, batch loss: 0.352591, train loss: 0.292893, train accuracy: 92.03%, validation loss: 0.315113, validation accuracy: 91.74%\n",
      "step: 192000, batch loss: 0.059601, train loss: 0.289855, train accuracy: 92.00%, validation loss: 0.313811, validation accuracy: 91.66%\n",
      "step: 193000, batch loss: 0.193547, train loss: 0.286813, train accuracy: 92.19%, validation loss: 0.309518, validation accuracy: 91.76%\n",
      "step: 194000, batch loss: 0.567448, train loss: 0.286491, train accuracy: 92.16%, validation loss: 0.311112, validation accuracy: 91.50%\n",
      "step: 195000, batch loss: 0.327267, train loss: 0.290822, train accuracy: 91.87%, validation loss: 0.313781, validation accuracy: 91.72%\n",
      "step: 196000, batch loss: 0.403358, train loss: 0.286307, train accuracy: 92.18%, validation loss: 0.309407, validation accuracy: 91.58%\n",
      "step: 197000, batch loss: 0.030066, train loss: 0.286418, train accuracy: 92.11%, validation loss: 0.307317, validation accuracy: 91.88%\n",
      "step: 198000, batch loss: 0.033751, train loss: 0.288009, train accuracy: 92.13%, validation loss: 0.309312, validation accuracy: 91.86%\n",
      "step: 199000, batch loss: 0.052215, train loss: 0.285269, train accuracy: 92.24%, validation loss: 0.310325, validation accuracy: 91.60%\n",
      "step: 200000, batch loss: 0.116207, train loss: 0.284867, train accuracy: 92.20%, validation loss: 0.310711, validation accuracy: 91.96%\n",
      "step: 201000, batch loss: 0.059099, train loss: 0.286407, train accuracy: 92.11%, validation loss: 0.309618, validation accuracy: 91.60%\n",
      "step: 202000, batch loss: 0.242745, train loss: 0.290320, train accuracy: 92.04%, validation loss: 0.310377, validation accuracy: 91.80%\n",
      "step: 203000, batch loss: 0.272889, train loss: 0.285340, train accuracy: 92.19%, validation loss: 0.309474, validation accuracy: 91.74%\n",
      "step: 204000, batch loss: 0.125555, train loss: 0.283653, train accuracy: 92.32%, validation loss: 0.308266, validation accuracy: 91.62%\n",
      "step: 205000, batch loss: 0.077443, train loss: 0.285655, train accuracy: 92.15%, validation loss: 0.310485, validation accuracy: 91.72%\n",
      "step: 206000, batch loss: 0.198843, train loss: 0.288183, train accuracy: 91.97%, validation loss: 0.314960, validation accuracy: 91.60%\n",
      "step: 207000, batch loss: 0.113022, train loss: 0.283378, train accuracy: 92.26%, validation loss: 0.307715, validation accuracy: 91.92%\n",
      "step: 208000, batch loss: 0.011645, train loss: 0.282579, train accuracy: 92.27%, validation loss: 0.307647, validation accuracy: 91.90%\n",
      "step: 209000, batch loss: 0.308569, train loss: 0.286111, train accuracy: 92.15%, validation loss: 0.312377, validation accuracy: 91.90%\n",
      "step: 210000, batch loss: 0.003846, train loss: 0.283600, train accuracy: 92.22%, validation loss: 0.307811, validation accuracy: 91.70%\n",
      "step: 211000, batch loss: 0.156096, train loss: 0.283162, train accuracy: 92.23%, validation loss: 0.309612, validation accuracy: 91.58%\n",
      "step: 212000, batch loss: 0.508894, train loss: 0.284703, train accuracy: 92.06%, validation loss: 0.308136, validation accuracy: 91.90%\n",
      "step: 213000, batch loss: 0.618600, train loss: 0.286047, train accuracy: 92.13%, validation loss: 0.308193, validation accuracy: 91.70%\n",
      "step: 214000, batch loss: 0.066670, train loss: 0.281869, train accuracy: 92.23%, validation loss: 0.306057, validation accuracy: 91.86%\n",
      "step: 215000, batch loss: 0.001857, train loss: 0.282022, train accuracy: 92.26%, validation loss: 0.306666, validation accuracy: 92.06%\n",
      "step: 216000, batch loss: 0.088295, train loss: 0.283096, train accuracy: 92.27%, validation loss: 0.307880, validation accuracy: 92.04%\n",
      "step: 217000, batch loss: 0.032755, train loss: 0.280609, train accuracy: 92.33%, validation loss: 0.304959, validation accuracy: 91.96%\n",
      "step: 218000, batch loss: 0.100165, train loss: 0.279492, train accuracy: 92.33%, validation loss: 0.304007, validation accuracy: 91.90%\n",
      "step: 219000, batch loss: 0.213201, train loss: 0.282159, train accuracy: 92.28%, validation loss: 0.305906, validation accuracy: 92.02%\n",
      "step: 220000, batch loss: 0.532426, train loss: 0.280621, train accuracy: 92.38%, validation loss: 0.306612, validation accuracy: 91.64%\n",
      "step: 221000, batch loss: 0.015365, train loss: 0.280396, train accuracy: 92.30%, validation loss: 0.305536, validation accuracy: 91.82%\n",
      "step: 222000, batch loss: 0.080158, train loss: 0.279534, train accuracy: 92.34%, validation loss: 0.303626, validation accuracy: 92.06%\n",
      "step: 223000, batch loss: 0.180248, train loss: 0.279572, train accuracy: 92.34%, validation loss: 0.305752, validation accuracy: 92.06%\n",
      "step: 224000, batch loss: 0.061406, train loss: 0.280150, train accuracy: 92.28%, validation loss: 0.304647, validation accuracy: 92.00%\n",
      "step: 225000, batch loss: 0.113261, train loss: 0.278049, train accuracy: 92.38%, validation loss: 0.303508, validation accuracy: 92.00%\n",
      "step: 226000, batch loss: 0.159426, train loss: 0.279165, train accuracy: 92.29%, validation loss: 0.305136, validation accuracy: 91.76%\n",
      "step: 227000, batch loss: 0.099481, train loss: 0.278212, train accuracy: 92.38%, validation loss: 0.304339, validation accuracy: 91.68%\n",
      "step: 228000, batch loss: 0.303155, train loss: 0.280772, train accuracy: 92.28%, validation loss: 0.305852, validation accuracy: 92.08%\n",
      "step: 229000, batch loss: 0.043968, train loss: 0.278657, train accuracy: 92.38%, validation loss: 0.305670, validation accuracy: 91.68%\n",
      "step: 230000, batch loss: 0.098287, train loss: 0.281082, train accuracy: 92.22%, validation loss: 0.307029, validation accuracy: 91.68%\n",
      "step: 231000, batch loss: 0.010727, train loss: 0.278371, train accuracy: 92.39%, validation loss: 0.306032, validation accuracy: 91.92%\n",
      "step: 232000, batch loss: 0.293756, train loss: 0.279006, train accuracy: 92.36%, validation loss: 0.306764, validation accuracy: 91.92%\n",
      "step: 233000, batch loss: 0.005771, train loss: 0.278311, train accuracy: 92.37%, validation loss: 0.302484, validation accuracy: 91.72%\n",
      "step: 234000, batch loss: 0.259120, train loss: 0.277655, train accuracy: 92.35%, validation loss: 0.303795, validation accuracy: 92.22%\n",
      "step: 235000, batch loss: 0.404063, train loss: 0.281192, train accuracy: 92.18%, validation loss: 0.307631, validation accuracy: 91.76%\n",
      "step: 236000, batch loss: 1.128133, train loss: 0.278257, train accuracy: 92.44%, validation loss: 0.303593, validation accuracy: 91.92%\n",
      "step: 237000, batch loss: 1.861108, train loss: 0.277688, train accuracy: 92.42%, validation loss: 0.304296, validation accuracy: 92.02%\n",
      "step: 238000, batch loss: 1.341657, train loss: 0.276016, train accuracy: 92.42%, validation loss: 0.301385, validation accuracy: 91.86%\n",
      "step: 239000, batch loss: 0.060202, train loss: 0.275418, train accuracy: 92.42%, validation loss: 0.302681, validation accuracy: 91.94%\n",
      "step: 240000, batch loss: 0.029298, train loss: 0.276942, train accuracy: 92.42%, validation loss: 0.303255, validation accuracy: 91.82%\n",
      "step: 241000, batch loss: 0.010426, train loss: 0.275931, train accuracy: 92.48%, validation loss: 0.304409, validation accuracy: 91.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 242000, batch loss: 0.109894, train loss: 0.278450, train accuracy: 92.31%, validation loss: 0.305376, validation accuracy: 91.80%\n",
      "step: 243000, batch loss: 0.078152, train loss: 0.276202, train accuracy: 92.50%, validation loss: 0.303158, validation accuracy: 91.80%\n",
      "step: 244000, batch loss: 0.320425, train loss: 0.275559, train accuracy: 92.51%, validation loss: 0.301244, validation accuracy: 91.82%\n",
      "step: 245000, batch loss: 0.030176, train loss: 0.276415, train accuracy: 92.47%, validation loss: 0.301957, validation accuracy: 92.02%\n",
      "step: 246000, batch loss: 0.095441, train loss: 0.274594, train accuracy: 92.55%, validation loss: 0.300683, validation accuracy: 91.90%\n",
      "step: 247000, batch loss: 0.088449, train loss: 0.276909, train accuracy: 92.38%, validation loss: 0.299924, validation accuracy: 92.06%\n",
      "step: 248000, batch loss: 0.014002, train loss: 0.274331, train accuracy: 92.49%, validation loss: 0.301190, validation accuracy: 91.98%\n",
      "step: 249000, batch loss: 0.582693, train loss: 0.274762, train accuracy: 92.48%, validation loss: 0.301466, validation accuracy: 91.92%\n",
      "step: 250000, batch loss: 1.052654, train loss: 0.275185, train accuracy: 92.43%, validation loss: 0.301477, validation accuracy: 91.88%\n",
      "step: 251000, batch loss: 0.333580, train loss: 0.274628, train accuracy: 92.49%, validation loss: 0.303424, validation accuracy: 92.02%\n",
      "step: 252000, batch loss: 0.043656, train loss: 0.275396, train accuracy: 92.56%, validation loss: 0.303888, validation accuracy: 91.80%\n",
      "step: 253000, batch loss: 0.056677, train loss: 0.281490, train accuracy: 92.27%, validation loss: 0.306759, validation accuracy: 91.74%\n",
      "step: 254000, batch loss: 0.772844, train loss: 0.273336, train accuracy: 92.51%, validation loss: 0.301894, validation accuracy: 91.84%\n",
      "step: 255000, batch loss: 0.174840, train loss: 0.273183, train accuracy: 92.49%, validation loss: 0.301849, validation accuracy: 91.98%\n",
      "step: 256000, batch loss: 0.286090, train loss: 0.274505, train accuracy: 92.48%, validation loss: 0.302804, validation accuracy: 91.80%\n",
      "step: 257000, batch loss: 0.051326, train loss: 0.272609, train accuracy: 92.49%, validation loss: 0.300119, validation accuracy: 91.78%\n",
      "step: 258000, batch loss: 0.860998, train loss: 0.275407, train accuracy: 92.45%, validation loss: 0.303258, validation accuracy: 92.04%\n",
      "step: 259000, batch loss: 0.154818, train loss: 0.272848, train accuracy: 92.52%, validation loss: 0.298117, validation accuracy: 92.08%\n",
      "step: 260000, batch loss: 0.268655, train loss: 0.278548, train accuracy: 92.31%, validation loss: 0.306266, validation accuracy: 91.92%\n",
      "step: 261000, batch loss: 0.163328, train loss: 0.272029, train accuracy: 92.57%, validation loss: 0.300713, validation accuracy: 92.06%\n",
      "step: 262000, batch loss: 0.041441, train loss: 0.272155, train accuracy: 92.53%, validation loss: 0.301625, validation accuracy: 92.10%\n",
      "step: 263000, batch loss: 0.025157, train loss: 0.271881, train accuracy: 92.53%, validation loss: 0.301825, validation accuracy: 91.82%\n",
      "step: 264000, batch loss: 0.093454, train loss: 0.273832, train accuracy: 92.46%, validation loss: 0.303267, validation accuracy: 91.64%\n",
      "step: 265000, batch loss: 0.500999, train loss: 0.278420, train accuracy: 92.26%, validation loss: 0.304231, validation accuracy: 91.96%\n",
      "step: 266000, batch loss: 0.172128, train loss: 0.271454, train accuracy: 92.61%, validation loss: 0.300292, validation accuracy: 91.74%\n",
      "step: 267000, batch loss: 0.076632, train loss: 0.272746, train accuracy: 92.57%, validation loss: 0.302724, validation accuracy: 92.02%\n",
      "step: 268000, batch loss: 0.336497, train loss: 0.271414, train accuracy: 92.57%, validation loss: 0.299428, validation accuracy: 92.02%\n",
      "step: 269000, batch loss: 0.372787, train loss: 0.273453, train accuracy: 92.51%, validation loss: 0.304737, validation accuracy: 91.92%\n",
      "step: 270000, batch loss: 0.918618, train loss: 0.273122, train accuracy: 92.46%, validation loss: 0.303133, validation accuracy: 91.98%\n",
      "step: 271000, batch loss: 0.044719, train loss: 0.272570, train accuracy: 92.52%, validation loss: 0.302384, validation accuracy: 92.00%\n",
      "step: 272000, batch loss: 0.023913, train loss: 0.270733, train accuracy: 92.53%, validation loss: 0.300312, validation accuracy: 91.98%\n",
      "step: 273000, batch loss: 0.413034, train loss: 0.269548, train accuracy: 92.57%, validation loss: 0.300044, validation accuracy: 91.92%\n",
      "step: 274000, batch loss: 0.075939, train loss: 0.272837, train accuracy: 92.46%, validation loss: 0.301235, validation accuracy: 91.80%\n",
      "step: 275000, batch loss: 0.021720, train loss: 0.270397, train accuracy: 92.62%, validation loss: 0.297226, validation accuracy: 91.98%\n",
      "step: 276000, batch loss: 0.404869, train loss: 0.271139, train accuracy: 92.49%, validation loss: 0.298449, validation accuracy: 91.88%\n",
      "step: 277000, batch loss: 0.006037, train loss: 0.271275, train accuracy: 92.59%, validation loss: 0.298859, validation accuracy: 92.00%\n",
      "step: 278000, batch loss: 0.466373, train loss: 0.272588, train accuracy: 92.41%, validation loss: 0.300696, validation accuracy: 91.94%\n",
      "step: 279000, batch loss: 0.401746, train loss: 0.269961, train accuracy: 92.59%, validation loss: 0.300047, validation accuracy: 92.02%\n",
      "step: 280000, batch loss: 0.125053, train loss: 0.269897, train accuracy: 92.61%, validation loss: 0.300603, validation accuracy: 91.98%\n",
      "step: 281000, batch loss: 0.399657, train loss: 0.268044, train accuracy: 92.69%, validation loss: 0.299718, validation accuracy: 91.76%\n",
      "step: 282000, batch loss: 0.001346, train loss: 0.271262, train accuracy: 92.54%, validation loss: 0.302525, validation accuracy: 91.84%\n",
      "step: 283000, batch loss: 0.008597, train loss: 0.269529, train accuracy: 92.64%, validation loss: 0.298105, validation accuracy: 91.94%\n",
      "step: 284000, batch loss: 0.106267, train loss: 0.267934, train accuracy: 92.66%, validation loss: 0.297350, validation accuracy: 91.90%\n",
      "step: 285000, batch loss: 0.387990, train loss: 0.270915, train accuracy: 92.53%, validation loss: 0.298109, validation accuracy: 91.94%\n",
      "step: 286000, batch loss: 0.060167, train loss: 0.270527, train accuracy: 92.57%, validation loss: 0.301526, validation accuracy: 92.14%\n",
      "step: 287000, batch loss: 0.106428, train loss: 0.270928, train accuracy: 92.51%, validation loss: 0.300438, validation accuracy: 92.00%\n",
      "step: 288000, batch loss: 0.126906, train loss: 0.269571, train accuracy: 92.66%, validation loss: 0.299628, validation accuracy: 91.94%\n",
      "step: 289000, batch loss: 0.584221, train loss: 0.267758, train accuracy: 92.62%, validation loss: 0.300785, validation accuracy: 92.00%\n",
      "step: 290000, batch loss: 0.037121, train loss: 0.269461, train accuracy: 92.55%, validation loss: 0.300447, validation accuracy: 91.66%\n",
      "step: 291000, batch loss: 0.151790, train loss: 0.268947, train accuracy: 92.58%, validation loss: 0.300043, validation accuracy: 92.12%\n",
      "step: 292000, batch loss: 0.841901, train loss: 0.269273, train accuracy: 92.58%, validation loss: 0.297002, validation accuracy: 91.98%\n",
      "step: 293000, batch loss: 0.014599, train loss: 0.266888, train accuracy: 92.72%, validation loss: 0.296165, validation accuracy: 92.20%\n",
      "step: 294000, batch loss: 0.560866, train loss: 0.268584, train accuracy: 92.57%, validation loss: 0.300088, validation accuracy: 92.08%\n",
      "step: 295000, batch loss: 0.026388, train loss: 0.267222, train accuracy: 92.69%, validation loss: 0.296794, validation accuracy: 92.14%\n",
      "step: 296000, batch loss: 0.140134, train loss: 0.266951, train accuracy: 92.75%, validation loss: 0.297088, validation accuracy: 92.06%\n",
      "step: 297000, batch loss: 0.099014, train loss: 0.266209, train accuracy: 92.78%, validation loss: 0.296542, validation accuracy: 92.10%\n",
      "step: 298000, batch loss: 0.083703, train loss: 0.271883, train accuracy: 92.50%, validation loss: 0.304490, validation accuracy: 92.16%\n",
      "step: 299000, batch loss: 0.024455, train loss: 0.266927, train accuracy: 92.79%, validation loss: 0.296093, validation accuracy: 92.28%\n",
      "step: 300000, batch loss: 0.081211, train loss: 0.266954, train accuracy: 92.69%, validation loss: 0.296360, validation accuracy: 91.98%\n",
      "step: 301000, batch loss: 0.664966, train loss: 0.267345, train accuracy: 92.60%, validation loss: 0.297632, validation accuracy: 92.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 302000, batch loss: 0.302227, train loss: 0.265834, train accuracy: 92.71%, validation loss: 0.296097, validation accuracy: 91.94%\n",
      "step: 303000, batch loss: 0.068760, train loss: 0.267009, train accuracy: 92.71%, validation loss: 0.298202, validation accuracy: 92.30%\n",
      "step: 304000, batch loss: 0.063799, train loss: 0.265710, train accuracy: 92.83%, validation loss: 0.298079, validation accuracy: 92.20%\n",
      "step: 305000, batch loss: 0.071579, train loss: 0.267347, train accuracy: 92.57%, validation loss: 0.298885, validation accuracy: 92.04%\n",
      "step: 306000, batch loss: 0.297785, train loss: 0.267209, train accuracy: 92.67%, validation loss: 0.298111, validation accuracy: 91.92%\n",
      "step: 307000, batch loss: 0.029402, train loss: 0.266248, train accuracy: 92.73%, validation loss: 0.296843, validation accuracy: 91.84%\n",
      "step: 308000, batch loss: 0.139817, train loss: 0.270148, train accuracy: 92.60%, validation loss: 0.299635, validation accuracy: 92.10%\n",
      "step: 309000, batch loss: 0.622306, train loss: 0.267063, train accuracy: 92.64%, validation loss: 0.296921, validation accuracy: 91.60%\n",
      "step: 310000, batch loss: 0.111413, train loss: 0.265541, train accuracy: 92.71%, validation loss: 0.295012, validation accuracy: 92.20%\n",
      "step: 311000, batch loss: 0.421059, train loss: 0.264359, train accuracy: 92.80%, validation loss: 0.294693, validation accuracy: 92.16%\n",
      "step: 312000, batch loss: 0.661566, train loss: 0.270004, train accuracy: 92.48%, validation loss: 0.303159, validation accuracy: 91.92%\n",
      "step: 313000, batch loss: 0.007620, train loss: 0.265251, train accuracy: 92.72%, validation loss: 0.297251, validation accuracy: 92.24%\n",
      "step: 314000, batch loss: 0.098420, train loss: 0.264835, train accuracy: 92.76%, validation loss: 0.296416, validation accuracy: 91.98%\n",
      "step: 315000, batch loss: 0.734476, train loss: 0.265939, train accuracy: 92.73%, validation loss: 0.298204, validation accuracy: 91.84%\n",
      "step: 316000, batch loss: 0.349131, train loss: 0.264957, train accuracy: 92.74%, validation loss: 0.294987, validation accuracy: 92.06%\n",
      "step: 317000, batch loss: 0.236602, train loss: 0.264325, train accuracy: 92.78%, validation loss: 0.295588, validation accuracy: 92.04%\n",
      "step: 318000, batch loss: 0.593487, train loss: 0.263811, train accuracy: 92.77%, validation loss: 0.296593, validation accuracy: 92.06%\n",
      "step: 319000, batch loss: 0.305110, train loss: 0.264274, train accuracy: 92.77%, validation loss: 0.296392, validation accuracy: 92.02%\n",
      "step: 320000, batch loss: 0.111525, train loss: 0.264472, train accuracy: 92.77%, validation loss: 0.296675, validation accuracy: 92.12%\n",
      "step: 321000, batch loss: 0.028599, train loss: 0.264128, train accuracy: 92.68%, validation loss: 0.296298, validation accuracy: 91.88%\n",
      "step: 322000, batch loss: 0.141229, train loss: 0.263831, train accuracy: 92.74%, validation loss: 0.295168, validation accuracy: 92.12%\n",
      "step: 323000, batch loss: 0.440994, train loss: 0.263281, train accuracy: 92.75%, validation loss: 0.293926, validation accuracy: 92.04%\n",
      "step: 324000, batch loss: 0.043695, train loss: 0.264477, train accuracy: 92.71%, validation loss: 0.296425, validation accuracy: 91.94%\n",
      "step: 325000, batch loss: 0.830950, train loss: 0.263846, train accuracy: 92.73%, validation loss: 0.294568, validation accuracy: 92.28%\n",
      "step: 326000, batch loss: 0.014121, train loss: 0.263543, train accuracy: 92.78%, validation loss: 0.295479, validation accuracy: 92.10%\n",
      "step: 327000, batch loss: 0.125671, train loss: 0.263298, train accuracy: 92.79%, validation loss: 0.297266, validation accuracy: 92.14%\n",
      "step: 328000, batch loss: 0.007415, train loss: 0.264407, train accuracy: 92.80%, validation loss: 0.296119, validation accuracy: 92.22%\n",
      "step: 329000, batch loss: 0.082445, train loss: 0.263681, train accuracy: 92.76%, validation loss: 0.295671, validation accuracy: 91.84%\n",
      "step: 330000, batch loss: 0.027396, train loss: 0.263708, train accuracy: 92.74%, validation loss: 0.296164, validation accuracy: 92.06%\n",
      "step: 331000, batch loss: 0.810795, train loss: 0.263784, train accuracy: 92.84%, validation loss: 0.296930, validation accuracy: 92.06%\n",
      "step: 332000, batch loss: 0.248430, train loss: 0.265450, train accuracy: 92.65%, validation loss: 0.299021, validation accuracy: 92.12%\n",
      "step: 333000, batch loss: 0.462093, train loss: 0.262552, train accuracy: 92.80%, validation loss: 0.297415, validation accuracy: 91.94%\n",
      "step: 334000, batch loss: 0.053179, train loss: 0.263069, train accuracy: 92.80%, validation loss: 0.294377, validation accuracy: 92.14%\n",
      "step: 335000, batch loss: 0.698641, train loss: 0.263835, train accuracy: 92.72%, validation loss: 0.293395, validation accuracy: 92.22%\n",
      "step: 336000, batch loss: 0.138831, train loss: 0.263961, train accuracy: 92.69%, validation loss: 0.294565, validation accuracy: 92.22%\n",
      "step: 337000, batch loss: 0.169650, train loss: 0.264868, train accuracy: 92.62%, validation loss: 0.296242, validation accuracy: 92.02%\n",
      "step: 338000, batch loss: 0.001774, train loss: 0.262798, train accuracy: 92.79%, validation loss: 0.295006, validation accuracy: 92.12%\n",
      "step: 339000, batch loss: 0.901287, train loss: 0.262414, train accuracy: 92.87%, validation loss: 0.292954, validation accuracy: 92.18%\n",
      "step: 340000, batch loss: 0.255521, train loss: 0.261817, train accuracy: 92.81%, validation loss: 0.293738, validation accuracy: 92.18%\n",
      "step: 341000, batch loss: 0.047316, train loss: 0.262210, train accuracy: 92.82%, validation loss: 0.292546, validation accuracy: 92.24%\n",
      "step: 342000, batch loss: 0.167135, train loss: 0.261813, train accuracy: 92.91%, validation loss: 0.294482, validation accuracy: 92.06%\n",
      "step: 343000, batch loss: 0.403996, train loss: 0.260758, train accuracy: 92.86%, validation loss: 0.296022, validation accuracy: 92.08%\n",
      "step: 344000, batch loss: 0.798479, train loss: 0.261416, train accuracy: 92.83%, validation loss: 0.293270, validation accuracy: 92.08%\n",
      "step: 345000, batch loss: 0.033003, train loss: 0.260577, train accuracy: 92.88%, validation loss: 0.292583, validation accuracy: 92.06%\n",
      "step: 346000, batch loss: 0.277982, train loss: 0.260855, train accuracy: 92.90%, validation loss: 0.293433, validation accuracy: 92.06%\n",
      "step: 347000, batch loss: 0.010821, train loss: 0.261125, train accuracy: 92.90%, validation loss: 0.293048, validation accuracy: 92.32%\n",
      "step: 348000, batch loss: 0.059589, train loss: 0.265673, train accuracy: 92.75%, validation loss: 0.299231, validation accuracy: 92.08%\n",
      "step: 349000, batch loss: 0.034384, train loss: 0.260226, train accuracy: 92.90%, validation loss: 0.294042, validation accuracy: 92.20%\n",
      "step: 350000, batch loss: 0.160137, train loss: 0.261684, train accuracy: 92.83%, validation loss: 0.296031, validation accuracy: 92.26%\n",
      "step: 351000, batch loss: 0.070009, train loss: 0.264451, train accuracy: 92.65%, validation loss: 0.297880, validation accuracy: 92.26%\n",
      "step: 352000, batch loss: 0.121063, train loss: 0.259259, train accuracy: 93.00%, validation loss: 0.292550, validation accuracy: 92.24%\n",
      "step: 353000, batch loss: 0.019368, train loss: 0.260491, train accuracy: 92.89%, validation loss: 0.295018, validation accuracy: 92.02%\n",
      "step: 354000, batch loss: 0.031977, train loss: 0.263014, train accuracy: 92.73%, validation loss: 0.296723, validation accuracy: 92.08%\n",
      "step: 355000, batch loss: 0.076612, train loss: 0.261491, train accuracy: 92.78%, validation loss: 0.294194, validation accuracy: 92.18%\n",
      "step: 356000, batch loss: 0.040368, train loss: 0.258823, train accuracy: 92.93%, validation loss: 0.292274, validation accuracy: 92.28%\n",
      "step: 357000, batch loss: 0.833849, train loss: 0.262891, train accuracy: 92.72%, validation loss: 0.296336, validation accuracy: 92.02%\n",
      "step: 358000, batch loss: 0.064943, train loss: 0.259011, train accuracy: 92.87%, validation loss: 0.291769, validation accuracy: 92.08%\n",
      "step: 359000, batch loss: 0.132158, train loss: 0.260615, train accuracy: 92.83%, validation loss: 0.294942, validation accuracy: 92.06%\n",
      "step: 360000, batch loss: 0.433313, train loss: 0.258719, train accuracy: 92.95%, validation loss: 0.291083, validation accuracy: 92.10%\n",
      "step: 361000, batch loss: 0.913300, train loss: 0.261366, train accuracy: 92.88%, validation loss: 0.294011, validation accuracy: 91.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 362000, batch loss: 0.256218, train loss: 0.260435, train accuracy: 92.86%, validation loss: 0.293719, validation accuracy: 92.14%\n",
      "step: 363000, batch loss: 0.119500, train loss: 0.260085, train accuracy: 92.87%, validation loss: 0.291404, validation accuracy: 92.00%\n",
      "step: 364000, batch loss: 0.076321, train loss: 0.259652, train accuracy: 92.86%, validation loss: 0.292448, validation accuracy: 92.12%\n",
      "step: 365000, batch loss: 0.082586, train loss: 0.258921, train accuracy: 92.82%, validation loss: 0.291222, validation accuracy: 92.06%\n",
      "step: 366000, batch loss: 0.186302, train loss: 0.260408, train accuracy: 92.81%, validation loss: 0.292430, validation accuracy: 92.06%\n",
      "step: 367000, batch loss: 0.261681, train loss: 0.260250, train accuracy: 92.88%, validation loss: 0.295770, validation accuracy: 92.24%\n",
      "step: 368000, batch loss: 0.010776, train loss: 0.259487, train accuracy: 92.89%, validation loss: 0.293593, validation accuracy: 92.18%\n",
      "step: 369000, batch loss: 0.024068, train loss: 0.258348, train accuracy: 92.99%, validation loss: 0.290253, validation accuracy: 92.28%\n",
      "step: 370000, batch loss: 0.064914, train loss: 0.258645, train accuracy: 92.91%, validation loss: 0.291368, validation accuracy: 92.22%\n",
      "step: 371000, batch loss: 0.343431, train loss: 0.261121, train accuracy: 92.88%, validation loss: 0.292508, validation accuracy: 92.04%\n",
      "step: 372000, batch loss: 0.182916, train loss: 0.258297, train accuracy: 92.91%, validation loss: 0.290241, validation accuracy: 92.32%\n",
      "step: 373000, batch loss: 0.105909, train loss: 0.260369, train accuracy: 92.88%, validation loss: 0.292638, validation accuracy: 92.32%\n",
      "step: 374000, batch loss: 0.113602, train loss: 0.258722, train accuracy: 92.89%, validation loss: 0.292084, validation accuracy: 92.32%\n",
      "step: 375000, batch loss: 0.176710, train loss: 0.259185, train accuracy: 92.89%, validation loss: 0.293489, validation accuracy: 92.32%\n",
      "step: 376000, batch loss: 0.657393, train loss: 0.260152, train accuracy: 92.81%, validation loss: 0.293616, validation accuracy: 92.20%\n",
      "step: 377000, batch loss: 0.018608, train loss: 0.257910, train accuracy: 92.93%, validation loss: 0.291474, validation accuracy: 92.18%\n",
      "step: 378000, batch loss: 0.060860, train loss: 0.259995, train accuracy: 92.87%, validation loss: 0.296618, validation accuracy: 92.20%\n",
      "step: 379000, batch loss: 0.134134, train loss: 0.257028, train accuracy: 92.91%, validation loss: 0.290974, validation accuracy: 92.12%\n",
      "step: 380000, batch loss: 0.178889, train loss: 0.257874, train accuracy: 92.87%, validation loss: 0.294317, validation accuracy: 91.94%\n",
      "step: 381000, batch loss: 0.795907, train loss: 0.259281, train accuracy: 92.87%, validation loss: 0.293233, validation accuracy: 92.30%\n",
      "step: 382000, batch loss: 0.438940, train loss: 0.256845, train accuracy: 92.93%, validation loss: 0.291692, validation accuracy: 91.84%\n",
      "step: 383000, batch loss: 1.507670, train loss: 0.257229, train accuracy: 92.86%, validation loss: 0.292276, validation accuracy: 92.04%\n",
      "step: 384000, batch loss: 0.329284, train loss: 0.263981, train accuracy: 92.76%, validation loss: 0.295894, validation accuracy: 92.04%\n",
      "step: 385000, batch loss: 0.156077, train loss: 0.258668, train accuracy: 92.88%, validation loss: 0.293060, validation accuracy: 91.98%\n",
      "step: 386000, batch loss: 0.075030, train loss: 0.259864, train accuracy: 92.82%, validation loss: 0.295123, validation accuracy: 91.92%\n",
      "step: 387000, batch loss: 0.285947, train loss: 0.257271, train accuracy: 92.99%, validation loss: 0.289933, validation accuracy: 92.16%\n",
      "step: 388000, batch loss: 0.045651, train loss: 0.257918, train accuracy: 92.97%, validation loss: 0.291174, validation accuracy: 92.34%\n",
      "step: 389000, batch loss: 0.100120, train loss: 0.257227, train accuracy: 92.94%, validation loss: 0.292082, validation accuracy: 92.42%\n",
      "step: 390000, batch loss: 0.037868, train loss: 0.256902, train accuracy: 92.98%, validation loss: 0.291777, validation accuracy: 92.06%\n",
      "step: 391000, batch loss: 0.272471, train loss: 0.258056, train accuracy: 92.89%, validation loss: 0.292507, validation accuracy: 92.00%\n",
      "step: 392000, batch loss: 0.056655, train loss: 0.257140, train accuracy: 92.94%, validation loss: 0.293121, validation accuracy: 92.14%\n",
      "step: 393000, batch loss: 0.027922, train loss: 0.258689, train accuracy: 92.80%, validation loss: 0.293466, validation accuracy: 92.08%\n",
      "step: 394000, batch loss: 0.203276, train loss: 0.257341, train accuracy: 92.99%, validation loss: 0.291753, validation accuracy: 92.26%\n",
      "step: 395000, batch loss: 0.039049, train loss: 0.256350, train accuracy: 93.01%, validation loss: 0.290817, validation accuracy: 92.20%\n",
      "step: 396000, batch loss: 0.054353, train loss: 0.255514, train accuracy: 93.00%, validation loss: 0.290312, validation accuracy: 92.30%\n",
      "step: 397000, batch loss: 0.232843, train loss: 0.254945, train accuracy: 92.97%, validation loss: 0.289272, validation accuracy: 92.14%\n",
      "step: 398000, batch loss: 0.235783, train loss: 0.254854, train accuracy: 93.00%, validation loss: 0.289082, validation accuracy: 92.30%\n",
      "step: 399000, batch loss: 0.172120, train loss: 0.259763, train accuracy: 92.77%, validation loss: 0.293685, validation accuracy: 91.96%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr5RSPYwmfqWU6mE08SulVA/z/2+VucMAMmslAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd831f5b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression(\n",
    "    learning_rate=0.02,\n",
    "    steps = 400 * 1000,\n",
    "    l2_regularization_strength=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with LeRU and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunNeuralNetwork(learning_rate = 0.01,\n",
    "                     batch_size = 16,\n",
    "                     steps = 100 * 1000,\n",
    "                     sample = 5000,\n",
    "                     dropout_keep_prob = 1.0,\n",
    "                     hidden_sizes = [100, 75, 50]):\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer = inputs\n",
    "        for hidden_size in hidden_sizes:\n",
    "            input_size = layer.get_shape()[1].value\n",
    "            weights = tf.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1))\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[hidden_size]))\n",
    "            layer = tf.nn.relu(tf.matmul(layer, weights) + biases)\n",
    "            if dropout_keep_prob < 1.0:\n",
    "                layer = tf.nn.dropout(layer, keep_prob)\n",
    "\n",
    "        input_size = layer.get_shape()[1].value\n",
    "        weights = tf.Variable(tf.truncated_normal([input_size, num_classes], stddev=0.1))\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(layer, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "        # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)    \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # This must be called after all Variable definitions.\n",
    "        init_variables = tf.global_variables_initializer()\n",
    "\n",
    "    step_records = []\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label, keep_prob: dropout_keep_prob})\n",
    "            if step % sample == 0 or step == steps - 1:\n",
    "                batch_entropy = sess.run(\n",
    "                    cross_entropy,\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                train_entropy, train_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.train.images, labels: mnist.train.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                validation_entropy, validation_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                print 'step: %d, batch loss: %.4f, train loss: %f, train accuracy: %.2f%%, validation loss: %.4f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if train_entropy < 0.5:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.3103, train loss: 2.306453, train accuracy: 10.57%, validation loss: 2.3103, validation accuracy: 10.44%\n",
      "step: 5000, batch loss: 0.2332, train loss: 0.248508, train accuracy: 92.85%, validation loss: 0.2332, validation accuracy: 93.50%\n",
      "step: 10000, batch loss: 0.1719, train loss: 0.180477, train accuracy: 94.84%, validation loss: 0.1719, validation accuracy: 94.92%\n",
      "step: 15000, batch loss: 0.1422, train loss: 0.145580, train accuracy: 95.62%, validation loss: 0.1422, validation accuracy: 95.98%\n",
      "step: 20000, batch loss: 0.1234, train loss: 0.119749, train accuracy: 96.45%, validation loss: 0.1234, validation accuracy: 96.64%\n",
      "step: 25000, batch loss: 0.1110, train loss: 0.099360, train accuracy: 96.99%, validation loss: 0.1110, validation accuracy: 96.98%\n",
      "step: 30000, batch loss: 0.1024, train loss: 0.085546, train accuracy: 97.50%, validation loss: 0.1024, validation accuracy: 97.14%\n",
      "step: 35000, batch loss: 0.0971, train loss: 0.073554, train accuracy: 97.81%, validation loss: 0.0971, validation accuracy: 97.22%\n",
      "step: 40000, batch loss: 0.0899, train loss: 0.062689, train accuracy: 98.21%, validation loss: 0.0899, validation accuracy: 97.42%\n",
      "step: 45000, batch loss: 0.0960, train loss: 0.060623, train accuracy: 98.15%, validation loss: 0.0960, validation accuracy: 97.26%\n",
      "step: 50000, batch loss: 0.0818, train loss: 0.047470, train accuracy: 98.62%, validation loss: 0.0818, validation accuracy: 97.48%\n",
      "step: 55000, batch loss: 0.0813, train loss: 0.041583, train accuracy: 98.85%, validation loss: 0.0813, validation accuracy: 97.52%\n",
      "step: 60000, batch loss: 0.0827, train loss: 0.035013, train accuracy: 98.97%, validation loss: 0.0827, validation accuracy: 97.62%\n",
      "step: 65000, batch loss: 0.0845, train loss: 0.030761, train accuracy: 99.15%, validation loss: 0.0845, validation accuracy: 97.58%\n",
      "step: 70000, batch loss: 0.0818, train loss: 0.025492, train accuracy: 99.37%, validation loss: 0.0818, validation accuracy: 97.74%\n",
      "step: 75000, batch loss: 0.0857, train loss: 0.024484, train accuracy: 99.33%, validation loss: 0.0857, validation accuracy: 97.54%\n",
      "step: 80000, batch loss: 0.0860, train loss: 0.019233, train accuracy: 99.55%, validation loss: 0.0860, validation accuracy: 97.54%\n",
      "step: 85000, batch loss: 0.0866, train loss: 0.016642, train accuracy: 99.57%, validation loss: 0.0866, validation accuracy: 97.62%\n",
      "step: 90000, batch loss: 0.0883, train loss: 0.013224, train accuracy: 99.74%, validation loss: 0.0883, validation accuracy: 97.70%\n",
      "step: 95000, batch loss: 0.0912, train loss: 0.012174, train accuracy: 99.74%, validation loss: 0.0912, validation accuracy: 97.54%\n",
      "step: 99999, batch loss: 0.0989, train loss: 0.011625, train accuracy: 99.72%, validation loss: 0.0989, validation accuracy: 97.36%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd82bdc350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.3636, train loss: 2.367889, train accuracy: 14.22%, validation loss: 2.3636, validation accuracy: 14.40%\n",
      "step: 5000, batch loss: 0.3597, train loss: 0.366947, train accuracy: 89.84%, validation loss: 0.3597, validation accuracy: 90.20%\n",
      "step: 10000, batch loss: 0.2390, train loss: 0.242679, train accuracy: 92.96%, validation loss: 0.2390, validation accuracy: 93.26%\n",
      "step: 15000, batch loss: 0.1822, train loss: 0.185622, train accuracy: 94.60%, validation loss: 0.1822, validation accuracy: 94.96%\n",
      "step: 20000, batch loss: 0.1512, train loss: 0.153032, train accuracy: 95.48%, validation loss: 0.1512, validation accuracy: 95.90%\n",
      "step: 25000, batch loss: 0.1359, train loss: 0.131686, train accuracy: 96.14%, validation loss: 0.1359, validation accuracy: 96.26%\n",
      "step: 30000, batch loss: 0.1252, train loss: 0.114522, train accuracy: 96.67%, validation loss: 0.1252, validation accuracy: 96.72%\n",
      "step: 35000, batch loss: 0.1162, train loss: 0.100767, train accuracy: 97.06%, validation loss: 0.1162, validation accuracy: 96.94%\n",
      "step: 40000, batch loss: 0.1127, train loss: 0.091058, train accuracy: 97.39%, validation loss: 0.1127, validation accuracy: 97.04%\n",
      "step: 45000, batch loss: 0.1053, train loss: 0.082327, train accuracy: 97.67%, validation loss: 0.1053, validation accuracy: 97.18%\n",
      "step: 50000, batch loss: 0.1021, train loss: 0.075219, train accuracy: 97.85%, validation loss: 0.1021, validation accuracy: 97.30%\n",
      "step: 55000, batch loss: 0.0983, train loss: 0.069599, train accuracy: 98.03%, validation loss: 0.0983, validation accuracy: 97.30%\n",
      "step: 60000, batch loss: 0.0971, train loss: 0.065461, train accuracy: 98.16%, validation loss: 0.0971, validation accuracy: 97.46%\n",
      "step: 65000, batch loss: 0.0941, train loss: 0.061117, train accuracy: 98.25%, validation loss: 0.0941, validation accuracy: 97.50%\n",
      "step: 70000, batch loss: 0.0929, train loss: 0.056222, train accuracy: 98.40%, validation loss: 0.0929, validation accuracy: 97.46%\n",
      "step: 75000, batch loss: 0.0963, train loss: 0.052588, train accuracy: 98.47%, validation loss: 0.0963, validation accuracy: 97.44%\n",
      "step: 80000, batch loss: 0.0928, train loss: 0.049256, train accuracy: 98.57%, validation loss: 0.0928, validation accuracy: 97.62%\n",
      "step: 85000, batch loss: 0.0912, train loss: 0.046469, train accuracy: 98.67%, validation loss: 0.0912, validation accuracy: 97.80%\n",
      "step: 90000, batch loss: 0.0910, train loss: 0.044282, train accuracy: 98.71%, validation loss: 0.0910, validation accuracy: 97.66%\n",
      "step: 95000, batch loss: 0.0900, train loss: 0.042219, train accuracy: 98.78%, validation loss: 0.0900, validation accuracy: 97.78%\n",
      "step: 100000, batch loss: 0.0883, train loss: 0.041434, train accuracy: 98.78%, validation loss: 0.0883, validation accuracy: 97.66%\n",
      "step: 105000, batch loss: 0.0907, train loss: 0.037078, train accuracy: 98.89%, validation loss: 0.0907, validation accuracy: 97.68%\n",
      "step: 110000, batch loss: 0.0889, train loss: 0.035013, train accuracy: 98.99%, validation loss: 0.0889, validation accuracy: 97.72%\n",
      "step: 115000, batch loss: 0.0868, train loss: 0.033150, train accuracy: 99.04%, validation loss: 0.0868, validation accuracy: 97.88%\n",
      "step: 120000, batch loss: 0.0893, train loss: 0.031237, train accuracy: 99.09%, validation loss: 0.0893, validation accuracy: 97.82%\n",
      "step: 125000, batch loss: 0.0862, train loss: 0.030526, train accuracy: 99.11%, validation loss: 0.0862, validation accuracy: 97.98%\n",
      "step: 130000, batch loss: 0.0877, train loss: 0.028547, train accuracy: 99.17%, validation loss: 0.0877, validation accuracy: 97.84%\n",
      "step: 135000, batch loss: 0.0896, train loss: 0.027019, train accuracy: 99.23%, validation loss: 0.0896, validation accuracy: 97.78%\n",
      "step: 140000, batch loss: 0.0898, train loss: 0.026342, train accuracy: 99.21%, validation loss: 0.0898, validation accuracy: 97.78%\n",
      "step: 145000, batch loss: 0.0866, train loss: 0.025658, train accuracy: 99.25%, validation loss: 0.0866, validation accuracy: 97.82%\n",
      "step: 150000, batch loss: 0.0870, train loss: 0.023124, train accuracy: 99.35%, validation loss: 0.0870, validation accuracy: 97.76%\n",
      "step: 155000, batch loss: 0.0907, train loss: 0.022311, train accuracy: 99.36%, validation loss: 0.0907, validation accuracy: 97.82%\n",
      "step: 160000, batch loss: 0.0951, train loss: 0.021940, train accuracy: 99.36%, validation loss: 0.0951, validation accuracy: 97.62%\n",
      "step: 165000, batch loss: 0.0931, train loss: 0.021943, train accuracy: 99.35%, validation loss: 0.0931, validation accuracy: 97.68%\n",
      "step: 170000, batch loss: 0.0938, train loss: 0.019675, train accuracy: 99.39%, validation loss: 0.0938, validation accuracy: 97.88%\n",
      "step: 175000, batch loss: 0.0948, train loss: 0.019606, train accuracy: 99.42%, validation loss: 0.0948, validation accuracy: 97.80%\n",
      "step: 180000, batch loss: 0.0937, train loss: 0.018228, train accuracy: 99.45%, validation loss: 0.0937, validation accuracy: 97.76%\n",
      "step: 185000, batch loss: 0.0942, train loss: 0.017996, train accuracy: 99.47%, validation loss: 0.0942, validation accuracy: 97.78%\n",
      "step: 190000, batch loss: 0.0920, train loss: 0.016481, train accuracy: 99.50%, validation loss: 0.0920, validation accuracy: 97.98%\n",
      "step: 195000, batch loss: 0.0932, train loss: 0.016483, train accuracy: 99.49%, validation loss: 0.0932, validation accuracy: 97.86%\n",
      "step: 199999, batch loss: 0.0918, train loss: 0.015323, train accuracy: 99.58%, validation loss: 0.0918, validation accuracy: 97.78%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCn4RkQTz/wGUhBXFXW+G1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd829173d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork(\n",
    "    dropout_keep_prob=0.5,\n",
    "    steps = 200 * 1000,\n",
    "    hidden_sizes = [200, 150, 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network\n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html)\n",
    "- [An example in tensorflow: convolutional.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ConvConfig = collections.namedtuple('ConvConfig', ['patch', 'channel'])\n",
    "\n",
    "default_conv_configs = [\n",
    "    ConvConfig(patch=5, channel=32),\n",
    "    ConvConfig(patch=5, channel=64),\n",
    "]\n",
    "\n",
    "image_width = 28\n",
    "assert image_width * image_width == image_size\n",
    "\n",
    "def RunConvolutionalNN(\n",
    "    batch_size = 64,\n",
    "    learning_rate = 0.01,\n",
    "    steps = 10000,\n",
    "    sample = 100,\n",
    "    conv_configs=default_conv_configs,\n",
    "    fully_connect_sizes = [512],\n",
    "    dropout_keep_prob = 1.0,\n",
    "):\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        images = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer = tf.reshape(images, [-1, image_width, image_width, 1], name='reshaped')\n",
    "        print 'reshaped input shape:', layer.get_shape()\n",
    "        for i in xrange(len(conv_configs)):\n",
    "            config = conv_configs[i]\n",
    "            weight_shape = [config.patch, config.patch, layer.get_shape()[-1].value, config.channel]\n",
    "            weight = tf.Variable(tf.truncated_normal(weight_shape, stddev=0.1))\n",
    "            conv = tf.nn.conv2d(layer, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            bias = tf.Variable(tf.zeros([config.channel]))\n",
    "            conv = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "\n",
    "            # Is the order of relu and max_pool important?\n",
    "            layer = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            print 'conv layer.shape (%d):' % i, layer.get_shape()\n",
    "\n",
    "        layer = tf.reshape(layer, [-1, 7 * 7 * 64])\n",
    "        for i in xrange(len(fully_connect_sizes)):\n",
    "          size = fully_connect_sizes[i]\n",
    "          weight = tf.Variable(tf.truncated_normal([layer.get_shape()[1].value, size], stddev=0.1))\n",
    "          bias = tf.Variable(tf.constant(0.1, shape=[size]))\n",
    "          layer = tf.nn.relu(tf.matmul(layer, weight) + bias)\n",
    "          if dropout_keep_prob < 1.0:\n",
    "            layer = tf.nn.dropout(layer, keep_prob)\n",
    "          print 'fully-connected.shape (%d):' % i, layer.get_shape()\n",
    "\n",
    "        weight = tf.Variable(tf.truncated_normal([layer.get_shape()[1].value, num_classes], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(layer, weight) + bias\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "        # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)    \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        \n",
    "        correct_count = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.int32))\n",
    "\n",
    "        init_variables = tf.global_variables_initializer()\n",
    "        \n",
    "    step_records = []\n",
    "    validation_losses = []\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    # This is important to save RAM.\n",
    "    def evaluate_in_batches(dataset):\n",
    "        EVAL_BATCH_SIZE = 128\n",
    "        size = dataset.images.shape[0]\n",
    "        losses = []\n",
    "        correct_sum = 0\n",
    "        for start in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "            end = start + EVAL_BATCH_SIZE\n",
    "            if end > size:\n",
    "                end = size\n",
    "            loss, correct = sess.run([cross_entropy, correct_count], {images: dataset.images[start:end], labels:dataset.labels[start:end], keep_prob: 1.0})\n",
    "            losses.append(loss)\n",
    "            correct_sum += correct\n",
    "        return np.mean(losses), 1.0 * correct_sum / size\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "\n",
    "        for step in xrange(steps):\n",
    "            batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {images: batch_images, labels: batch_labels, keep_prob: dropout_keep_prob})\n",
    "            if step % sample == 0 or step == steps - 1:\n",
    "                batch_loss = cross_entropy.eval({images: batch_images, labels: batch_labels, keep_prob: 1.0})\n",
    "                print 'step: %d, batch loss: %f' % (step, batch_loss)\n",
    "                validation_loss, validation_accuracy = evaluate_in_batches(mnist.validation)\n",
    "                print 'step: %d, validation loss: %.4f, validation accuracy: %.2f%%' % (\n",
    "                    step, validation_loss, 100. * validation_accuracy)\n",
    "                if validation_loss < 0.5:\n",
    "                    step_records.append(step)\n",
    "                    validation_losses.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped input shape: (?, 28, 28, 1)\n",
      "conv layer.shape (0): (?, 14, 14, 32)\n",
      "conv layer.shape (1): (?, 7, 7, 64)\n",
      "fully-connected.shape (0): (?, 512)\n",
      "step: 0, batch loss: 3.610088\n",
      "step: 0, validation loss: 4.1555, validation accuracy: 8.46%\n",
      "step: 100, batch loss: 0.679053\n",
      "step: 100, validation loss: 0.5363, validation accuracy: 86.00%\n",
      "step: 200, batch loss: 0.319081\n",
      "step: 200, validation loss: 0.3148, validation accuracy: 91.22%\n",
      "step: 300, batch loss: 0.218794\n",
      "step: 300, validation loss: 0.2365, validation accuracy: 93.34%\n",
      "step: 400, batch loss: 0.178418\n",
      "step: 400, validation loss: 0.1963, validation accuracy: 94.40%\n",
      "step: 500, batch loss: 0.225837\n",
      "step: 500, validation loss: 0.1699, validation accuracy: 95.26%\n",
      "step: 600, batch loss: 0.258834\n",
      "step: 600, validation loss: 0.1500, validation accuracy: 95.82%\n",
      "step: 700, batch loss: 0.224436\n",
      "step: 700, validation loss: 0.1318, validation accuracy: 96.32%\n",
      "step: 800, batch loss: 0.084706\n",
      "step: 800, validation loss: 0.1252, validation accuracy: 96.42%\n",
      "step: 900, batch loss: 0.139207\n",
      "step: 900, validation loss: 0.1140, validation accuracy: 96.66%\n",
      "step: 1000, batch loss: 0.044892\n",
      "step: 1000, validation loss: 0.1076, validation accuracy: 96.92%\n",
      "step: 1100, batch loss: 0.126365\n",
      "step: 1100, validation loss: 0.0995, validation accuracy: 97.06%\n",
      "step: 1200, batch loss: 0.086129\n",
      "step: 1200, validation loss: 0.0955, validation accuracy: 97.08%\n",
      "step: 1300, batch loss: 0.086968\n",
      "step: 1300, validation loss: 0.0887, validation accuracy: 97.40%\n",
      "step: 1400, batch loss: 0.281812\n",
      "step: 1400, validation loss: 0.0835, validation accuracy: 97.56%\n",
      "step: 1500, batch loss: 0.037274\n",
      "step: 1500, validation loss: 0.0806, validation accuracy: 97.50%\n",
      "step: 1600, batch loss: 0.074932\n",
      "step: 1600, validation loss: 0.0770, validation accuracy: 97.58%\n",
      "step: 1700, batch loss: 0.053984\n",
      "step: 1700, validation loss: 0.0743, validation accuracy: 97.74%\n",
      "step: 1800, batch loss: 0.069744\n",
      "step: 1800, validation loss: 0.0735, validation accuracy: 97.64%\n",
      "step: 1900, batch loss: 0.016551\n",
      "step: 1900, validation loss: 0.0688, validation accuracy: 97.90%\n",
      "step: 2000, batch loss: 0.156132\n",
      "step: 2000, validation loss: 0.0647, validation accuracy: 97.98%\n",
      "step: 2100, batch loss: 0.030076\n",
      "step: 2100, validation loss: 0.0641, validation accuracy: 97.96%\n",
      "step: 2200, batch loss: 0.059528\n",
      "step: 2200, validation loss: 0.0640, validation accuracy: 97.98%\n",
      "step: 2300, batch loss: 0.032527\n",
      "step: 2300, validation loss: 0.0603, validation accuracy: 98.16%\n",
      "step: 2400, batch loss: 0.020096\n",
      "step: 2400, validation loss: 0.0567, validation accuracy: 98.26%\n",
      "step: 2500, batch loss: 0.052695\n",
      "step: 2500, validation loss: 0.0543, validation accuracy: 98.38%\n",
      "step: 2600, batch loss: 0.021221\n",
      "step: 2600, validation loss: 0.0560, validation accuracy: 98.24%\n",
      "step: 2700, batch loss: 0.035133\n",
      "step: 2700, validation loss: 0.0535, validation accuracy: 98.46%\n",
      "step: 2800, batch loss: 0.088517\n",
      "step: 2800, validation loss: 0.0536, validation accuracy: 98.48%\n",
      "step: 2900, batch loss: 0.065257\n",
      "step: 2900, validation loss: 0.0530, validation accuracy: 98.52%\n",
      "step: 3000, batch loss: 0.157532\n",
      "step: 3000, validation loss: 0.0528, validation accuracy: 98.34%\n",
      "step: 3100, batch loss: 0.058344\n",
      "step: 3100, validation loss: 0.0513, validation accuracy: 98.58%\n",
      "step: 3200, batch loss: 0.064242\n",
      "step: 3200, validation loss: 0.0503, validation accuracy: 98.32%\n",
      "step: 3300, batch loss: 0.016876\n",
      "step: 3300, validation loss: 0.0501, validation accuracy: 98.58%\n",
      "step: 3400, batch loss: 0.006816\n",
      "step: 3400, validation loss: 0.0468, validation accuracy: 98.58%\n",
      "step: 3500, batch loss: 0.052937\n",
      "step: 3500, validation loss: 0.0474, validation accuracy: 98.54%\n",
      "step: 3600, batch loss: 0.031659\n",
      "step: 3600, validation loss: 0.0467, validation accuracy: 98.64%\n",
      "step: 3700, batch loss: 0.088242\n",
      "step: 3700, validation loss: 0.0451, validation accuracy: 98.68%\n",
      "step: 3800, batch loss: 0.088260\n",
      "step: 3800, validation loss: 0.0438, validation accuracy: 98.88%\n",
      "step: 3900, batch loss: 0.014414\n",
      "step: 3900, validation loss: 0.0418, validation accuracy: 98.78%\n",
      "step: 4000, batch loss: 0.013384\n",
      "step: 4000, validation loss: 0.0417, validation accuracy: 98.76%\n",
      "step: 4100, batch loss: 0.040272\n",
      "step: 4100, validation loss: 0.0457, validation accuracy: 98.76%\n",
      "step: 4200, batch loss: 0.038327\n",
      "step: 4200, validation loss: 0.0420, validation accuracy: 98.78%\n",
      "step: 4300, batch loss: 0.008421\n",
      "step: 4300, validation loss: 0.0427, validation accuracy: 98.66%\n",
      "step: 4400, batch loss: 0.026924\n",
      "step: 4400, validation loss: 0.0437, validation accuracy: 98.66%\n",
      "step: 4500, batch loss: 0.010637\n",
      "step: 4500, validation loss: 0.0414, validation accuracy: 98.72%\n",
      "step: 4600, batch loss: 0.027593\n",
      "step: 4600, validation loss: 0.0408, validation accuracy: 98.82%\n",
      "step: 4700, batch loss: 0.026389\n",
      "step: 4700, validation loss: 0.0395, validation accuracy: 98.82%\n",
      "step: 4800, batch loss: 0.047848\n",
      "step: 4800, validation loss: 0.0417, validation accuracy: 98.64%\n",
      "step: 4900, batch loss: 0.007337\n",
      "step: 4900, validation loss: 0.0396, validation accuracy: 98.86%\n",
      "step: 5000, batch loss: 0.048781\n",
      "step: 5000, validation loss: 0.0401, validation accuracy: 98.84%\n",
      "step: 5100, batch loss: 0.103163\n",
      "step: 5100, validation loss: 0.0421, validation accuracy: 98.66%\n",
      "step: 5200, batch loss: 0.019429\n",
      "step: 5200, validation loss: 0.0386, validation accuracy: 98.86%\n",
      "step: 5300, batch loss: 0.007185\n",
      "step: 5300, validation loss: 0.0382, validation accuracy: 98.90%\n",
      "step: 5400, batch loss: 0.032517\n",
      "step: 5400, validation loss: 0.0401, validation accuracy: 98.86%\n",
      "step: 5500, batch loss: 0.005853\n",
      "step: 5500, validation loss: 0.0377, validation accuracy: 98.92%\n",
      "step: 5600, batch loss: 0.006042\n",
      "step: 5600, validation loss: 0.0365, validation accuracy: 98.92%\n",
      "step: 5700, batch loss: 0.061145\n",
      "step: 5700, validation loss: 0.0345, validation accuracy: 98.96%\n",
      "step: 5800, batch loss: 0.077576\n",
      "step: 5800, validation loss: 0.0374, validation accuracy: 99.04%\n",
      "step: 5900, batch loss: 0.009227\n",
      "step: 5900, validation loss: 0.0352, validation accuracy: 98.96%\n",
      "step: 6000, batch loss: 0.007471\n",
      "step: 6000, validation loss: 0.0384, validation accuracy: 98.88%\n",
      "step: 6100, batch loss: 0.020185\n",
      "step: 6100, validation loss: 0.0358, validation accuracy: 98.86%\n",
      "step: 6200, batch loss: 0.003126\n",
      "step: 6200, validation loss: 0.0353, validation accuracy: 98.94%\n",
      "step: 6300, batch loss: 0.055501\n",
      "step: 6300, validation loss: 0.0384, validation accuracy: 99.00%\n",
      "step: 6400, batch loss: 0.048281\n",
      "step: 6400, validation loss: 0.0380, validation accuracy: 98.88%\n",
      "step: 6500, batch loss: 0.094834\n",
      "step: 6500, validation loss: 0.0367, validation accuracy: 99.00%\n",
      "step: 6600, batch loss: 0.021742\n",
      "step: 6600, validation loss: 0.0399, validation accuracy: 98.68%\n",
      "step: 6700, batch loss: 0.001093\n",
      "step: 6700, validation loss: 0.0359, validation accuracy: 98.94%\n",
      "step: 6800, batch loss: 0.049920\n",
      "step: 6800, validation loss: 0.0334, validation accuracy: 99.06%\n",
      "step: 6900, batch loss: 0.049552\n",
      "step: 6900, validation loss: 0.0332, validation accuracy: 99.12%\n",
      "step: 7000, batch loss: 0.011219\n",
      "step: 7000, validation loss: 0.0333, validation accuracy: 99.12%\n",
      "step: 7100, batch loss: 0.006690\n",
      "step: 7100, validation loss: 0.0334, validation accuracy: 99.06%\n",
      "step: 7200, batch loss: 0.014304\n",
      "step: 7200, validation loss: 0.0325, validation accuracy: 99.08%\n",
      "step: 7300, batch loss: 0.001694\n",
      "step: 7300, validation loss: 0.0312, validation accuracy: 99.06%\n",
      "step: 7400, batch loss: 0.003437\n",
      "step: 7400, validation loss: 0.0346, validation accuracy: 98.98%\n",
      "step: 7500, batch loss: 0.005955\n",
      "step: 7500, validation loss: 0.0333, validation accuracy: 99.06%\n",
      "step: 7600, batch loss: 0.001890\n",
      "step: 7600, validation loss: 0.0315, validation accuracy: 99.08%\n",
      "step: 7700, batch loss: 0.002808\n",
      "step: 7700, validation loss: 0.0311, validation accuracy: 99.04%\n",
      "step: 7800, batch loss: 0.016560\n",
      "step: 7800, validation loss: 0.0322, validation accuracy: 99.00%\n",
      "step: 7900, batch loss: 0.000703\n",
      "step: 7900, validation loss: 0.0326, validation accuracy: 99.08%\n",
      "step: 8000, batch loss: 0.025410\n",
      "step: 8000, validation loss: 0.0319, validation accuracy: 99.16%\n",
      "step: 8100, batch loss: 0.002783\n",
      "step: 8100, validation loss: 0.0311, validation accuracy: 99.20%\n",
      "step: 8200, batch loss: 0.002668\n",
      "step: 8200, validation loss: 0.0326, validation accuracy: 99.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8300, batch loss: 0.001848\n",
      "step: 8300, validation loss: 0.0330, validation accuracy: 99.14%\n",
      "step: 8400, batch loss: 0.008309\n",
      "step: 8400, validation loss: 0.0326, validation accuracy: 99.14%\n",
      "step: 8500, batch loss: 0.003805\n",
      "step: 8500, validation loss: 0.0324, validation accuracy: 99.10%\n",
      "step: 8600, batch loss: 0.004567\n",
      "step: 8600, validation loss: 0.0304, validation accuracy: 99.00%\n",
      "step: 8700, batch loss: 0.008709\n",
      "step: 8700, validation loss: 0.0340, validation accuracy: 99.00%\n",
      "step: 8800, batch loss: 0.008041\n",
      "step: 8800, validation loss: 0.0328, validation accuracy: 99.10%\n",
      "step: 8900, batch loss: 0.010004\n",
      "step: 8900, validation loss: 0.0319, validation accuracy: 98.98%\n",
      "step: 9000, batch loss: 0.006477\n",
      "step: 9000, validation loss: 0.0300, validation accuracy: 99.16%\n",
      "step: 9100, batch loss: 0.002148\n",
      "step: 9100, validation loss: 0.0310, validation accuracy: 99.08%\n",
      "step: 9200, batch loss: 0.001864\n",
      "step: 9200, validation loss: 0.0320, validation accuracy: 99.08%\n",
      "step: 9300, batch loss: 0.001845\n",
      "step: 9300, validation loss: 0.0312, validation accuracy: 99.22%\n",
      "step: 9400, batch loss: 0.007546\n",
      "step: 9400, validation loss: 0.0316, validation accuracy: 99.08%\n",
      "step: 9500, batch loss: 0.005580\n",
      "step: 9500, validation loss: 0.0323, validation accuracy: 99.16%\n",
      "step: 9600, batch loss: 0.001664\n",
      "step: 9600, validation loss: 0.0323, validation accuracy: 99.12%\n",
      "step: 9700, batch loss: 0.018374\n",
      "step: 9700, validation loss: 0.0295, validation accuracy: 99.18%\n",
      "step: 9800, batch loss: 0.004289\n",
      "step: 9800, validation loss: 0.0300, validation accuracy: 99.10%\n",
      "step: 9900, batch loss: 0.003777\n",
      "step: 9900, validation loss: 0.0295, validation accuracy: 99.22%\n",
      "step: 9999, batch loss: 0.035445\n",
      "step: 9999, validation loss: 0.0289, validation accuracy: 99.12%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB5svFB/U9p/MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd83047850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunConvolutionalNN(dropout_keep_prob=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
