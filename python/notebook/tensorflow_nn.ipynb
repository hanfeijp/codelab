{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import contextlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "image_size = 28 * 28\n",
    "num_classes = 10\n",
    "assert mnist.train.images.shape[1] == image_size\n",
    "assert mnist.train.labels.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Logistic Regression\n",
    "References\n",
    "- [MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html)\n",
    "- [L2 Reguralization example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py)\n",
    "  - How is `tf.nn.l2_loss` different from square and reduce_sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RunLogsticRegression(learning_rate = 0.05,\n",
    "                         batch_size = 8,\n",
    "                         steps = 200 * 1000,\n",
    "                         sample = 1000,\n",
    "                         l2_regularization_strength = 0.0):\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    step_records = []\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "        weights = tf.Variable(tf.truncated_normal([image_size, num_classes], stddev=1), name='a')\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(inputs, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        loss = cross_entropy\n",
    "        if l2_regularization_strength > 0:\n",
    "            regularizer = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "            loss += l2_regularization_strength * regularizer\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        init_variables = tf.initialize_all_variables()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label})\n",
    "            if step % sample == 0:\n",
    "                batch_entropy, batch_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: batch_input, labels: batch_label})\n",
    "                train_entropy, train_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: mnist.train.images, labels: mnist.train.labels})\n",
    "                validation_entropy, validation_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                                   {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "                print 'step: %d, batch loss: %f, train loss: %f, train accuracy: %.2f%%, validation loss: %f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if step > 10000:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 11.875708, train loss: 12.878407, train accuracy: 6.91%, validation loss: 13.036783, validation accuracy: 7.24%\n",
      "step: 1000, batch loss: 1.344817, train loss: 1.242410, train accuracy: 73.33%, validation loss: 1.190305, validation accuracy: 74.60%\n",
      "step: 2000, batch loss: 0.971194, train loss: 0.914998, train accuracy: 80.01%, validation loss: 0.888195, validation accuracy: 81.42%\n",
      "step: 3000, batch loss: 0.940881, train loss: 0.773140, train accuracy: 82.70%, validation loss: 0.756958, validation accuracy: 83.86%\n",
      "step: 4000, batch loss: 0.229205, train loss: 0.721512, train accuracy: 83.65%, validation loss: 0.706955, validation accuracy: 84.42%\n",
      "step: 5000, batch loss: 1.263245, train loss: 0.653587, train accuracy: 84.86%, validation loss: 0.651343, validation accuracy: 85.58%\n",
      "step: 6000, batch loss: 0.097111, train loss: 0.596512, train accuracy: 85.96%, validation loss: 0.593828, validation accuracy: 86.40%\n",
      "step: 7000, batch loss: 1.366979, train loss: 0.595970, train accuracy: 86.25%, validation loss: 0.593137, validation accuracy: 86.72%\n",
      "step: 8000, batch loss: 0.722569, train loss: 0.554765, train accuracy: 86.87%, validation loss: 0.542987, validation accuracy: 87.24%\n",
      "step: 9000, batch loss: 0.104795, train loss: 0.523562, train accuracy: 87.47%, validation loss: 0.520222, validation accuracy: 88.06%\n",
      "step: 10000, batch loss: 1.429459, train loss: 0.534259, train accuracy: 87.12%, validation loss: 0.525548, validation accuracy: 87.42%\n",
      "step: 11000, batch loss: 0.011713, train loss: 0.501055, train accuracy: 87.95%, validation loss: 0.503917, validation accuracy: 88.24%\n",
      "step: 12000, batch loss: 0.541579, train loss: 0.501262, train accuracy: 87.62%, validation loss: 0.502403, validation accuracy: 88.34%\n",
      "step: 13000, batch loss: 0.177723, train loss: 0.473190, train accuracy: 88.34%, validation loss: 0.476128, validation accuracy: 88.74%\n",
      "step: 14000, batch loss: 0.677690, train loss: 0.478168, train accuracy: 88.24%, validation loss: 0.484825, validation accuracy: 88.86%\n",
      "step: 15000, batch loss: 0.039822, train loss: 0.458658, train accuracy: 88.44%, validation loss: 0.459942, validation accuracy: 88.86%\n",
      "step: 16000, batch loss: 0.543980, train loss: 0.454868, train accuracy: 88.59%, validation loss: 0.457432, validation accuracy: 89.10%\n",
      "step: 17000, batch loss: 0.007314, train loss: 0.451308, train accuracy: 88.65%, validation loss: 0.452161, validation accuracy: 89.42%\n",
      "step: 18000, batch loss: 1.690646, train loss: 0.441517, train accuracy: 88.85%, validation loss: 0.442805, validation accuracy: 89.54%\n",
      "step: 19000, batch loss: 0.029487, train loss: 0.428993, train accuracy: 89.08%, validation loss: 0.432959, validation accuracy: 89.46%\n",
      "step: 20000, batch loss: 0.380414, train loss: 0.423520, train accuracy: 89.27%, validation loss: 0.427864, validation accuracy: 89.66%\n",
      "step: 21000, batch loss: 0.010101, train loss: 0.411961, train accuracy: 89.55%, validation loss: 0.417536, validation accuracy: 89.88%\n",
      "step: 22000, batch loss: 0.012842, train loss: 0.405858, train accuracy: 89.60%, validation loss: 0.416689, validation accuracy: 89.82%\n",
      "step: 23000, batch loss: 0.068039, train loss: 0.410099, train accuracy: 89.40%, validation loss: 0.415598, validation accuracy: 89.78%\n",
      "step: 24000, batch loss: 0.040061, train loss: 0.403194, train accuracy: 89.69%, validation loss: 0.410506, validation accuracy: 90.06%\n",
      "step: 25000, batch loss: 0.690736, train loss: 0.396222, train accuracy: 89.68%, validation loss: 0.405707, validation accuracy: 89.98%\n",
      "step: 26000, batch loss: 0.305894, train loss: 0.388049, train accuracy: 89.89%, validation loss: 0.392535, validation accuracy: 90.20%\n",
      "step: 27000, batch loss: 0.015190, train loss: 0.384216, train accuracy: 90.16%, validation loss: 0.392909, validation accuracy: 90.16%\n",
      "step: 28000, batch loss: 0.031233, train loss: 0.384473, train accuracy: 90.07%, validation loss: 0.394593, validation accuracy: 90.10%\n",
      "step: 29000, batch loss: 0.757419, train loss: 0.388904, train accuracy: 89.93%, validation loss: 0.400288, validation accuracy: 90.06%\n",
      "step: 30000, batch loss: 0.258700, train loss: 0.383642, train accuracy: 89.81%, validation loss: 0.391768, validation accuracy: 90.14%\n",
      "step: 31000, batch loss: 0.148558, train loss: 0.381570, train accuracy: 89.91%, validation loss: 0.386027, validation accuracy: 90.28%\n",
      "step: 32000, batch loss: 0.151526, train loss: 0.372976, train accuracy: 90.14%, validation loss: 0.386437, validation accuracy: 90.16%\n",
      "step: 33000, batch loss: 0.282092, train loss: 0.365228, train accuracy: 90.41%, validation loss: 0.374827, validation accuracy: 90.30%\n",
      "step: 34000, batch loss: 0.406444, train loss: 0.360321, train accuracy: 90.37%, validation loss: 0.372535, validation accuracy: 90.36%\n",
      "step: 35000, batch loss: 1.024041, train loss: 0.369232, train accuracy: 90.33%, validation loss: 0.382025, validation accuracy: 90.06%\n",
      "step: 36000, batch loss: 0.285876, train loss: 0.363631, train accuracy: 90.42%, validation loss: 0.377891, validation accuracy: 90.28%\n",
      "step: 37000, batch loss: 0.436917, train loss: 0.358947, train accuracy: 90.50%, validation loss: 0.375421, validation accuracy: 90.50%\n",
      "step: 38000, batch loss: 0.167768, train loss: 0.350652, train accuracy: 90.71%, validation loss: 0.366384, validation accuracy: 90.58%\n",
      "step: 39000, batch loss: 0.384917, train loss: 0.353138, train accuracy: 90.56%, validation loss: 0.370435, validation accuracy: 90.26%\n",
      "step: 40000, batch loss: 0.300859, train loss: 0.348255, train accuracy: 90.85%, validation loss: 0.364425, validation accuracy: 90.78%\n",
      "step: 41000, batch loss: 0.060856, train loss: 0.351980, train accuracy: 90.53%, validation loss: 0.367988, validation accuracy: 90.38%\n",
      "step: 42000, batch loss: 0.312250, train loss: 0.354692, train accuracy: 90.59%, validation loss: 0.366880, validation accuracy: 90.82%\n",
      "step: 43000, batch loss: 0.078118, train loss: 0.338303, train accuracy: 91.00%, validation loss: 0.354085, validation accuracy: 90.88%\n",
      "step: 44000, batch loss: 0.104902, train loss: 0.338592, train accuracy: 91.01%, validation loss: 0.355618, validation accuracy: 90.88%\n",
      "step: 45000, batch loss: 0.048950, train loss: 0.337564, train accuracy: 91.04%, validation loss: 0.351723, validation accuracy: 91.14%\n",
      "step: 46000, batch loss: 0.165823, train loss: 0.351335, train accuracy: 90.45%, validation loss: 0.364033, validation accuracy: 90.40%\n",
      "step: 47000, batch loss: 0.199204, train loss: 0.339649, train accuracy: 90.80%, validation loss: 0.357074, validation accuracy: 90.58%\n",
      "step: 48000, batch loss: 0.258964, train loss: 0.336924, train accuracy: 90.97%, validation loss: 0.352837, validation accuracy: 90.84%\n",
      "step: 49000, batch loss: 1.414614, train loss: 0.331426, train accuracy: 91.04%, validation loss: 0.347895, validation accuracy: 90.92%\n",
      "step: 50000, batch loss: 0.028385, train loss: 0.338424, train accuracy: 90.80%, validation loss: 0.354180, validation accuracy: 90.66%\n",
      "step: 51000, batch loss: 0.176873, train loss: 0.324096, train accuracy: 91.25%, validation loss: 0.343956, validation accuracy: 91.14%\n",
      "step: 52000, batch loss: 0.163931, train loss: 0.334690, train accuracy: 90.90%, validation loss: 0.352700, validation accuracy: 90.92%\n",
      "step: 53000, batch loss: 0.025671, train loss: 0.325134, train accuracy: 91.21%, validation loss: 0.339257, validation accuracy: 91.36%\n",
      "step: 54000, batch loss: 0.033478, train loss: 0.328343, train accuracy: 91.17%, validation loss: 0.343927, validation accuracy: 91.10%\n",
      "step: 55000, batch loss: 0.053056, train loss: 0.320340, train accuracy: 91.37%, validation loss: 0.341747, validation accuracy: 91.08%\n",
      "step: 56000, batch loss: 0.730495, train loss: 0.319927, train accuracy: 91.47%, validation loss: 0.338421, validation accuracy: 91.06%\n",
      "step: 57000, batch loss: 0.283399, train loss: 0.328582, train accuracy: 91.29%, validation loss: 0.343569, validation accuracy: 91.16%\n",
      "step: 58000, batch loss: 0.006563, train loss: 0.333043, train accuracy: 90.99%, validation loss: 0.349576, validation accuracy: 90.56%\n",
      "step: 59000, batch loss: 0.599126, train loss: 0.339120, train accuracy: 90.56%, validation loss: 0.358207, validation accuracy: 90.54%\n",
      "step: 60000, batch loss: 0.364832, train loss: 0.315270, train accuracy: 91.51%, validation loss: 0.337620, validation accuracy: 91.34%\n",
      "step: 61000, batch loss: 0.156183, train loss: 0.315761, train accuracy: 91.36%, validation loss: 0.336434, validation accuracy: 91.32%\n",
      "step: 62000, batch loss: 0.060540, train loss: 0.317645, train accuracy: 91.37%, validation loss: 0.338299, validation accuracy: 91.24%\n",
      "step: 63000, batch loss: 0.238821, train loss: 0.314052, train accuracy: 91.39%, validation loss: 0.333329, validation accuracy: 91.08%\n",
      "step: 64000, batch loss: 0.219413, train loss: 0.332114, train accuracy: 91.02%, validation loss: 0.349302, validation accuracy: 90.78%\n",
      "step: 65000, batch loss: 0.514408, train loss: 0.322801, train accuracy: 91.07%, validation loss: 0.341903, validation accuracy: 90.88%\n",
      "step: 66000, batch loss: 0.026386, train loss: 0.311788, train accuracy: 91.51%, validation loss: 0.330047, validation accuracy: 91.58%\n",
      "step: 67000, batch loss: 0.083285, train loss: 0.307036, train accuracy: 91.73%, validation loss: 0.329344, validation accuracy: 91.60%\n",
      "step: 68000, batch loss: 0.146158, train loss: 0.308944, train accuracy: 91.65%, validation loss: 0.330353, validation accuracy: 91.26%\n",
      "step: 69000, batch loss: 0.051405, train loss: 0.301584, train accuracy: 91.88%, validation loss: 0.323504, validation accuracy: 91.34%\n",
      "step: 70000, batch loss: 0.563535, train loss: 0.310651, train accuracy: 91.63%, validation loss: 0.332586, validation accuracy: 91.44%\n",
      "step: 71000, batch loss: 0.062202, train loss: 0.302430, train accuracy: 91.79%, validation loss: 0.324960, validation accuracy: 91.60%\n",
      "step: 72000, batch loss: 0.025670, train loss: 0.311355, train accuracy: 91.48%, validation loss: 0.331811, validation accuracy: 91.52%\n",
      "step: 73000, batch loss: 0.351820, train loss: 0.303361, train accuracy: 91.84%, validation loss: 0.324617, validation accuracy: 91.40%\n",
      "step: 74000, batch loss: 0.176994, train loss: 0.305764, train accuracy: 91.67%, validation loss: 0.330155, validation accuracy: 91.34%\n",
      "step: 75000, batch loss: 0.116263, train loss: 0.301713, train accuracy: 91.86%, validation loss: 0.325162, validation accuracy: 91.52%\n",
      "step: 76000, batch loss: 0.384650, train loss: 0.297408, train accuracy: 91.95%, validation loss: 0.320072, validation accuracy: 91.56%\n",
      "step: 77000, batch loss: 0.126323, train loss: 0.305055, train accuracy: 91.63%, validation loss: 0.328874, validation accuracy: 91.36%\n",
      "step: 78000, batch loss: 0.189790, train loss: 0.295892, train accuracy: 92.01%, validation loss: 0.315691, validation accuracy: 91.76%\n",
      "step: 79000, batch loss: 0.837114, train loss: 0.312054, train accuracy: 91.19%, validation loss: 0.326686, validation accuracy: 91.40%\n",
      "step: 80000, batch loss: 0.041228, train loss: 0.294347, train accuracy: 91.96%, validation loss: 0.317191, validation accuracy: 91.80%\n",
      "step: 81000, batch loss: 0.419925, train loss: 0.303556, train accuracy: 91.87%, validation loss: 0.330269, validation accuracy: 91.50%\n",
      "step: 82000, batch loss: 0.271393, train loss: 0.291790, train accuracy: 92.15%, validation loss: 0.319619, validation accuracy: 91.72%\n",
      "step: 83000, batch loss: 0.072562, train loss: 0.297066, train accuracy: 91.97%, validation loss: 0.325624, validation accuracy: 91.26%\n",
      "step: 84000, batch loss: 0.009078, train loss: 0.292402, train accuracy: 91.99%, validation loss: 0.320603, validation accuracy: 91.48%\n",
      "step: 85000, batch loss: 0.421914, train loss: 0.292411, train accuracy: 91.90%, validation loss: 0.318919, validation accuracy: 91.60%\n",
      "step: 86000, batch loss: 0.037675, train loss: 0.297603, train accuracy: 91.84%, validation loss: 0.322597, validation accuracy: 91.42%\n",
      "step: 87000, batch loss: 0.096209, train loss: 0.295627, train accuracy: 91.89%, validation loss: 0.320156, validation accuracy: 91.58%\n",
      "step: 88000, batch loss: 0.037595, train loss: 0.290307, train accuracy: 92.11%, validation loss: 0.316474, validation accuracy: 91.52%\n",
      "step: 89000, batch loss: 0.152517, train loss: 0.294222, train accuracy: 91.92%, validation loss: 0.318869, validation accuracy: 91.56%\n",
      "step: 90000, batch loss: 0.056313, train loss: 0.290673, train accuracy: 92.19%, validation loss: 0.316552, validation accuracy: 92.12%\n",
      "step: 91000, batch loss: 0.981634, train loss: 0.293588, train accuracy: 91.92%, validation loss: 0.316561, validation accuracy: 91.42%\n",
      "step: 92000, batch loss: 0.089757, train loss: 0.289862, train accuracy: 92.02%, validation loss: 0.316645, validation accuracy: 91.34%\n",
      "step: 93000, batch loss: 0.100885, train loss: 0.285077, train accuracy: 92.23%, validation loss: 0.312374, validation accuracy: 91.60%\n",
      "step: 94000, batch loss: 0.501015, train loss: 0.292127, train accuracy: 91.94%, validation loss: 0.317310, validation accuracy: 91.50%\n",
      "step: 95000, batch loss: 0.046077, train loss: 0.292823, train accuracy: 91.89%, validation loss: 0.314060, validation accuracy: 91.50%\n",
      "step: 96000, batch loss: 0.183422, train loss: 0.286226, train accuracy: 92.20%, validation loss: 0.312681, validation accuracy: 91.72%\n",
      "step: 97000, batch loss: 0.051065, train loss: 0.289018, train accuracy: 92.16%, validation loss: 0.313610, validation accuracy: 91.58%\n",
      "step: 98000, batch loss: 0.590273, train loss: 0.288427, train accuracy: 92.04%, validation loss: 0.316534, validation accuracy: 91.64%\n",
      "step: 99000, batch loss: 0.030132, train loss: 0.288342, train accuracy: 92.02%, validation loss: 0.317981, validation accuracy: 91.32%\n",
      "step: 100000, batch loss: 0.041163, train loss: 0.288578, train accuracy: 91.95%, validation loss: 0.317489, validation accuracy: 91.56%\n",
      "step: 101000, batch loss: 0.428594, train loss: 0.284352, train accuracy: 92.15%, validation loss: 0.312834, validation accuracy: 91.54%\n",
      "step: 102000, batch loss: 0.090052, train loss: 0.282436, train accuracy: 92.45%, validation loss: 0.308179, validation accuracy: 91.86%\n",
      "step: 103000, batch loss: 0.234282, train loss: 0.288861, train accuracy: 91.93%, validation loss: 0.319426, validation accuracy: 91.24%\n",
      "step: 104000, batch loss: 0.182110, train loss: 0.281656, train accuracy: 92.30%, validation loss: 0.309104, validation accuracy: 91.70%\n",
      "step: 105000, batch loss: 0.143799, train loss: 0.277972, train accuracy: 92.50%, validation loss: 0.307461, validation accuracy: 92.00%\n",
      "step: 106000, batch loss: 0.035745, train loss: 0.288258, train accuracy: 92.05%, validation loss: 0.311750, validation accuracy: 91.80%\n",
      "step: 107000, batch loss: 0.052714, train loss: 0.283301, train accuracy: 92.21%, validation loss: 0.308966, validation accuracy: 91.46%\n",
      "step: 108000, batch loss: 0.086979, train loss: 0.284164, train accuracy: 92.15%, validation loss: 0.315493, validation accuracy: 91.50%\n",
      "step: 109000, batch loss: 0.055076, train loss: 0.278596, train accuracy: 92.43%, validation loss: 0.307839, validation accuracy: 91.82%\n",
      "step: 110000, batch loss: 0.135073, train loss: 0.279191, train accuracy: 92.30%, validation loss: 0.306814, validation accuracy: 91.84%\n",
      "step: 111000, batch loss: 0.016641, train loss: 0.280491, train accuracy: 92.35%, validation loss: 0.306141, validation accuracy: 91.74%\n",
      "step: 112000, batch loss: 0.053428, train loss: 0.285423, train accuracy: 92.31%, validation loss: 0.313195, validation accuracy: 91.90%\n",
      "step: 113000, batch loss: 0.040247, train loss: 0.291989, train accuracy: 91.99%, validation loss: 0.318870, validation accuracy: 91.38%\n",
      "step: 114000, batch loss: 0.283487, train loss: 0.277295, train accuracy: 92.42%, validation loss: 0.305008, validation accuracy: 91.82%\n",
      "step: 115000, batch loss: 0.277645, train loss: 0.287034, train accuracy: 92.07%, validation loss: 0.316354, validation accuracy: 91.58%\n",
      "step: 116000, batch loss: 0.290183, train loss: 0.293527, train accuracy: 91.65%, validation loss: 0.317829, validation accuracy: 91.52%\n",
      "step: 117000, batch loss: 0.426445, train loss: 0.276292, train accuracy: 92.52%, validation loss: 0.304807, validation accuracy: 91.88%\n",
      "step: 118000, batch loss: 0.041464, train loss: 0.280191, train accuracy: 92.28%, validation loss: 0.308511, validation accuracy: 91.48%\n",
      "step: 119000, batch loss: 0.036546, train loss: 0.278358, train accuracy: 92.35%, validation loss: 0.307654, validation accuracy: 91.60%\n",
      "step: 120000, batch loss: 0.095668, train loss: 0.279384, train accuracy: 92.26%, validation loss: 0.311972, validation accuracy: 91.34%\n",
      "step: 121000, batch loss: 0.102159, train loss: 0.279731, train accuracy: 92.23%, validation loss: 0.311806, validation accuracy: 91.56%\n",
      "step: 122000, batch loss: 0.041221, train loss: 0.286893, train accuracy: 91.93%, validation loss: 0.313187, validation accuracy: 91.76%\n",
      "step: 123000, batch loss: 0.063551, train loss: 0.277025, train accuracy: 92.22%, validation loss: 0.308688, validation accuracy: 91.50%\n",
      "step: 124000, batch loss: 1.012365, train loss: 0.277729, train accuracy: 92.33%, validation loss: 0.309528, validation accuracy: 91.68%\n",
      "step: 125000, batch loss: 0.073029, train loss: 0.279614, train accuracy: 92.23%, validation loss: 0.311586, validation accuracy: 91.48%\n",
      "step: 126000, batch loss: 0.132900, train loss: 0.275864, train accuracy: 92.40%, validation loss: 0.304308, validation accuracy: 91.90%\n",
      "step: 127000, batch loss: 0.039689, train loss: 0.271575, train accuracy: 92.55%, validation loss: 0.302640, validation accuracy: 92.08%\n",
      "step: 128000, batch loss: 0.205343, train loss: 0.271384, train accuracy: 92.60%, validation loss: 0.298798, validation accuracy: 92.04%\n",
      "step: 129000, batch loss: 0.229060, train loss: 0.273248, train accuracy: 92.33%, validation loss: 0.304305, validation accuracy: 91.80%\n",
      "step: 130000, batch loss: 0.029620, train loss: 0.271181, train accuracy: 92.58%, validation loss: 0.301945, validation accuracy: 92.14%\n",
      "step: 131000, batch loss: 0.235779, train loss: 0.272641, train accuracy: 92.60%, validation loss: 0.305904, validation accuracy: 92.04%\n",
      "step: 132000, batch loss: 0.011459, train loss: 0.275515, train accuracy: 92.31%, validation loss: 0.308002, validation accuracy: 91.82%\n",
      "step: 133000, batch loss: 0.242112, train loss: 0.278675, train accuracy: 92.17%, validation loss: 0.308242, validation accuracy: 91.58%\n",
      "step: 134000, batch loss: 0.546513, train loss: 0.279143, train accuracy: 92.06%, validation loss: 0.306287, validation accuracy: 91.60%\n",
      "step: 135000, batch loss: 0.021376, train loss: 0.270503, train accuracy: 92.53%, validation loss: 0.302723, validation accuracy: 91.70%\n",
      "step: 136000, batch loss: 0.024168, train loss: 0.275515, train accuracy: 92.36%, validation loss: 0.307050, validation accuracy: 91.60%\n",
      "step: 137000, batch loss: 0.012649, train loss: 0.271088, train accuracy: 92.48%, validation loss: 0.304530, validation accuracy: 92.06%\n",
      "step: 138000, batch loss: 0.717520, train loss: 0.273392, train accuracy: 92.46%, validation loss: 0.305882, validation accuracy: 91.52%\n",
      "step: 139000, batch loss: 0.046877, train loss: 0.266034, train accuracy: 92.69%, validation loss: 0.296672, validation accuracy: 92.34%\n",
      "step: 140000, batch loss: 0.045952, train loss: 0.268373, train accuracy: 92.58%, validation loss: 0.305148, validation accuracy: 91.82%\n",
      "step: 141000, batch loss: 0.039195, train loss: 0.273628, train accuracy: 92.39%, validation loss: 0.308667, validation accuracy: 91.66%\n",
      "step: 142000, batch loss: 0.293057, train loss: 0.266148, train accuracy: 92.61%, validation loss: 0.298563, validation accuracy: 92.00%\n",
      "step: 143000, batch loss: 0.164721, train loss: 0.268702, train accuracy: 92.59%, validation loss: 0.299140, validation accuracy: 91.80%\n",
      "step: 144000, batch loss: 0.400150, train loss: 0.267476, train accuracy: 92.60%, validation loss: 0.298828, validation accuracy: 91.86%\n",
      "step: 145000, batch loss: 0.564525, train loss: 0.279189, train accuracy: 92.08%, validation loss: 0.310263, validation accuracy: 91.64%\n",
      "step: 146000, batch loss: 0.338167, train loss: 0.266864, train accuracy: 92.63%, validation loss: 0.296995, validation accuracy: 91.72%\n",
      "step: 147000, batch loss: 0.074266, train loss: 0.268245, train accuracy: 92.68%, validation loss: 0.299481, validation accuracy: 92.20%\n",
      "step: 148000, batch loss: 0.035933, train loss: 0.265023, train accuracy: 92.62%, validation loss: 0.300002, validation accuracy: 91.96%\n",
      "step: 149000, batch loss: 0.056218, train loss: 0.267393, train accuracy: 92.59%, validation loss: 0.298368, validation accuracy: 92.20%\n",
      "step: 150000, batch loss: 0.003219, train loss: 0.266242, train accuracy: 92.61%, validation loss: 0.299131, validation accuracy: 92.06%\n",
      "step: 151000, batch loss: 0.145799, train loss: 0.265975, train accuracy: 92.65%, validation loss: 0.300087, validation accuracy: 91.82%\n",
      "step: 152000, batch loss: 0.109538, train loss: 0.270302, train accuracy: 92.63%, validation loss: 0.304632, validation accuracy: 91.74%\n",
      "step: 153000, batch loss: 0.130379, train loss: 0.273008, train accuracy: 92.39%, validation loss: 0.303006, validation accuracy: 91.86%\n",
      "step: 154000, batch loss: 1.559688, train loss: 0.263330, train accuracy: 92.85%, validation loss: 0.295573, validation accuracy: 91.82%\n",
      "step: 155000, batch loss: 0.023592, train loss: 0.269361, train accuracy: 92.55%, validation loss: 0.301123, validation accuracy: 92.08%\n",
      "step: 156000, batch loss: 0.063350, train loss: 0.269036, train accuracy: 92.55%, validation loss: 0.300930, validation accuracy: 92.00%\n",
      "step: 157000, batch loss: 0.073971, train loss: 0.266371, train accuracy: 92.62%, validation loss: 0.298696, validation accuracy: 92.08%\n",
      "step: 158000, batch loss: 0.089389, train loss: 0.268552, train accuracy: 92.72%, validation loss: 0.303171, validation accuracy: 91.92%\n",
      "step: 159000, batch loss: 1.159910, train loss: 0.269937, train accuracy: 92.57%, validation loss: 0.299582, validation accuracy: 91.86%\n",
      "step: 160000, batch loss: 0.165832, train loss: 0.265917, train accuracy: 92.55%, validation loss: 0.296104, validation accuracy: 92.00%\n",
      "step: 161000, batch loss: 0.024760, train loss: 0.263975, train accuracy: 92.72%, validation loss: 0.298284, validation accuracy: 91.88%\n",
      "step: 162000, batch loss: 0.234591, train loss: 0.266106, train accuracy: 92.61%, validation loss: 0.298510, validation accuracy: 91.80%\n",
      "step: 163000, batch loss: 0.008980, train loss: 0.266825, train accuracy: 92.56%, validation loss: 0.301110, validation accuracy: 91.62%\n",
      "step: 164000, batch loss: 0.141295, train loss: 0.263896, train accuracy: 92.83%, validation loss: 0.298631, validation accuracy: 92.02%\n",
      "step: 165000, batch loss: 0.127181, train loss: 0.263771, train accuracy: 92.65%, validation loss: 0.302647, validation accuracy: 91.70%\n",
      "step: 166000, batch loss: 0.022006, train loss: 0.264590, train accuracy: 92.57%, validation loss: 0.295802, validation accuracy: 92.28%\n",
      "step: 167000, batch loss: 0.064074, train loss: 0.265214, train accuracy: 92.77%, validation loss: 0.298729, validation accuracy: 91.98%\n",
      "step: 168000, batch loss: 0.750578, train loss: 0.266430, train accuracy: 92.58%, validation loss: 0.300283, validation accuracy: 91.70%\n",
      "step: 169000, batch loss: 0.055972, train loss: 0.260243, train accuracy: 92.86%, validation loss: 0.292776, validation accuracy: 92.34%\n",
      "step: 170000, batch loss: 0.034236, train loss: 0.262360, train accuracy: 92.66%, validation loss: 0.297539, validation accuracy: 91.68%\n",
      "step: 171000, batch loss: 0.998042, train loss: 0.269807, train accuracy: 92.54%, validation loss: 0.303857, validation accuracy: 92.10%\n",
      "step: 172000, batch loss: 0.044879, train loss: 0.267162, train accuracy: 92.53%, validation loss: 0.301728, validation accuracy: 92.08%\n",
      "step: 173000, batch loss: 0.098883, train loss: 0.262072, train accuracy: 92.71%, validation loss: 0.296731, validation accuracy: 92.00%\n",
      "step: 174000, batch loss: 0.064672, train loss: 0.263270, train accuracy: 92.67%, validation loss: 0.298522, validation accuracy: 91.94%\n",
      "step: 175000, batch loss: 0.106915, train loss: 0.259486, train accuracy: 92.80%, validation loss: 0.296456, validation accuracy: 91.90%\n",
      "step: 176000, batch loss: 0.022397, train loss: 0.260901, train accuracy: 92.74%, validation loss: 0.294784, validation accuracy: 92.00%\n",
      "step: 177000, batch loss: 0.137726, train loss: 0.260664, train accuracy: 92.79%, validation loss: 0.297325, validation accuracy: 91.96%\n",
      "step: 178000, batch loss: 0.084813, train loss: 0.263284, train accuracy: 92.73%, validation loss: 0.298863, validation accuracy: 92.10%\n",
      "step: 179000, batch loss: 0.069452, train loss: 0.260075, train accuracy: 92.77%, validation loss: 0.289316, validation accuracy: 92.40%\n",
      "step: 180000, batch loss: 0.126446, train loss: 0.257873, train accuracy: 92.98%, validation loss: 0.290655, validation accuracy: 92.30%\n",
      "step: 181000, batch loss: 0.054905, train loss: 0.267623, train accuracy: 92.51%, validation loss: 0.305006, validation accuracy: 91.80%\n",
      "step: 182000, batch loss: 0.067723, train loss: 0.259208, train accuracy: 92.91%, validation loss: 0.292119, validation accuracy: 92.36%\n",
      "step: 183000, batch loss: 0.264943, train loss: 0.264219, train accuracy: 92.66%, validation loss: 0.301402, validation accuracy: 92.00%\n",
      "step: 184000, batch loss: 0.070233, train loss: 0.256815, train accuracy: 92.83%, validation loss: 0.294741, validation accuracy: 91.98%\n",
      "step: 185000, batch loss: 0.070090, train loss: 0.262884, train accuracy: 92.65%, validation loss: 0.296774, validation accuracy: 91.98%\n",
      "step: 186000, batch loss: 0.160202, train loss: 0.268581, train accuracy: 92.43%, validation loss: 0.305353, validation accuracy: 91.76%\n",
      "step: 187000, batch loss: 0.396825, train loss: 0.258315, train accuracy: 93.06%, validation loss: 0.297708, validation accuracy: 91.96%\n",
      "step: 188000, batch loss: 0.064825, train loss: 0.257670, train accuracy: 92.85%, validation loss: 0.291184, validation accuracy: 92.32%\n",
      "step: 189000, batch loss: 0.092441, train loss: 0.259343, train accuracy: 92.89%, validation loss: 0.297900, validation accuracy: 91.92%\n",
      "step: 190000, batch loss: 0.081203, train loss: 0.256433, train accuracy: 92.84%, validation loss: 0.292793, validation accuracy: 92.24%\n",
      "step: 191000, batch loss: 0.011925, train loss: 0.263290, train accuracy: 92.69%, validation loss: 0.297156, validation accuracy: 92.00%\n",
      "step: 192000, batch loss: 0.068867, train loss: 0.259215, train accuracy: 92.81%, validation loss: 0.293161, validation accuracy: 92.20%\n",
      "step: 193000, batch loss: 0.422895, train loss: 0.259716, train accuracy: 92.80%, validation loss: 0.296348, validation accuracy: 92.00%\n",
      "step: 194000, batch loss: 0.120570, train loss: 0.256539, train accuracy: 92.97%, validation loss: 0.298264, validation accuracy: 92.04%\n",
      "step: 195000, batch loss: 0.041396, train loss: 0.257461, train accuracy: 92.85%, validation loss: 0.296042, validation accuracy: 91.98%\n",
      "step: 196000, batch loss: 0.087349, train loss: 0.253639, train accuracy: 93.01%, validation loss: 0.292463, validation accuracy: 92.20%\n",
      "step: 197000, batch loss: 0.250280, train loss: 0.263233, train accuracy: 92.63%, validation loss: 0.299421, validation accuracy: 91.92%\n",
      "step: 198000, batch loss: 0.056307, train loss: 0.257126, train accuracy: 92.84%, validation loss: 0.292811, validation accuracy: 92.16%\n",
      "step: 199000, batch loss: 0.222282, train loss: 0.264085, train accuracy: 92.45%, validation loss: 0.298363, validation accuracy: 91.94%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndGmgUEopla7/A/1XCnWJjMd+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110744690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 10.017137, train loss: 13.211545, train accuracy: 10.55%, validation loss: 13.283140, validation accuracy: 10.70%\n",
      "step: 1000, batch loss: 2.636510, train loss: 2.020568, train accuracy: 60.73%, validation loss: 1.991344, validation accuracy: 60.78%\n",
      "step: 2000, batch loss: 3.190465, train loss: 1.399799, train accuracy: 70.92%, validation loss: 1.355783, validation accuracy: 71.98%\n",
      "step: 3000, batch loss: 0.751365, train loss: 1.131583, train accuracy: 75.76%, validation loss: 1.087955, validation accuracy: 77.00%\n",
      "step: 4000, batch loss: 0.036782, train loss: 0.983232, train accuracy: 78.61%, validation loss: 0.940905, validation accuracy: 79.70%\n",
      "step: 5000, batch loss: 1.104783, train loss: 0.890700, train accuracy: 80.35%, validation loss: 0.848130, validation accuracy: 81.50%\n",
      "step: 6000, batch loss: 0.731043, train loss: 0.826056, train accuracy: 81.77%, validation loss: 0.790685, validation accuracy: 82.94%\n",
      "step: 7000, batch loss: 0.304695, train loss: 0.773462, train accuracy: 82.72%, validation loss: 0.735971, validation accuracy: 83.88%\n",
      "step: 8000, batch loss: 0.038340, train loss: 0.731691, train accuracy: 83.54%, validation loss: 0.697435, validation accuracy: 84.72%\n",
      "step: 9000, batch loss: 1.808890, train loss: 0.697092, train accuracy: 84.24%, validation loss: 0.666420, validation accuracy: 85.22%\n",
      "step: 10000, batch loss: 1.122760, train loss: 0.671834, train accuracy: 84.66%, validation loss: 0.645580, validation accuracy: 85.80%\n",
      "step: 11000, batch loss: 1.519717, train loss: 0.646450, train accuracy: 85.24%, validation loss: 0.623083, validation accuracy: 86.18%\n",
      "step: 12000, batch loss: 0.004876, train loss: 0.633591, train accuracy: 85.36%, validation loss: 0.613703, validation accuracy: 86.60%\n",
      "step: 13000, batch loss: 0.653081, train loss: 0.610052, train accuracy: 85.77%, validation loss: 0.588574, validation accuracy: 86.78%\n",
      "step: 14000, batch loss: 0.812359, train loss: 0.596368, train accuracy: 86.09%, validation loss: 0.579269, validation accuracy: 87.00%\n",
      "step: 15000, batch loss: 0.033478, train loss: 0.580473, train accuracy: 86.32%, validation loss: 0.562174, validation accuracy: 87.12%\n",
      "step: 16000, batch loss: 0.412561, train loss: 0.568979, train accuracy: 86.60%, validation loss: 0.552272, validation accuracy: 87.40%\n",
      "step: 17000, batch loss: 0.204663, train loss: 0.561952, train accuracy: 86.60%, validation loss: 0.548967, validation accuracy: 87.66%\n",
      "step: 18000, batch loss: 0.025335, train loss: 0.545256, train accuracy: 86.96%, validation loss: 0.530924, validation accuracy: 87.54%\n",
      "step: 19000, batch loss: 0.061746, train loss: 0.536909, train accuracy: 87.09%, validation loss: 0.523205, validation accuracy: 87.76%\n",
      "step: 20000, batch loss: 1.445359, train loss: 0.528451, train accuracy: 87.21%, validation loss: 0.515995, validation accuracy: 87.94%\n",
      "step: 21000, batch loss: 1.472491, train loss: 0.519234, train accuracy: 87.47%, validation loss: 0.506162, validation accuracy: 88.06%\n",
      "step: 22000, batch loss: 0.435017, train loss: 0.515097, train accuracy: 87.55%, validation loss: 0.502807, validation accuracy: 88.10%\n",
      "step: 23000, batch loss: 0.433646, train loss: 0.504729, train accuracy: 87.67%, validation loss: 0.495766, validation accuracy: 88.06%\n",
      "step: 24000, batch loss: 0.166590, train loss: 0.494343, train accuracy: 87.86%, validation loss: 0.487771, validation accuracy: 88.12%\n",
      "step: 25000, batch loss: 0.179025, train loss: 0.489678, train accuracy: 87.87%, validation loss: 0.483458, validation accuracy: 88.26%\n",
      "step: 26000, batch loss: 0.629451, train loss: 0.480306, train accuracy: 88.15%, validation loss: 0.471999, validation accuracy: 88.34%\n",
      "step: 27000, batch loss: 0.138007, train loss: 0.477617, train accuracy: 88.31%, validation loss: 0.471429, validation accuracy: 88.52%\n",
      "step: 28000, batch loss: 0.142164, train loss: 0.474171, train accuracy: 88.18%, validation loss: 0.470765, validation accuracy: 88.66%\n",
      "step: 29000, batch loss: 0.062411, train loss: 0.473048, train accuracy: 88.22%, validation loss: 0.470511, validation accuracy: 88.52%\n",
      "step: 30000, batch loss: 1.288503, train loss: 0.459960, train accuracy: 88.53%, validation loss: 0.456887, validation accuracy: 88.88%\n",
      "step: 31000, batch loss: 0.275330, train loss: 0.458134, train accuracy: 88.59%, validation loss: 0.453183, validation accuracy: 89.10%\n",
      "step: 32000, batch loss: 0.115123, train loss: 0.451416, train accuracy: 88.65%, validation loss: 0.450604, validation accuracy: 88.94%\n",
      "step: 33000, batch loss: 0.235162, train loss: 0.446210, train accuracy: 88.81%, validation loss: 0.443881, validation accuracy: 89.02%\n",
      "step: 34000, batch loss: 0.208785, train loss: 0.445991, train accuracy: 88.87%, validation loss: 0.438185, validation accuracy: 89.26%\n",
      "step: 35000, batch loss: 0.023637, train loss: 0.440794, train accuracy: 89.01%, validation loss: 0.436699, validation accuracy: 89.36%\n",
      "step: 36000, batch loss: 0.568698, train loss: 0.436523, train accuracy: 89.05%, validation loss: 0.434099, validation accuracy: 89.38%\n",
      "step: 37000, batch loss: 0.023569, train loss: 0.432599, train accuracy: 89.12%, validation loss: 0.431762, validation accuracy: 89.38%\n",
      "step: 38000, batch loss: 0.024302, train loss: 0.427276, train accuracy: 89.17%, validation loss: 0.428594, validation accuracy: 89.32%\n",
      "step: 39000, batch loss: 1.136586, train loss: 0.426133, train accuracy: 89.06%, validation loss: 0.425260, validation accuracy: 89.44%\n",
      "step: 40000, batch loss: 2.092540, train loss: 0.423116, train accuracy: 89.25%, validation loss: 0.420409, validation accuracy: 89.54%\n",
      "step: 41000, batch loss: 0.248087, train loss: 0.417110, train accuracy: 89.34%, validation loss: 0.416328, validation accuracy: 89.48%\n",
      "step: 42000, batch loss: 0.047133, train loss: 0.416696, train accuracy: 89.31%, validation loss: 0.415190, validation accuracy: 89.40%\n",
      "step: 43000, batch loss: 0.363803, train loss: 0.418183, train accuracy: 89.24%, validation loss: 0.419608, validation accuracy: 89.52%\n",
      "step: 44000, batch loss: 0.132205, train loss: 0.409310, train accuracy: 89.48%, validation loss: 0.410070, validation accuracy: 89.46%\n",
      "step: 45000, batch loss: 0.146176, train loss: 0.408407, train accuracy: 89.46%, validation loss: 0.411834, validation accuracy: 89.50%\n",
      "step: 46000, batch loss: 0.395710, train loss: 0.406843, train accuracy: 89.51%, validation loss: 0.407719, validation accuracy: 89.60%\n",
      "step: 47000, batch loss: 0.130998, train loss: 0.402276, train accuracy: 89.69%, validation loss: 0.406499, validation accuracy: 89.54%\n",
      "step: 48000, batch loss: 0.006059, train loss: 0.400413, train accuracy: 89.71%, validation loss: 0.401622, validation accuracy: 89.60%\n",
      "step: 49000, batch loss: 0.180508, train loss: 0.401679, train accuracy: 89.57%, validation loss: 0.401723, validation accuracy: 89.64%\n",
      "step: 50000, batch loss: 0.052280, train loss: 0.395681, train accuracy: 89.74%, validation loss: 0.398569, validation accuracy: 89.56%\n",
      "step: 51000, batch loss: 0.197059, train loss: 0.395462, train accuracy: 89.76%, validation loss: 0.397408, validation accuracy: 89.66%\n",
      "step: 52000, batch loss: 0.053559, train loss: 0.390226, train accuracy: 89.92%, validation loss: 0.393000, validation accuracy: 89.98%\n",
      "step: 53000, batch loss: 0.096290, train loss: 0.386995, train accuracy: 89.97%, validation loss: 0.390911, validation accuracy: 89.82%\n",
      "step: 54000, batch loss: 0.017045, train loss: 0.387419, train accuracy: 89.92%, validation loss: 0.392637, validation accuracy: 89.80%\n",
      "step: 55000, batch loss: 0.324668, train loss: 0.387667, train accuracy: 90.04%, validation loss: 0.391103, validation accuracy: 89.86%\n",
      "step: 56000, batch loss: 0.026777, train loss: 0.384799, train accuracy: 90.02%, validation loss: 0.390006, validation accuracy: 89.78%\n",
      "step: 57000, batch loss: 0.423816, train loss: 0.383935, train accuracy: 89.95%, validation loss: 0.388293, validation accuracy: 89.92%\n",
      "step: 58000, batch loss: 0.114775, train loss: 0.379063, train accuracy: 90.11%, validation loss: 0.383607, validation accuracy: 89.92%\n",
      "step: 59000, batch loss: 0.074166, train loss: 0.378358, train accuracy: 90.05%, validation loss: 0.382264, validation accuracy: 89.76%\n",
      "step: 60000, batch loss: 1.845241, train loss: 0.375905, train accuracy: 90.14%, validation loss: 0.380758, validation accuracy: 89.88%\n",
      "step: 61000, batch loss: 0.009301, train loss: 0.374699, train accuracy: 90.14%, validation loss: 0.382232, validation accuracy: 89.84%\n",
      "step: 62000, batch loss: 0.014274, train loss: 0.373166, train accuracy: 90.21%, validation loss: 0.376499, validation accuracy: 90.06%\n",
      "step: 63000, batch loss: 0.029483, train loss: 0.371227, train accuracy: 90.15%, validation loss: 0.377979, validation accuracy: 89.98%\n",
      "step: 64000, batch loss: 0.932390, train loss: 0.369086, train accuracy: 90.30%, validation loss: 0.375046, validation accuracy: 90.18%\n",
      "step: 65000, batch loss: 0.144434, train loss: 0.371496, train accuracy: 90.05%, validation loss: 0.378638, validation accuracy: 90.00%\n",
      "step: 66000, batch loss: 0.207978, train loss: 0.368626, train accuracy: 90.25%, validation loss: 0.372412, validation accuracy: 90.04%\n",
      "step: 67000, batch loss: 0.067758, train loss: 0.365835, train accuracy: 90.37%, validation loss: 0.374690, validation accuracy: 90.16%\n",
      "step: 68000, batch loss: 0.023692, train loss: 0.364502, train accuracy: 90.35%, validation loss: 0.372163, validation accuracy: 90.08%\n",
      "step: 69000, batch loss: 0.631801, train loss: 0.364323, train accuracy: 90.40%, validation loss: 0.368282, validation accuracy: 90.16%\n",
      "step: 70000, batch loss: 0.074827, train loss: 0.362773, train accuracy: 90.40%, validation loss: 0.369890, validation accuracy: 90.04%\n",
      "step: 71000, batch loss: 0.085435, train loss: 0.358693, train accuracy: 90.57%, validation loss: 0.367155, validation accuracy: 90.26%\n",
      "step: 72000, batch loss: 0.078425, train loss: 0.359719, train accuracy: 90.45%, validation loss: 0.366142, validation accuracy: 90.24%\n",
      "step: 73000, batch loss: 0.106038, train loss: 0.356461, train accuracy: 90.56%, validation loss: 0.364099, validation accuracy: 90.32%\n",
      "step: 74000, batch loss: 0.014318, train loss: 0.360931, train accuracy: 90.36%, validation loss: 0.368441, validation accuracy: 90.34%\n",
      "step: 75000, batch loss: 0.204207, train loss: 0.353500, train accuracy: 90.66%, validation loss: 0.360140, validation accuracy: 90.50%\n",
      "step: 76000, batch loss: 0.182524, train loss: 0.352985, train accuracy: 90.64%, validation loss: 0.358300, validation accuracy: 90.50%\n",
      "step: 77000, batch loss: 0.083718, train loss: 0.354868, train accuracy: 90.57%, validation loss: 0.359566, validation accuracy: 90.52%\n",
      "step: 78000, batch loss: 0.014706, train loss: 0.350406, train accuracy: 90.71%, validation loss: 0.358005, validation accuracy: 90.54%\n",
      "step: 79000, batch loss: 0.008818, train loss: 0.350953, train accuracy: 90.73%, validation loss: 0.358797, validation accuracy: 90.80%\n",
      "step: 80000, batch loss: 0.390653, train loss: 0.349033, train accuracy: 90.71%, validation loss: 0.355487, validation accuracy: 90.66%\n",
      "step: 81000, batch loss: 0.548246, train loss: 0.350700, train accuracy: 90.68%, validation loss: 0.358636, validation accuracy: 90.72%\n",
      "step: 82000, batch loss: 0.058045, train loss: 0.346068, train accuracy: 90.75%, validation loss: 0.355111, validation accuracy: 90.78%\n",
      "step: 83000, batch loss: 0.606448, train loss: 0.345391, train accuracy: 90.73%, validation loss: 0.356720, validation accuracy: 90.46%\n",
      "step: 84000, batch loss: 1.718356, train loss: 0.345767, train accuracy: 90.79%, validation loss: 0.354541, validation accuracy: 90.64%\n",
      "step: 85000, batch loss: 1.599991, train loss: 0.344276, train accuracy: 90.87%, validation loss: 0.352167, validation accuracy: 90.86%\n",
      "step: 86000, batch loss: 0.014521, train loss: 0.342925, train accuracy: 90.85%, validation loss: 0.352270, validation accuracy: 90.86%\n",
      "step: 87000, batch loss: 0.412774, train loss: 0.342329, train accuracy: 90.89%, validation loss: 0.349163, validation accuracy: 90.80%\n",
      "step: 88000, batch loss: 0.056610, train loss: 0.340407, train accuracy: 90.85%, validation loss: 0.348425, validation accuracy: 90.78%\n",
      "step: 89000, batch loss: 0.039738, train loss: 0.341527, train accuracy: 90.87%, validation loss: 0.351687, validation accuracy: 90.78%\n",
      "step: 90000, batch loss: 0.738256, train loss: 0.338107, train accuracy: 90.93%, validation loss: 0.347849, validation accuracy: 90.94%\n",
      "step: 91000, batch loss: 0.044235, train loss: 0.337948, train accuracy: 90.97%, validation loss: 0.349580, validation accuracy: 90.78%\n",
      "step: 92000, batch loss: 0.145014, train loss: 0.337526, train accuracy: 90.93%, validation loss: 0.349523, validation accuracy: 90.74%\n",
      "step: 93000, batch loss: 0.212137, train loss: 0.337354, train accuracy: 90.99%, validation loss: 0.348718, validation accuracy: 90.90%\n",
      "step: 94000, batch loss: 0.595887, train loss: 0.335222, train accuracy: 91.07%, validation loss: 0.342557, validation accuracy: 91.06%\n",
      "step: 95000, batch loss: 0.098405, train loss: 0.334711, train accuracy: 91.06%, validation loss: 0.343816, validation accuracy: 91.02%\n",
      "step: 96000, batch loss: 0.013373, train loss: 0.333372, train accuracy: 91.00%, validation loss: 0.345756, validation accuracy: 90.88%\n",
      "step: 97000, batch loss: 0.052089, train loss: 0.332934, train accuracy: 91.09%, validation loss: 0.343757, validation accuracy: 91.00%\n",
      "step: 98000, batch loss: 0.034006, train loss: 0.332737, train accuracy: 91.12%, validation loss: 0.345177, validation accuracy: 91.06%\n",
      "step: 99000, batch loss: 0.440986, train loss: 0.331338, train accuracy: 91.05%, validation loss: 0.342193, validation accuracy: 90.92%\n",
      "step: 100000, batch loss: 0.216705, train loss: 0.335296, train accuracy: 90.93%, validation loss: 0.348253, validation accuracy: 90.90%\n",
      "step: 101000, batch loss: 0.168459, train loss: 0.328322, train accuracy: 91.17%, validation loss: 0.339013, validation accuracy: 91.20%\n",
      "step: 102000, batch loss: 0.071241, train loss: 0.328250, train accuracy: 91.17%, validation loss: 0.339635, validation accuracy: 91.44%\n",
      "step: 103000, batch loss: 0.017449, train loss: 0.328282, train accuracy: 91.22%, validation loss: 0.339189, validation accuracy: 91.16%\n",
      "step: 104000, batch loss: 0.126405, train loss: 0.327815, train accuracy: 91.16%, validation loss: 0.338274, validation accuracy: 91.16%\n",
      "step: 105000, batch loss: 0.041601, train loss: 0.325980, train accuracy: 91.22%, validation loss: 0.335701, validation accuracy: 91.20%\n",
      "step: 106000, batch loss: 0.206976, train loss: 0.326878, train accuracy: 91.17%, validation loss: 0.337097, validation accuracy: 91.06%\n",
      "step: 107000, batch loss: 0.031989, train loss: 0.323831, train accuracy: 91.30%, validation loss: 0.334344, validation accuracy: 91.34%\n",
      "step: 108000, batch loss: 0.015436, train loss: 0.323807, train accuracy: 91.30%, validation loss: 0.334095, validation accuracy: 91.36%\n",
      "step: 109000, batch loss: 1.021381, train loss: 0.327710, train accuracy: 91.13%, validation loss: 0.341934, validation accuracy: 90.84%\n",
      "step: 110000, batch loss: 0.027169, train loss: 0.324701, train accuracy: 91.26%, validation loss: 0.336736, validation accuracy: 91.12%\n",
      "step: 111000, batch loss: 0.083107, train loss: 0.325364, train accuracy: 91.21%, validation loss: 0.337098, validation accuracy: 90.98%\n",
      "step: 112000, batch loss: 0.620365, train loss: 0.324275, train accuracy: 91.18%, validation loss: 0.337468, validation accuracy: 91.32%\n",
      "step: 113000, batch loss: 0.058940, train loss: 0.321378, train accuracy: 91.29%, validation loss: 0.334834, validation accuracy: 91.34%\n",
      "step: 114000, batch loss: 0.092217, train loss: 0.321554, train accuracy: 91.29%, validation loss: 0.333319, validation accuracy: 91.52%\n",
      "step: 115000, batch loss: 0.470474, train loss: 0.321764, train accuracy: 91.20%, validation loss: 0.334366, validation accuracy: 91.14%\n",
      "step: 116000, batch loss: 1.987607, train loss: 0.322082, train accuracy: 91.35%, validation loss: 0.334487, validation accuracy: 91.24%\n",
      "step: 117000, batch loss: 0.190340, train loss: 0.317535, train accuracy: 91.49%, validation loss: 0.333175, validation accuracy: 91.28%\n",
      "step: 118000, batch loss: 0.049149, train loss: 0.319319, train accuracy: 91.41%, validation loss: 0.332784, validation accuracy: 91.48%\n",
      "step: 119000, batch loss: 0.290417, train loss: 0.319214, train accuracy: 91.41%, validation loss: 0.333436, validation accuracy: 91.30%\n",
      "step: 120000, batch loss: 1.236069, train loss: 0.318338, train accuracy: 91.42%, validation loss: 0.331817, validation accuracy: 91.32%\n",
      "step: 121000, batch loss: 0.586069, train loss: 0.318837, train accuracy: 91.24%, validation loss: 0.334511, validation accuracy: 91.24%\n",
      "step: 122000, batch loss: 0.019626, train loss: 0.316203, train accuracy: 91.39%, validation loss: 0.330049, validation accuracy: 91.52%\n",
      "step: 123000, batch loss: 0.059503, train loss: 0.313819, train accuracy: 91.51%, validation loss: 0.326649, validation accuracy: 91.36%\n",
      "step: 124000, batch loss: 0.019451, train loss: 0.318132, train accuracy: 91.35%, validation loss: 0.328910, validation accuracy: 91.48%\n",
      "step: 125000, batch loss: 0.459250, train loss: 0.318599, train accuracy: 91.31%, validation loss: 0.333590, validation accuracy: 91.08%\n",
      "step: 126000, batch loss: 0.018668, train loss: 0.313163, train accuracy: 91.48%, validation loss: 0.326161, validation accuracy: 91.40%\n",
      "step: 127000, batch loss: 0.029668, train loss: 0.314358, train accuracy: 91.48%, validation loss: 0.325925, validation accuracy: 91.42%\n",
      "step: 128000, batch loss: 0.516950, train loss: 0.314767, train accuracy: 91.46%, validation loss: 0.327456, validation accuracy: 91.42%\n",
      "step: 129000, batch loss: 0.373055, train loss: 0.311089, train accuracy: 91.64%, validation loss: 0.327959, validation accuracy: 91.30%\n",
      "step: 130000, batch loss: 0.291119, train loss: 0.312365, train accuracy: 91.48%, validation loss: 0.326112, validation accuracy: 91.40%\n",
      "step: 131000, batch loss: 0.244406, train loss: 0.309656, train accuracy: 91.61%, validation loss: 0.322294, validation accuracy: 91.46%\n",
      "step: 132000, batch loss: 0.428139, train loss: 0.311580, train accuracy: 91.45%, validation loss: 0.325348, validation accuracy: 91.48%\n",
      "step: 133000, batch loss: 0.058733, train loss: 0.309689, train accuracy: 91.57%, validation loss: 0.323957, validation accuracy: 91.42%\n",
      "step: 134000, batch loss: 0.579114, train loss: 0.308260, train accuracy: 91.65%, validation loss: 0.322348, validation accuracy: 91.60%\n",
      "step: 135000, batch loss: 0.011484, train loss: 0.309196, train accuracy: 91.63%, validation loss: 0.325915, validation accuracy: 91.58%\n",
      "step: 136000, batch loss: 1.222999, train loss: 0.307572, train accuracy: 91.68%, validation loss: 0.323030, validation accuracy: 91.58%\n",
      "step: 137000, batch loss: 0.858047, train loss: 0.307901, train accuracy: 91.59%, validation loss: 0.324662, validation accuracy: 91.48%\n",
      "step: 138000, batch loss: 0.028316, train loss: 0.306772, train accuracy: 91.66%, validation loss: 0.324686, validation accuracy: 91.38%\n",
      "step: 139000, batch loss: 0.018397, train loss: 0.307096, train accuracy: 91.68%, validation loss: 0.321564, validation accuracy: 91.58%\n",
      "step: 140000, batch loss: 0.043435, train loss: 0.308096, train accuracy: 91.61%, validation loss: 0.325849, validation accuracy: 91.30%\n",
      "step: 141000, batch loss: 0.079134, train loss: 0.308024, train accuracy: 91.61%, validation loss: 0.325759, validation accuracy: 91.24%\n",
      "step: 142000, batch loss: 0.162503, train loss: 0.303406, train accuracy: 91.70%, validation loss: 0.318491, validation accuracy: 91.64%\n",
      "step: 143000, batch loss: 0.051653, train loss: 0.307712, train accuracy: 91.61%, validation loss: 0.324836, validation accuracy: 91.20%\n",
      "step: 144000, batch loss: 0.150054, train loss: 0.303710, train accuracy: 91.80%, validation loss: 0.319286, validation accuracy: 91.62%\n",
      "step: 145000, batch loss: 0.083378, train loss: 0.303857, train accuracy: 91.85%, validation loss: 0.318906, validation accuracy: 91.52%\n",
      "step: 146000, batch loss: 0.298851, train loss: 0.303906, train accuracy: 91.78%, validation loss: 0.319375, validation accuracy: 91.58%\n",
      "step: 147000, batch loss: 0.780930, train loss: 0.307772, train accuracy: 91.51%, validation loss: 0.323877, validation accuracy: 91.40%\n",
      "step: 148000, batch loss: 0.416786, train loss: 0.303961, train accuracy: 91.69%, validation loss: 0.320208, validation accuracy: 91.66%\n",
      "step: 149000, batch loss: 0.154361, train loss: 0.304507, train accuracy: 91.67%, validation loss: 0.323798, validation accuracy: 91.38%\n",
      "step: 150000, batch loss: 0.176474, train loss: 0.303809, train accuracy: 91.70%, validation loss: 0.318527, validation accuracy: 91.48%\n",
      "step: 151000, batch loss: 0.185055, train loss: 0.303415, train accuracy: 91.72%, validation loss: 0.321073, validation accuracy: 91.54%\n",
      "step: 152000, batch loss: 0.028678, train loss: 0.302229, train accuracy: 91.81%, validation loss: 0.320225, validation accuracy: 91.56%\n",
      "step: 153000, batch loss: 0.021974, train loss: 0.299620, train accuracy: 91.87%, validation loss: 0.316164, validation accuracy: 91.62%\n",
      "step: 154000, batch loss: 0.605763, train loss: 0.300832, train accuracy: 91.83%, validation loss: 0.318340, validation accuracy: 91.56%\n",
      "step: 155000, batch loss: 1.689807, train loss: 0.299032, train accuracy: 91.82%, validation loss: 0.316834, validation accuracy: 91.36%\n",
      "step: 156000, batch loss: 0.408242, train loss: 0.299475, train accuracy: 91.85%, validation loss: 0.315390, validation accuracy: 91.64%\n",
      "step: 157000, batch loss: 0.294528, train loss: 0.298773, train accuracy: 91.92%, validation loss: 0.316072, validation accuracy: 91.68%\n",
      "step: 158000, batch loss: 0.067559, train loss: 0.296931, train accuracy: 91.90%, validation loss: 0.314701, validation accuracy: 91.68%\n",
      "step: 159000, batch loss: 0.428587, train loss: 0.297983, train accuracy: 91.90%, validation loss: 0.316636, validation accuracy: 91.68%\n",
      "step: 160000, batch loss: 1.148984, train loss: 0.298933, train accuracy: 91.86%, validation loss: 0.320398, validation accuracy: 91.44%\n",
      "step: 161000, batch loss: 0.169167, train loss: 0.301109, train accuracy: 91.76%, validation loss: 0.320092, validation accuracy: 91.38%\n",
      "step: 162000, batch loss: 0.007795, train loss: 0.296133, train accuracy: 91.93%, validation loss: 0.315574, validation accuracy: 91.60%\n",
      "step: 163000, batch loss: 0.292012, train loss: 0.295748, train accuracy: 91.91%, validation loss: 0.314522, validation accuracy: 91.62%\n",
      "step: 164000, batch loss: 0.430985, train loss: 0.294292, train accuracy: 91.95%, validation loss: 0.312423, validation accuracy: 91.62%\n",
      "step: 165000, batch loss: 0.084505, train loss: 0.298592, train accuracy: 91.77%, validation loss: 0.315089, validation accuracy: 91.64%\n",
      "step: 166000, batch loss: 0.594617, train loss: 0.295785, train accuracy: 91.88%, validation loss: 0.311610, validation accuracy: 91.48%\n",
      "step: 167000, batch loss: 0.001743, train loss: 0.298088, train accuracy: 91.83%, validation loss: 0.317471, validation accuracy: 91.50%\n",
      "step: 168000, batch loss: 0.006213, train loss: 0.298870, train accuracy: 91.80%, validation loss: 0.314492, validation accuracy: 91.72%\n",
      "step: 169000, batch loss: 0.356387, train loss: 0.292528, train accuracy: 91.99%, validation loss: 0.310721, validation accuracy: 91.72%\n",
      "step: 170000, batch loss: 0.407230, train loss: 0.293945, train accuracy: 91.92%, validation loss: 0.310663, validation accuracy: 91.64%\n",
      "step: 171000, batch loss: 0.388483, train loss: 0.292654, train accuracy: 92.05%, validation loss: 0.312509, validation accuracy: 91.64%\n",
      "step: 172000, batch loss: 0.261117, train loss: 0.292148, train accuracy: 92.00%, validation loss: 0.309711, validation accuracy: 91.62%\n",
      "step: 173000, batch loss: 0.378531, train loss: 0.292014, train accuracy: 91.95%, validation loss: 0.312513, validation accuracy: 91.60%\n",
      "step: 174000, batch loss: 0.195857, train loss: 0.292356, train accuracy: 91.99%, validation loss: 0.312340, validation accuracy: 91.66%\n",
      "step: 175000, batch loss: 0.482365, train loss: 0.294903, train accuracy: 91.94%, validation loss: 0.313474, validation accuracy: 91.54%\n",
      "step: 176000, batch loss: 0.065906, train loss: 0.291380, train accuracy: 92.00%, validation loss: 0.309064, validation accuracy: 91.70%\n",
      "step: 177000, batch loss: 0.032500, train loss: 0.291722, train accuracy: 92.01%, validation loss: 0.309899, validation accuracy: 91.74%\n",
      "step: 178000, batch loss: 0.127070, train loss: 0.293273, train accuracy: 91.98%, validation loss: 0.312126, validation accuracy: 91.80%\n",
      "step: 179000, batch loss: 0.326988, train loss: 0.293239, train accuracy: 91.92%, validation loss: 0.313774, validation accuracy: 91.74%\n",
      "step: 180000, batch loss: 0.014490, train loss: 0.291273, train accuracy: 92.02%, validation loss: 0.312182, validation accuracy: 91.84%\n",
      "step: 181000, batch loss: 0.843399, train loss: 0.292237, train accuracy: 91.90%, validation loss: 0.312296, validation accuracy: 91.68%\n",
      "step: 182000, batch loss: 0.595746, train loss: 0.293115, train accuracy: 91.98%, validation loss: 0.314660, validation accuracy: 91.64%\n",
      "step: 183000, batch loss: 0.097322, train loss: 0.294993, train accuracy: 91.94%, validation loss: 0.311423, validation accuracy: 91.82%\n",
      "step: 184000, batch loss: 0.111621, train loss: 0.292397, train accuracy: 91.96%, validation loss: 0.310549, validation accuracy: 91.84%\n",
      "step: 185000, batch loss: 0.151148, train loss: 0.292118, train accuracy: 92.08%, validation loss: 0.310092, validation accuracy: 91.88%\n",
      "step: 186000, batch loss: 0.117391, train loss: 0.289390, train accuracy: 92.06%, validation loss: 0.305993, validation accuracy: 91.98%\n",
      "step: 187000, batch loss: 0.029115, train loss: 0.288380, train accuracy: 92.12%, validation loss: 0.308776, validation accuracy: 91.76%\n",
      "step: 188000, batch loss: 0.093859, train loss: 0.289784, train accuracy: 91.98%, validation loss: 0.309640, validation accuracy: 91.52%\n",
      "step: 189000, batch loss: 0.188395, train loss: 0.290056, train accuracy: 92.06%, validation loss: 0.309446, validation accuracy: 91.78%\n",
      "step: 190000, batch loss: 0.279919, train loss: 0.288956, train accuracy: 92.14%, validation loss: 0.308834, validation accuracy: 91.84%\n",
      "step: 191000, batch loss: 0.377424, train loss: 0.289822, train accuracy: 92.07%, validation loss: 0.309927, validation accuracy: 91.56%\n",
      "step: 192000, batch loss: 1.765377, train loss: 0.285318, train accuracy: 92.17%, validation loss: 0.306384, validation accuracy: 91.76%\n",
      "step: 193000, batch loss: 0.049274, train loss: 0.287146, train accuracy: 92.17%, validation loss: 0.308951, validation accuracy: 91.64%\n",
      "step: 194000, batch loss: 0.283193, train loss: 0.288067, train accuracy: 92.12%, validation loss: 0.308765, validation accuracy: 91.78%\n",
      "step: 195000, batch loss: 0.042456, train loss: 0.285464, train accuracy: 92.21%, validation loss: 0.308596, validation accuracy: 91.72%\n",
      "step: 196000, batch loss: 0.900202, train loss: 0.288399, train accuracy: 92.04%, validation loss: 0.311862, validation accuracy: 91.48%\n",
      "step: 197000, batch loss: 0.300892, train loss: 0.285070, train accuracy: 92.15%, validation loss: 0.305688, validation accuracy: 91.86%\n",
      "step: 198000, batch loss: 0.182412, train loss: 0.283915, train accuracy: 92.24%, validation loss: 0.305290, validation accuracy: 91.94%\n",
      "step: 199000, batch loss: 0.057109, train loss: 0.287307, train accuracy: 92.13%, validation loss: 0.306950, validation accuracy: 91.82%\n",
      "step: 200000, batch loss: 0.056306, train loss: 0.284176, train accuracy: 92.25%, validation loss: 0.304365, validation accuracy: 91.80%\n",
      "step: 201000, batch loss: 0.227781, train loss: 0.283617, train accuracy: 92.19%, validation loss: 0.304957, validation accuracy: 91.90%\n",
      "step: 202000, batch loss: 0.110939, train loss: 0.286081, train accuracy: 92.15%, validation loss: 0.309467, validation accuracy: 91.66%\n",
      "step: 203000, batch loss: 0.547568, train loss: 0.284226, train accuracy: 92.16%, validation loss: 0.304073, validation accuracy: 91.64%\n",
      "step: 204000, batch loss: 0.061595, train loss: 0.286798, train accuracy: 92.07%, validation loss: 0.307738, validation accuracy: 91.58%\n",
      "step: 205000, batch loss: 0.797269, train loss: 0.283865, train accuracy: 92.19%, validation loss: 0.305863, validation accuracy: 91.78%\n",
      "step: 206000, batch loss: 0.068506, train loss: 0.284497, train accuracy: 92.15%, validation loss: 0.306171, validation accuracy: 91.82%\n",
      "step: 207000, batch loss: 0.082052, train loss: 0.283721, train accuracy: 92.18%, validation loss: 0.306814, validation accuracy: 91.80%\n",
      "step: 208000, batch loss: 0.213717, train loss: 0.283809, train accuracy: 92.17%, validation loss: 0.307730, validation accuracy: 91.52%\n",
      "step: 209000, batch loss: 0.019675, train loss: 0.282097, train accuracy: 92.22%, validation loss: 0.305159, validation accuracy: 91.76%\n",
      "step: 210000, batch loss: 0.291087, train loss: 0.288277, train accuracy: 91.98%, validation loss: 0.310582, validation accuracy: 91.56%\n",
      "step: 211000, batch loss: 0.297782, train loss: 0.284314, train accuracy: 92.16%, validation loss: 0.304420, validation accuracy: 91.86%\n",
      "step: 212000, batch loss: 0.463205, train loss: 0.281446, train accuracy: 92.31%, validation loss: 0.301091, validation accuracy: 92.12%\n",
      "step: 213000, batch loss: 0.246657, train loss: 0.282132, train accuracy: 92.31%, validation loss: 0.301091, validation accuracy: 91.90%\n",
      "step: 214000, batch loss: 0.183684, train loss: 0.281004, train accuracy: 92.33%, validation loss: 0.302080, validation accuracy: 92.12%\n",
      "step: 215000, batch loss: 0.312220, train loss: 0.281352, train accuracy: 92.31%, validation loss: 0.304021, validation accuracy: 91.90%\n",
      "step: 216000, batch loss: 0.013938, train loss: 0.283597, train accuracy: 92.20%, validation loss: 0.305360, validation accuracy: 91.76%\n",
      "step: 217000, batch loss: 0.053693, train loss: 0.284646, train accuracy: 92.13%, validation loss: 0.306255, validation accuracy: 91.80%\n",
      "step: 218000, batch loss: 0.012786, train loss: 0.281503, train accuracy: 92.32%, validation loss: 0.302598, validation accuracy: 91.88%\n",
      "step: 219000, batch loss: 0.100105, train loss: 0.288961, train accuracy: 91.90%, validation loss: 0.308438, validation accuracy: 91.56%\n",
      "step: 220000, batch loss: 0.233137, train loss: 0.280369, train accuracy: 92.27%, validation loss: 0.303145, validation accuracy: 91.82%\n",
      "step: 221000, batch loss: 0.016082, train loss: 0.278784, train accuracy: 92.30%, validation loss: 0.301927, validation accuracy: 91.82%\n",
      "step: 222000, batch loss: 0.020689, train loss: 0.282706, train accuracy: 92.11%, validation loss: 0.303386, validation accuracy: 91.80%\n",
      "step: 223000, batch loss: 0.166906, train loss: 0.280688, train accuracy: 92.35%, validation loss: 0.302265, validation accuracy: 92.00%\n",
      "step: 224000, batch loss: 0.056674, train loss: 0.279913, train accuracy: 92.31%, validation loss: 0.304244, validation accuracy: 91.94%\n",
      "step: 225000, batch loss: 0.195746, train loss: 0.278403, train accuracy: 92.39%, validation loss: 0.302622, validation accuracy: 91.90%\n",
      "step: 226000, batch loss: 0.185247, train loss: 0.278446, train accuracy: 92.31%, validation loss: 0.302431, validation accuracy: 91.80%\n",
      "step: 227000, batch loss: 0.013961, train loss: 0.278594, train accuracy: 92.39%, validation loss: 0.300729, validation accuracy: 92.00%\n",
      "step: 228000, batch loss: 0.022007, train loss: 0.282474, train accuracy: 92.07%, validation loss: 0.306431, validation accuracy: 91.72%\n",
      "step: 229000, batch loss: 0.045576, train loss: 0.276904, train accuracy: 92.34%, validation loss: 0.300085, validation accuracy: 91.86%\n",
      "step: 230000, batch loss: 0.019698, train loss: 0.277648, train accuracy: 92.30%, validation loss: 0.300277, validation accuracy: 91.80%\n",
      "step: 231000, batch loss: 0.047206, train loss: 0.278161, train accuracy: 92.39%, validation loss: 0.304024, validation accuracy: 91.78%\n",
      "step: 232000, batch loss: 0.256747, train loss: 0.275721, train accuracy: 92.40%, validation loss: 0.299544, validation accuracy: 91.98%\n",
      "step: 233000, batch loss: 0.431820, train loss: 0.277723, train accuracy: 92.36%, validation loss: 0.302408, validation accuracy: 91.76%\n",
      "step: 234000, batch loss: 0.053822, train loss: 0.278654, train accuracy: 92.31%, validation loss: 0.304676, validation accuracy: 91.68%\n",
      "step: 235000, batch loss: 1.754255, train loss: 0.279512, train accuracy: 92.25%, validation loss: 0.303051, validation accuracy: 91.84%\n",
      "step: 236000, batch loss: 0.065195, train loss: 0.276588, train accuracy: 92.33%, validation loss: 0.299079, validation accuracy: 92.08%\n",
      "step: 237000, batch loss: 0.378328, train loss: 0.276776, train accuracy: 92.36%, validation loss: 0.300780, validation accuracy: 91.82%\n",
      "step: 238000, batch loss: 0.025489, train loss: 0.276520, train accuracy: 92.37%, validation loss: 0.299459, validation accuracy: 91.88%\n",
      "step: 239000, batch loss: 0.037049, train loss: 0.276951, train accuracy: 92.42%, validation loss: 0.300547, validation accuracy: 91.96%\n",
      "step: 240000, batch loss: 0.013957, train loss: 0.276073, train accuracy: 92.44%, validation loss: 0.298564, validation accuracy: 92.04%\n",
      "step: 241000, batch loss: 0.194207, train loss: 0.275906, train accuracy: 92.38%, validation loss: 0.300995, validation accuracy: 91.84%\n",
      "step: 242000, batch loss: 0.380742, train loss: 0.275644, train accuracy: 92.47%, validation loss: 0.299599, validation accuracy: 91.98%\n",
      "step: 243000, batch loss: 0.136667, train loss: 0.278112, train accuracy: 92.21%, validation loss: 0.303950, validation accuracy: 91.70%\n",
      "step: 244000, batch loss: 0.096312, train loss: 0.277015, train accuracy: 92.41%, validation loss: 0.302735, validation accuracy: 92.00%\n",
      "step: 245000, batch loss: 0.031290, train loss: 0.274384, train accuracy: 92.49%, validation loss: 0.298416, validation accuracy: 92.12%\n",
      "step: 246000, batch loss: 0.094913, train loss: 0.274229, train accuracy: 92.41%, validation loss: 0.298830, validation accuracy: 91.82%\n",
      "step: 247000, batch loss: 1.095939, train loss: 0.273952, train accuracy: 92.47%, validation loss: 0.296972, validation accuracy: 92.20%\n",
      "step: 248000, batch loss: 0.426023, train loss: 0.274858, train accuracy: 92.47%, validation loss: 0.299394, validation accuracy: 92.02%\n",
      "step: 249000, batch loss: 0.084613, train loss: 0.274490, train accuracy: 92.40%, validation loss: 0.299969, validation accuracy: 92.10%\n",
      "step: 250000, batch loss: 0.104159, train loss: 0.275514, train accuracy: 92.40%, validation loss: 0.298246, validation accuracy: 92.06%\n",
      "step: 251000, batch loss: 0.668973, train loss: 0.272930, train accuracy: 92.51%, validation loss: 0.297530, validation accuracy: 92.06%\n",
      "step: 252000, batch loss: 0.080533, train loss: 0.273473, train accuracy: 92.46%, validation loss: 0.296846, validation accuracy: 91.98%\n",
      "step: 253000, batch loss: 0.059527, train loss: 0.274094, train accuracy: 92.37%, validation loss: 0.297804, validation accuracy: 92.10%\n",
      "step: 254000, batch loss: 0.138354, train loss: 0.271852, train accuracy: 92.45%, validation loss: 0.295697, validation accuracy: 91.96%\n",
      "step: 255000, batch loss: 1.267643, train loss: 0.271845, train accuracy: 92.52%, validation loss: 0.295807, validation accuracy: 92.04%\n",
      "step: 256000, batch loss: 0.505173, train loss: 0.272536, train accuracy: 92.48%, validation loss: 0.297787, validation accuracy: 91.80%\n",
      "step: 257000, batch loss: 0.190411, train loss: 0.273780, train accuracy: 92.39%, validation loss: 0.298598, validation accuracy: 91.92%\n",
      "step: 258000, batch loss: 0.452681, train loss: 0.272358, train accuracy: 92.43%, validation loss: 0.296139, validation accuracy: 91.96%\n",
      "step: 259000, batch loss: 0.420090, train loss: 0.274080, train accuracy: 92.47%, validation loss: 0.300564, validation accuracy: 91.98%\n",
      "step: 260000, batch loss: 0.179436, train loss: 0.278408, train accuracy: 92.20%, validation loss: 0.301774, validation accuracy: 91.80%\n",
      "step: 261000, batch loss: 0.067538, train loss: 0.273243, train accuracy: 92.41%, validation loss: 0.297385, validation accuracy: 92.00%\n",
      "step: 262000, batch loss: 0.274151, train loss: 0.273476, train accuracy: 92.45%, validation loss: 0.299575, validation accuracy: 91.76%\n",
      "step: 263000, batch loss: 0.354605, train loss: 0.271155, train accuracy: 92.51%, validation loss: 0.297415, validation accuracy: 91.84%\n",
      "step: 264000, batch loss: 0.133241, train loss: 0.275986, train accuracy: 92.47%, validation loss: 0.297558, validation accuracy: 92.18%\n",
      "step: 265000, batch loss: 0.054757, train loss: 0.271848, train accuracy: 92.48%, validation loss: 0.296355, validation accuracy: 91.86%\n",
      "step: 266000, batch loss: 0.085734, train loss: 0.269441, train accuracy: 92.58%, validation loss: 0.294559, validation accuracy: 91.98%\n",
      "step: 267000, batch loss: 0.154126, train loss: 0.274294, train accuracy: 92.51%, validation loss: 0.298631, validation accuracy: 92.18%\n",
      "step: 268000, batch loss: 1.297978, train loss: 0.272428, train accuracy: 92.45%, validation loss: 0.297962, validation accuracy: 91.88%\n",
      "step: 269000, batch loss: 0.021096, train loss: 0.269625, train accuracy: 92.60%, validation loss: 0.294625, validation accuracy: 92.06%\n",
      "step: 270000, batch loss: 0.004206, train loss: 0.275299, train accuracy: 92.26%, validation loss: 0.299306, validation accuracy: 92.14%\n",
      "step: 271000, batch loss: 0.027705, train loss: 0.272161, train accuracy: 92.37%, validation loss: 0.298122, validation accuracy: 91.78%\n",
      "step: 272000, batch loss: 0.031515, train loss: 0.272177, train accuracy: 92.49%, validation loss: 0.299558, validation accuracy: 92.08%\n",
      "step: 273000, batch loss: 0.031382, train loss: 0.269897, train accuracy: 92.61%, validation loss: 0.296907, validation accuracy: 91.80%\n",
      "step: 274000, batch loss: 0.115393, train loss: 0.271863, train accuracy: 92.46%, validation loss: 0.299480, validation accuracy: 91.76%\n",
      "step: 275000, batch loss: 0.182202, train loss: 0.269349, train accuracy: 92.56%, validation loss: 0.294586, validation accuracy: 92.08%\n",
      "step: 276000, batch loss: 0.516153, train loss: 0.268088, train accuracy: 92.61%, validation loss: 0.293059, validation accuracy: 92.18%\n",
      "step: 277000, batch loss: 0.161323, train loss: 0.268680, train accuracy: 92.65%, validation loss: 0.294314, validation accuracy: 92.02%\n",
      "step: 278000, batch loss: 0.329144, train loss: 0.271912, train accuracy: 92.35%, validation loss: 0.295951, validation accuracy: 91.98%\n",
      "step: 279000, batch loss: 0.219052, train loss: 0.268968, train accuracy: 92.57%, validation loss: 0.293917, validation accuracy: 92.20%\n",
      "step: 280000, batch loss: 0.538278, train loss: 0.268156, train accuracy: 92.59%, validation loss: 0.292402, validation accuracy: 92.16%\n",
      "step: 281000, batch loss: 0.562063, train loss: 0.269630, train accuracy: 92.55%, validation loss: 0.294724, validation accuracy: 92.14%\n",
      "step: 282000, batch loss: 0.358683, train loss: 0.268006, train accuracy: 92.63%, validation loss: 0.294584, validation accuracy: 92.08%\n",
      "step: 283000, batch loss: 0.055845, train loss: 0.269788, train accuracy: 92.53%, validation loss: 0.298044, validation accuracy: 92.10%\n",
      "step: 284000, batch loss: 0.352209, train loss: 0.269128, train accuracy: 92.48%, validation loss: 0.294833, validation accuracy: 92.16%\n",
      "step: 285000, batch loss: 0.120144, train loss: 0.268813, train accuracy: 92.57%, validation loss: 0.295653, validation accuracy: 92.08%\n",
      "step: 286000, batch loss: 0.695378, train loss: 0.267846, train accuracy: 92.61%, validation loss: 0.293582, validation accuracy: 92.08%\n",
      "step: 287000, batch loss: 0.339165, train loss: 0.267537, train accuracy: 92.60%, validation loss: 0.291849, validation accuracy: 92.32%\n",
      "step: 288000, batch loss: 0.251955, train loss: 0.267914, train accuracy: 92.57%, validation loss: 0.293426, validation accuracy: 92.14%\n",
      "step: 289000, batch loss: 0.176475, train loss: 0.268560, train accuracy: 92.60%, validation loss: 0.294244, validation accuracy: 92.12%\n",
      "step: 290000, batch loss: 0.554791, train loss: 0.270045, train accuracy: 92.53%, validation loss: 0.296455, validation accuracy: 91.90%\n",
      "step: 291000, batch loss: 0.023112, train loss: 0.268574, train accuracy: 92.59%, validation loss: 0.294566, validation accuracy: 92.08%\n",
      "step: 292000, batch loss: 0.907810, train loss: 0.269146, train accuracy: 92.57%, validation loss: 0.299105, validation accuracy: 92.04%\n",
      "step: 293000, batch loss: 0.030333, train loss: 0.266481, train accuracy: 92.66%, validation loss: 0.293690, validation accuracy: 92.16%\n",
      "step: 294000, batch loss: 0.169725, train loss: 0.267528, train accuracy: 92.58%, validation loss: 0.294014, validation accuracy: 92.16%\n",
      "step: 295000, batch loss: 0.631099, train loss: 0.267678, train accuracy: 92.63%, validation loss: 0.295905, validation accuracy: 92.00%\n",
      "step: 296000, batch loss: 0.005323, train loss: 0.268423, train accuracy: 92.64%, validation loss: 0.296620, validation accuracy: 92.08%\n",
      "step: 297000, batch loss: 0.201906, train loss: 0.269283, train accuracy: 92.46%, validation loss: 0.297047, validation accuracy: 91.90%\n",
      "step: 298000, batch loss: 0.247390, train loss: 0.265339, train accuracy: 92.68%, validation loss: 0.294415, validation accuracy: 92.14%\n",
      "step: 299000, batch loss: 0.007347, train loss: 0.268337, train accuracy: 92.61%, validation loss: 0.295444, validation accuracy: 92.14%\n",
      "step: 300000, batch loss: 0.125210, train loss: 0.265940, train accuracy: 92.63%, validation loss: 0.293311, validation accuracy: 92.10%\n",
      "step: 301000, batch loss: 0.049412, train loss: 0.264118, train accuracy: 92.73%, validation loss: 0.291757, validation accuracy: 92.32%\n",
      "step: 302000, batch loss: 0.086989, train loss: 0.267167, train accuracy: 92.61%, validation loss: 0.293697, validation accuracy: 92.02%\n",
      "step: 303000, batch loss: 0.317472, train loss: 0.266737, train accuracy: 92.61%, validation loss: 0.294618, validation accuracy: 92.00%\n",
      "step: 304000, batch loss: 0.296082, train loss: 0.266211, train accuracy: 92.61%, validation loss: 0.294148, validation accuracy: 92.24%\n",
      "step: 305000, batch loss: 0.060921, train loss: 0.264810, train accuracy: 92.70%, validation loss: 0.291765, validation accuracy: 92.16%\n",
      "step: 306000, batch loss: 0.035803, train loss: 0.266890, train accuracy: 92.59%, validation loss: 0.292825, validation accuracy: 92.22%\n",
      "step: 307000, batch loss: 0.149824, train loss: 0.267168, train accuracy: 92.58%, validation loss: 0.296049, validation accuracy: 92.04%\n",
      "step: 308000, batch loss: 0.062184, train loss: 0.264805, train accuracy: 92.61%, validation loss: 0.293629, validation accuracy: 92.10%\n",
      "step: 309000, batch loss: 0.369137, train loss: 0.265683, train accuracy: 92.57%, validation loss: 0.292921, validation accuracy: 92.34%\n",
      "step: 310000, batch loss: 0.013881, train loss: 0.263811, train accuracy: 92.73%, validation loss: 0.291610, validation accuracy: 91.92%\n",
      "step: 311000, batch loss: 0.067840, train loss: 0.264088, train accuracy: 92.75%, validation loss: 0.292959, validation accuracy: 92.00%\n",
      "step: 312000, batch loss: 0.614589, train loss: 0.263975, train accuracy: 92.68%, validation loss: 0.293885, validation accuracy: 92.00%\n",
      "step: 313000, batch loss: 0.220841, train loss: 0.263614, train accuracy: 92.75%, validation loss: 0.291270, validation accuracy: 92.40%\n",
      "step: 314000, batch loss: 0.116744, train loss: 0.264520, train accuracy: 92.69%, validation loss: 0.293691, validation accuracy: 92.10%\n",
      "step: 315000, batch loss: 0.090563, train loss: 0.264010, train accuracy: 92.62%, validation loss: 0.291849, validation accuracy: 92.20%\n",
      "step: 316000, batch loss: 0.045074, train loss: 0.263518, train accuracy: 92.65%, validation loss: 0.293092, validation accuracy: 92.14%\n",
      "step: 317000, batch loss: 0.097673, train loss: 0.264605, train accuracy: 92.65%, validation loss: 0.292671, validation accuracy: 92.32%\n",
      "step: 318000, batch loss: 0.245991, train loss: 0.262791, train accuracy: 92.70%, validation loss: 0.291752, validation accuracy: 92.30%\n",
      "step: 319000, batch loss: 0.310779, train loss: 0.264859, train accuracy: 92.73%, validation loss: 0.292547, validation accuracy: 92.20%\n",
      "step: 320000, batch loss: 0.046567, train loss: 0.263049, train accuracy: 92.71%, validation loss: 0.290139, validation accuracy: 92.30%\n",
      "step: 321000, batch loss: 0.028830, train loss: 0.262133, train accuracy: 92.73%, validation loss: 0.290274, validation accuracy: 92.26%\n",
      "step: 322000, batch loss: 0.037694, train loss: 0.263079, train accuracy: 92.67%, validation loss: 0.289172, validation accuracy: 92.30%\n",
      "step: 323000, batch loss: 0.052427, train loss: 0.266261, train accuracy: 92.55%, validation loss: 0.295321, validation accuracy: 91.72%\n",
      "step: 324000, batch loss: 0.721157, train loss: 0.264480, train accuracy: 92.62%, validation loss: 0.295638, validation accuracy: 91.88%\n",
      "step: 325000, batch loss: 0.023903, train loss: 0.263994, train accuracy: 92.75%, validation loss: 0.294496, validation accuracy: 92.10%\n",
      "step: 326000, batch loss: 0.197365, train loss: 0.262921, train accuracy: 92.72%, validation loss: 0.292286, validation accuracy: 92.12%\n",
      "step: 327000, batch loss: 0.102804, train loss: 0.262239, train accuracy: 92.70%, validation loss: 0.290863, validation accuracy: 92.28%\n",
      "step: 328000, batch loss: 0.198044, train loss: 0.264255, train accuracy: 92.69%, validation loss: 0.290610, validation accuracy: 92.30%\n",
      "step: 329000, batch loss: 0.166294, train loss: 0.263386, train accuracy: 92.68%, validation loss: 0.288378, validation accuracy: 92.46%\n",
      "step: 330000, batch loss: 0.035381, train loss: 0.261534, train accuracy: 92.74%, validation loss: 0.290229, validation accuracy: 92.40%\n",
      "step: 331000, batch loss: 1.257540, train loss: 0.265688, train accuracy: 92.63%, validation loss: 0.295249, validation accuracy: 92.06%\n",
      "step: 332000, batch loss: 0.012660, train loss: 0.266232, train accuracy: 92.47%, validation loss: 0.294616, validation accuracy: 91.96%\n",
      "step: 333000, batch loss: 0.158044, train loss: 0.264332, train accuracy: 92.69%, validation loss: 0.291261, validation accuracy: 92.28%\n",
      "step: 334000, batch loss: 0.098212, train loss: 0.261384, train accuracy: 92.77%, validation loss: 0.288695, validation accuracy: 92.38%\n",
      "step: 335000, batch loss: 0.116246, train loss: 0.262260, train accuracy: 92.71%, validation loss: 0.290979, validation accuracy: 92.16%\n",
      "step: 336000, batch loss: 0.217489, train loss: 0.261656, train accuracy: 92.73%, validation loss: 0.291120, validation accuracy: 92.42%\n",
      "step: 337000, batch loss: 0.022264, train loss: 0.260189, train accuracy: 92.78%, validation loss: 0.290138, validation accuracy: 92.22%\n",
      "step: 338000, batch loss: 0.048030, train loss: 0.260480, train accuracy: 92.81%, validation loss: 0.289253, validation accuracy: 92.42%\n",
      "step: 339000, batch loss: 0.216940, train loss: 0.261599, train accuracy: 92.71%, validation loss: 0.289130, validation accuracy: 92.44%\n",
      "step: 340000, batch loss: 0.088252, train loss: 0.262843, train accuracy: 92.79%, validation loss: 0.292611, validation accuracy: 92.12%\n",
      "step: 341000, batch loss: 0.671309, train loss: 0.261730, train accuracy: 92.74%, validation loss: 0.291726, validation accuracy: 92.10%\n",
      "step: 342000, batch loss: 0.153667, train loss: 0.262512, train accuracy: 92.69%, validation loss: 0.292719, validation accuracy: 92.20%\n",
      "step: 343000, batch loss: 0.624335, train loss: 0.261216, train accuracy: 92.84%, validation loss: 0.292926, validation accuracy: 91.94%\n",
      "step: 344000, batch loss: 0.125285, train loss: 0.261041, train accuracy: 92.79%, validation loss: 0.288985, validation accuracy: 92.30%\n",
      "step: 345000, batch loss: 0.682342, train loss: 0.261397, train accuracy: 92.78%, validation loss: 0.289448, validation accuracy: 92.24%\n",
      "step: 346000, batch loss: 0.029042, train loss: 0.260221, train accuracy: 92.81%, validation loss: 0.289918, validation accuracy: 92.36%\n",
      "step: 347000, batch loss: 0.113291, train loss: 0.264165, train accuracy: 92.66%, validation loss: 0.292847, validation accuracy: 92.02%\n",
      "step: 348000, batch loss: 0.482207, train loss: 0.260230, train accuracy: 92.77%, validation loss: 0.288799, validation accuracy: 92.42%\n",
      "step: 349000, batch loss: 0.085036, train loss: 0.258462, train accuracy: 92.82%, validation loss: 0.289295, validation accuracy: 92.32%\n",
      "step: 350000, batch loss: 0.036434, train loss: 0.259959, train accuracy: 92.81%, validation loss: 0.290290, validation accuracy: 92.18%\n",
      "step: 351000, batch loss: 0.515169, train loss: 0.260197, train accuracy: 92.81%, validation loss: 0.289135, validation accuracy: 92.18%\n",
      "step: 352000, batch loss: 0.076190, train loss: 0.265770, train accuracy: 92.61%, validation loss: 0.295559, validation accuracy: 92.06%\n",
      "step: 353000, batch loss: 0.192349, train loss: 0.263489, train accuracy: 92.56%, validation loss: 0.293588, validation accuracy: 91.96%\n",
      "step: 354000, batch loss: 0.018927, train loss: 0.259110, train accuracy: 92.85%, validation loss: 0.289317, validation accuracy: 92.30%\n",
      "step: 355000, batch loss: 0.447899, train loss: 0.258807, train accuracy: 92.79%, validation loss: 0.289748, validation accuracy: 92.22%\n",
      "step: 356000, batch loss: 0.059235, train loss: 0.258879, train accuracy: 92.84%, validation loss: 0.288156, validation accuracy: 92.38%\n",
      "step: 357000, batch loss: 0.079750, train loss: 0.261409, train accuracy: 92.75%, validation loss: 0.292453, validation accuracy: 92.08%\n",
      "step: 358000, batch loss: 0.083422, train loss: 0.261193, train accuracy: 92.91%, validation loss: 0.293277, validation accuracy: 92.20%\n",
      "step: 359000, batch loss: 0.518566, train loss: 0.259058, train accuracy: 92.82%, validation loss: 0.289173, validation accuracy: 92.40%\n",
      "step: 360000, batch loss: 0.192866, train loss: 0.260960, train accuracy: 92.74%, validation loss: 0.289983, validation accuracy: 92.18%\n",
      "step: 361000, batch loss: 0.174259, train loss: 0.259343, train accuracy: 92.78%, validation loss: 0.287317, validation accuracy: 92.42%\n",
      "step: 362000, batch loss: 0.183358, train loss: 0.260490, train accuracy: 92.76%, validation loss: 0.289509, validation accuracy: 92.34%\n",
      "step: 363000, batch loss: 0.044707, train loss: 0.257804, train accuracy: 92.83%, validation loss: 0.288920, validation accuracy: 92.38%\n",
      "step: 364000, batch loss: 0.230828, train loss: 0.258450, train accuracy: 92.83%, validation loss: 0.290804, validation accuracy: 92.10%\n",
      "step: 365000, batch loss: 0.308034, train loss: 0.260407, train accuracy: 92.77%, validation loss: 0.291032, validation accuracy: 92.20%\n",
      "step: 366000, batch loss: 0.633802, train loss: 0.259112, train accuracy: 92.83%, validation loss: 0.292647, validation accuracy: 92.18%\n",
      "step: 367000, batch loss: 0.180901, train loss: 0.259784, train accuracy: 92.73%, validation loss: 0.290954, validation accuracy: 92.04%\n",
      "step: 368000, batch loss: 0.492179, train loss: 0.259926, train accuracy: 92.64%, validation loss: 0.290901, validation accuracy: 92.08%\n",
      "step: 369000, batch loss: 0.079836, train loss: 0.258422, train accuracy: 92.80%, validation loss: 0.287596, validation accuracy: 92.38%\n",
      "step: 370000, batch loss: 0.704277, train loss: 0.258861, train accuracy: 92.86%, validation loss: 0.291365, validation accuracy: 92.18%\n",
      "step: 371000, batch loss: 0.046631, train loss: 0.260174, train accuracy: 92.73%, validation loss: 0.289102, validation accuracy: 92.26%\n",
      "step: 372000, batch loss: 0.269267, train loss: 0.258303, train accuracy: 92.89%, validation loss: 0.288702, validation accuracy: 92.52%\n",
      "step: 373000, batch loss: 0.056645, train loss: 0.257135, train accuracy: 92.86%, validation loss: 0.287875, validation accuracy: 92.32%\n",
      "step: 374000, batch loss: 0.015327, train loss: 0.257304, train accuracy: 92.86%, validation loss: 0.287402, validation accuracy: 92.48%\n",
      "step: 375000, batch loss: 0.037989, train loss: 0.263567, train accuracy: 92.59%, validation loss: 0.294315, validation accuracy: 92.02%\n",
      "step: 376000, batch loss: 0.145774, train loss: 0.258194, train accuracy: 92.83%, validation loss: 0.287809, validation accuracy: 92.56%\n",
      "step: 377000, batch loss: 0.037862, train loss: 0.256484, train accuracy: 92.90%, validation loss: 0.288475, validation accuracy: 92.34%\n",
      "step: 378000, batch loss: 0.164406, train loss: 0.259943, train accuracy: 92.73%, validation loss: 0.293126, validation accuracy: 91.78%\n",
      "step: 379000, batch loss: 0.602373, train loss: 0.257516, train accuracy: 92.81%, validation loss: 0.287633, validation accuracy: 92.28%\n",
      "step: 380000, batch loss: 0.188876, train loss: 0.258598, train accuracy: 92.79%, validation loss: 0.289202, validation accuracy: 92.26%\n",
      "step: 381000, batch loss: 0.324249, train loss: 0.259078, train accuracy: 92.88%, validation loss: 0.290692, validation accuracy: 92.12%\n",
      "step: 382000, batch loss: 0.144931, train loss: 0.256736, train accuracy: 92.85%, validation loss: 0.289061, validation accuracy: 92.18%\n",
      "step: 383000, batch loss: 0.190449, train loss: 0.259280, train accuracy: 92.89%, validation loss: 0.293264, validation accuracy: 92.38%\n",
      "step: 384000, batch loss: 0.071832, train loss: 0.256820, train accuracy: 92.91%, validation loss: 0.288158, validation accuracy: 92.54%\n",
      "step: 385000, batch loss: 0.198517, train loss: 0.256332, train accuracy: 92.93%, validation loss: 0.288713, validation accuracy: 92.30%\n",
      "step: 386000, batch loss: 0.054179, train loss: 0.259005, train accuracy: 92.84%, validation loss: 0.292519, validation accuracy: 92.02%\n",
      "step: 387000, batch loss: 0.088281, train loss: 0.257410, train accuracy: 92.91%, validation loss: 0.289349, validation accuracy: 92.28%\n",
      "step: 388000, batch loss: 0.270772, train loss: 0.256063, train accuracy: 92.87%, validation loss: 0.289363, validation accuracy: 92.18%\n",
      "step: 389000, batch loss: 1.161276, train loss: 0.257292, train accuracy: 92.90%, validation loss: 0.290328, validation accuracy: 92.12%\n",
      "step: 390000, batch loss: 0.088142, train loss: 0.257232, train accuracy: 92.91%, validation loss: 0.289540, validation accuracy: 92.32%\n",
      "step: 391000, batch loss: 0.177242, train loss: 0.257345, train accuracy: 92.92%, validation loss: 0.288408, validation accuracy: 92.32%\n",
      "step: 392000, batch loss: 0.050825, train loss: 0.255941, train accuracy: 92.86%, validation loss: 0.286133, validation accuracy: 92.34%\n",
      "step: 393000, batch loss: 0.104554, train loss: 0.257215, train accuracy: 92.89%, validation loss: 0.286994, validation accuracy: 92.32%\n",
      "step: 394000, batch loss: 0.113100, train loss: 0.258094, train accuracy: 92.83%, validation loss: 0.286648, validation accuracy: 92.70%\n",
      "step: 395000, batch loss: 0.259603, train loss: 0.255756, train accuracy: 92.90%, validation loss: 0.287363, validation accuracy: 92.30%\n",
      "step: 396000, batch loss: 0.312521, train loss: 0.256305, train accuracy: 92.89%, validation loss: 0.287191, validation accuracy: 92.38%\n",
      "step: 397000, batch loss: 0.044410, train loss: 0.256270, train accuracy: 92.93%, validation loss: 0.286845, validation accuracy: 92.48%\n",
      "step: 398000, batch loss: 0.045362, train loss: 0.256960, train accuracy: 92.84%, validation loss: 0.290201, validation accuracy: 92.38%\n",
      "step: 399000, batch loss: 0.069320, train loss: 0.257039, train accuracy: 92.80%, validation loss: 0.288642, validation accuracy: 92.06%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFIqS/8PD1csblHAIMUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112080690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression(\n",
    "    learning_rate=0.02,\n",
    "    steps = 400 * 1000,\n",
    "    l2_regularization_strength=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with LeRU and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunNeuralNetwork(learning_rate = 0.01,\n",
    "                     batch_size = 16,\n",
    "                     steps = 100 * 1000,\n",
    "                     sample = 5000,\n",
    "                     dropout_keep_prob = 1.0,\n",
    "                     hidden_sizes = [100, 75, 50]):\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer = inputs\n",
    "        for hidden_size in hidden_sizes:\n",
    "            input_size = layer.get_shape()[1].value\n",
    "            weights = tf.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1))\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[hidden_size]))\n",
    "            layer = tf.nn.relu(tf.matmul(layer, weights) + biases)\n",
    "            if dropout_keep_prob < 1.0:\n",
    "                layer = tf.nn.dropout(layer, keep_prob)\n",
    "\n",
    "        input_size = layer.get_shape()[1].value\n",
    "        weights = tf.Variable(tf.truncated_normal([input_size, num_classes], stddev=0.1))\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(layer, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "        # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)    \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # This must be called after all Variable definitions.\n",
    "        init_variables = tf.initialize_all_variables()\n",
    "\n",
    "    step_records = []\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label, keep_prob: dropout_keep_prob})\n",
    "            if step % sample == 0 or step == steps - 1:\n",
    "                batch_entropy = sess.run(\n",
    "                    cross_entropy,\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                train_entropy, train_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.train.images, labels: mnist.train.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                validation_entropy, validation_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                print 'step: %d, batch loss: %.4f, train loss: %f, train accuracy: %.2f%%, validation loss: %.4f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if train_entropy < 0.5:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.3219, train loss: 2.322336, train accuracy: 12.45%, validation loss: 2.3219, validation accuracy: 13.00%\n",
      "step: 5000, batch loss: 0.2438, train loss: 0.256495, train accuracy: 92.61%, validation loss: 0.2438, validation accuracy: 93.08%\n",
      "step: 10000, batch loss: 0.1753, train loss: 0.181387, train accuracy: 94.80%, validation loss: 0.1753, validation accuracy: 95.10%\n",
      "step: 15000, batch loss: 0.1476, train loss: 0.143394, train accuracy: 95.81%, validation loss: 0.1476, validation accuracy: 95.78%\n",
      "step: 20000, batch loss: 0.1225, train loss: 0.112974, train accuracy: 96.81%, validation loss: 0.1225, validation accuracy: 96.74%\n",
      "step: 25000, batch loss: 0.1177, train loss: 0.103648, train accuracy: 96.87%, validation loss: 0.1177, validation accuracy: 96.64%\n",
      "step: 30000, batch loss: 0.1114, train loss: 0.086415, train accuracy: 97.38%, validation loss: 0.1114, validation accuracy: 96.86%\n",
      "step: 35000, batch loss: 0.1030, train loss: 0.075174, train accuracy: 97.80%, validation loss: 0.1030, validation accuracy: 97.12%\n",
      "step: 40000, batch loss: 0.0974, train loss: 0.061350, train accuracy: 98.22%, validation loss: 0.0974, validation accuracy: 97.22%\n",
      "step: 45000, batch loss: 0.0949, train loss: 0.053592, train accuracy: 98.42%, validation loss: 0.0949, validation accuracy: 97.26%\n",
      "step: 50000, batch loss: 0.0936, train loss: 0.047333, train accuracy: 98.62%, validation loss: 0.0936, validation accuracy: 97.24%\n",
      "step: 55000, batch loss: 0.0937, train loss: 0.040713, train accuracy: 98.87%, validation loss: 0.0937, validation accuracy: 97.38%\n",
      "step: 60000, batch loss: 0.0929, train loss: 0.036937, train accuracy: 98.94%, validation loss: 0.0929, validation accuracy: 97.50%\n",
      "step: 65000, batch loss: 0.0938, train loss: 0.029085, train accuracy: 99.23%, validation loss: 0.0938, validation accuracy: 97.32%\n",
      "step: 70000, batch loss: 0.0959, train loss: 0.026904, train accuracy: 99.28%, validation loss: 0.0959, validation accuracy: 97.38%\n",
      "step: 75000, batch loss: 0.0954, train loss: 0.020678, train accuracy: 99.50%, validation loss: 0.0954, validation accuracy: 97.52%\n",
      "step: 80000, batch loss: 0.0990, train loss: 0.017836, train accuracy: 99.59%, validation loss: 0.0990, validation accuracy: 97.32%\n",
      "step: 85000, batch loss: 0.1032, train loss: 0.014932, train accuracy: 99.68%, validation loss: 0.1032, validation accuracy: 97.44%\n",
      "step: 90000, batch loss: 0.1012, train loss: 0.016818, train accuracy: 99.55%, validation loss: 0.1012, validation accuracy: 97.56%\n",
      "step: 95000, batch loss: 0.1135, train loss: 0.014234, train accuracy: 99.66%, validation loss: 0.1135, validation accuracy: 97.48%\n",
      "step: 99999, batch loss: 0.1171, train loss: 0.013184, train accuracy: 99.64%, validation loss: 0.1171, validation accuracy: 97.42%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKKKkm1+NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110744f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.3943, train loss: 2.388436, train accuracy: 12.43%, validation loss: 2.3943, validation accuracy: 11.94%\n",
      "step: 5000, batch loss: 0.3721, train loss: 0.377826, train accuracy: 89.47%, validation loss: 0.3721, validation accuracy: 89.84%\n",
      "step: 10000, batch loss: 0.2324, train loss: 0.236560, train accuracy: 92.91%, validation loss: 0.2324, validation accuracy: 93.30%\n",
      "step: 15000, batch loss: 0.1879, train loss: 0.184888, train accuracy: 94.52%, validation loss: 0.1879, validation accuracy: 94.66%\n",
      "step: 20000, batch loss: 0.1626, train loss: 0.155641, train accuracy: 95.42%, validation loss: 0.1626, validation accuracy: 95.36%\n",
      "step: 25000, batch loss: 0.1391, train loss: 0.130095, train accuracy: 96.12%, validation loss: 0.1391, validation accuracy: 96.28%\n",
      "step: 30000, batch loss: 0.1258, train loss: 0.113720, train accuracy: 96.57%, validation loss: 0.1258, validation accuracy: 96.42%\n",
      "step: 35000, batch loss: 0.1218, train loss: 0.101529, train accuracy: 97.02%, validation loss: 0.1218, validation accuracy: 96.72%\n",
      "step: 40000, batch loss: 0.1116, train loss: 0.090354, train accuracy: 97.37%, validation loss: 0.1116, validation accuracy: 96.80%\n",
      "step: 45000, batch loss: 0.1039, train loss: 0.080313, train accuracy: 97.67%, validation loss: 0.1039, validation accuracy: 97.10%\n",
      "step: 50000, batch loss: 0.1018, train loss: 0.074101, train accuracy: 97.85%, validation loss: 0.1018, validation accuracy: 97.24%\n",
      "step: 55000, batch loss: 0.1021, train loss: 0.068952, train accuracy: 97.99%, validation loss: 0.1021, validation accuracy: 97.28%\n",
      "step: 60000, batch loss: 0.0977, train loss: 0.062234, train accuracy: 98.15%, validation loss: 0.0977, validation accuracy: 97.46%\n",
      "step: 65000, batch loss: 0.0958, train loss: 0.059057, train accuracy: 98.28%, validation loss: 0.0958, validation accuracy: 97.22%\n",
      "step: 70000, batch loss: 0.0920, train loss: 0.053890, train accuracy: 98.41%, validation loss: 0.0920, validation accuracy: 97.52%\n",
      "step: 75000, batch loss: 0.0910, train loss: 0.051837, train accuracy: 98.48%, validation loss: 0.0910, validation accuracy: 97.56%\n",
      "step: 80000, batch loss: 0.0872, train loss: 0.046905, train accuracy: 98.61%, validation loss: 0.0872, validation accuracy: 97.62%\n",
      "step: 85000, batch loss: 0.0882, train loss: 0.045024, train accuracy: 98.67%, validation loss: 0.0882, validation accuracy: 97.58%\n",
      "step: 90000, batch loss: 0.0856, train loss: 0.041602, train accuracy: 98.78%, validation loss: 0.0856, validation accuracy: 97.60%\n",
      "step: 95000, batch loss: 0.0835, train loss: 0.039063, train accuracy: 98.86%, validation loss: 0.0835, validation accuracy: 97.76%\n",
      "step: 100000, batch loss: 0.0844, train loss: 0.038904, train accuracy: 98.90%, validation loss: 0.0844, validation accuracy: 97.74%\n",
      "step: 105000, batch loss: 0.0838, train loss: 0.035891, train accuracy: 98.92%, validation loss: 0.0838, validation accuracy: 97.64%\n",
      "step: 110000, batch loss: 0.0845, train loss: 0.033675, train accuracy: 98.99%, validation loss: 0.0845, validation accuracy: 97.66%\n",
      "step: 115000, batch loss: 0.0854, train loss: 0.031822, train accuracy: 99.06%, validation loss: 0.0854, validation accuracy: 97.70%\n",
      "step: 120000, batch loss: 0.0854, train loss: 0.030263, train accuracy: 99.13%, validation loss: 0.0854, validation accuracy: 97.70%\n",
      "step: 125000, batch loss: 0.0817, train loss: 0.028828, train accuracy: 99.15%, validation loss: 0.0817, validation accuracy: 97.80%\n",
      "step: 130000, batch loss: 0.0854, train loss: 0.027859, train accuracy: 99.17%, validation loss: 0.0854, validation accuracy: 97.62%\n",
      "step: 135000, batch loss: 0.0820, train loss: 0.025981, train accuracy: 99.23%, validation loss: 0.0820, validation accuracy: 97.82%\n",
      "step: 140000, batch loss: 0.0843, train loss: 0.026338, train accuracy: 99.26%, validation loss: 0.0843, validation accuracy: 97.80%\n",
      "step: 145000, batch loss: 0.0846, train loss: 0.023534, train accuracy: 99.31%, validation loss: 0.0846, validation accuracy: 98.00%\n",
      "step: 150000, batch loss: 0.0856, train loss: 0.022233, train accuracy: 99.37%, validation loss: 0.0856, validation accuracy: 97.78%\n",
      "step: 155000, batch loss: 0.0827, train loss: 0.022235, train accuracy: 99.38%, validation loss: 0.0827, validation accuracy: 97.90%\n",
      "step: 160000, batch loss: 0.0808, train loss: 0.020686, train accuracy: 99.39%, validation loss: 0.0808, validation accuracy: 97.86%\n",
      "step: 165000, batch loss: 0.0851, train loss: 0.020083, train accuracy: 99.39%, validation loss: 0.0851, validation accuracy: 97.94%\n",
      "step: 170000, batch loss: 0.0839, train loss: 0.019358, train accuracy: 99.45%, validation loss: 0.0839, validation accuracy: 98.04%\n",
      "step: 175000, batch loss: 0.0862, train loss: 0.018423, train accuracy: 99.47%, validation loss: 0.0862, validation accuracy: 97.80%\n",
      "step: 180000, batch loss: 0.0868, train loss: 0.017299, train accuracy: 99.50%, validation loss: 0.0868, validation accuracy: 97.86%\n",
      "step: 185000, batch loss: 0.0833, train loss: 0.016931, train accuracy: 99.51%, validation loss: 0.0833, validation accuracy: 98.06%\n",
      "step: 190000, batch loss: 0.0847, train loss: 0.016800, train accuracy: 99.52%, validation loss: 0.0847, validation accuracy: 97.86%\n",
      "step: 195000, batch loss: 0.0867, train loss: 0.015823, train accuracy: 99.53%, validation loss: 0.0867, validation accuracy: 97.92%\n",
      "step: 199999, batch loss: 0.0842, train loss: 0.015894, train accuracy: 99.51%, validation loss: 0.0842, validation accuracy: 98.08%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRER6pKAQEZEe/T893YfG1iVc3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1112d7f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork(\n",
    "    dropout_keep_prob=0.5,\n",
    "    steps = 200 * 1000,\n",
    "    hidden_sizes = [200, 150, 100],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
