{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import contextlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "image_size = 28 * 28\n",
    "num_classes = 10\n",
    "assert mnist.train.images.shape[1] == image_size\n",
    "assert mnist.train.labels.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Logistic Regression\n",
    "References\n",
    "- [MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html)\n",
    "- [L2 Reguralization example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py)\n",
    "  - How is `tf.nn.l2_loss` different from square and reduce_sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def RunLogsticRegression(learning_rate = 0.05,\n",
    "                         batch_size = 8,\n",
    "                         steps = 200 * 1000,\n",
    "                         sample = 1000,\n",
    "                         l2_regularization_strength = 0.0):\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    step_records = []\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "        weights = tf.Variable(tf.truncated_normal([image_size, num_classes], stddev=1), name='a')\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(inputs, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "        \n",
    "        loss = cross_entropy\n",
    "        if l2_regularization_strength > 0:\n",
    "            regularizer = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "            loss += l2_regularization_strength * regularizer\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        init_variables = tf.initialize_all_variables()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label})\n",
    "            if step % sample == 0:\n",
    "                batch_entropy, batch_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: batch_input, labels: batch_label})\n",
    "                train_entropy, train_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                         {inputs: mnist.train.images, labels: mnist.train.labels})\n",
    "                validation_entropy, validation_accuracy = sess.run((cross_entropy, accuracy),\n",
    "                                                                   {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "                print 'step: %d, batch loss: %f, train loss: %f, train accuracy: %.2f%%, validation loss: %f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if step > 10000:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 13.133772, train loss: 14.831784, train accuracy: 4.33%, validation loss: 14.903664, validation accuracy: 4.28%\n",
      "step: 1000, batch loss: 1.778423, train loss: 1.197491, train accuracy: 73.69%, validation loss: 1.117037, validation accuracy: 74.94%\n",
      "step: 2000, batch loss: 1.123490, train loss: 0.837739, train accuracy: 80.59%, validation loss: 0.796243, validation accuracy: 81.64%\n",
      "step: 3000, batch loss: 0.461213, train loss: 0.703239, train accuracy: 83.59%, validation loss: 0.659485, validation accuracy: 84.82%\n",
      "step: 4000, batch loss: 0.280273, train loss: 0.658648, train accuracy: 84.85%, validation loss: 0.624753, validation accuracy: 85.72%\n",
      "step: 5000, batch loss: 0.709875, train loss: 0.611246, train accuracy: 85.53%, validation loss: 0.576764, validation accuracy: 86.46%\n",
      "step: 6000, batch loss: 0.619224, train loss: 0.567790, train accuracy: 86.11%, validation loss: 0.547876, validation accuracy: 86.78%\n",
      "step: 7000, batch loss: 0.812442, train loss: 0.538390, train accuracy: 87.11%, validation loss: 0.518068, validation accuracy: 87.74%\n",
      "step: 8000, batch loss: 0.263009, train loss: 0.522481, train accuracy: 87.26%, validation loss: 0.506215, validation accuracy: 87.54%\n",
      "step: 9000, batch loss: 0.354969, train loss: 0.513107, train accuracy: 87.41%, validation loss: 0.500812, validation accuracy: 87.90%\n",
      "step: 10000, batch loss: 0.171883, train loss: 0.498400, train accuracy: 87.62%, validation loss: 0.486536, validation accuracy: 88.56%\n",
      "step: 11000, batch loss: 0.264883, train loss: 0.477820, train accuracy: 88.24%, validation loss: 0.462298, validation accuracy: 88.82%\n",
      "step: 12000, batch loss: 0.294068, train loss: 0.473397, train accuracy: 88.14%, validation loss: 0.460729, validation accuracy: 88.84%\n",
      "step: 13000, batch loss: 0.990197, train loss: 0.449590, train accuracy: 88.76%, validation loss: 0.448713, validation accuracy: 88.98%\n",
      "step: 14000, batch loss: 0.579006, train loss: 0.445167, train accuracy: 89.00%, validation loss: 0.441131, validation accuracy: 89.22%\n",
      "step: 15000, batch loss: 0.105830, train loss: 0.434725, train accuracy: 89.22%, validation loss: 0.436402, validation accuracy: 89.16%\n",
      "step: 16000, batch loss: 0.014418, train loss: 0.436030, train accuracy: 88.93%, validation loss: 0.435305, validation accuracy: 88.70%\n",
      "step: 17000, batch loss: 0.140269, train loss: 0.422393, train accuracy: 89.40%, validation loss: 0.423518, validation accuracy: 89.36%\n",
      "step: 18000, batch loss: 0.083118, train loss: 0.410336, train accuracy: 89.53%, validation loss: 0.412872, validation accuracy: 89.34%\n",
      "step: 19000, batch loss: 0.233313, train loss: 0.412235, train accuracy: 89.62%, validation loss: 0.408998, validation accuracy: 89.80%\n",
      "step: 20000, batch loss: 0.890926, train loss: 0.402835, train accuracy: 89.71%, validation loss: 0.405657, validation accuracy: 89.74%\n",
      "step: 21000, batch loss: 0.097624, train loss: 0.397195, train accuracy: 89.91%, validation loss: 0.399239, validation accuracy: 89.98%\n",
      "step: 22000, batch loss: 1.333018, train loss: 0.391780, train accuracy: 90.00%, validation loss: 0.394324, validation accuracy: 90.16%\n",
      "step: 23000, batch loss: 0.174169, train loss: 0.395727, train accuracy: 89.87%, validation loss: 0.402466, validation accuracy: 89.58%\n",
      "step: 24000, batch loss: 0.767565, train loss: 0.382610, train accuracy: 90.16%, validation loss: 0.388466, validation accuracy: 89.94%\n",
      "step: 25000, batch loss: 0.019599, train loss: 0.377819, train accuracy: 90.29%, validation loss: 0.384057, validation accuracy: 90.44%\n",
      "step: 26000, batch loss: 1.368634, train loss: 0.395879, train accuracy: 89.59%, validation loss: 0.406022, validation accuracy: 89.38%\n",
      "step: 27000, batch loss: 0.169489, train loss: 0.374191, train accuracy: 90.35%, validation loss: 0.379742, validation accuracy: 89.90%\n",
      "step: 28000, batch loss: 0.568967, train loss: 0.369767, train accuracy: 90.58%, validation loss: 0.373127, validation accuracy: 90.48%\n",
      "step: 29000, batch loss: 0.227671, train loss: 0.368909, train accuracy: 90.46%, validation loss: 0.375342, validation accuracy: 90.42%\n",
      "step: 30000, batch loss: 0.148678, train loss: 0.369126, train accuracy: 90.40%, validation loss: 0.376201, validation accuracy: 90.12%\n",
      "step: 31000, batch loss: 0.281162, train loss: 0.362387, train accuracy: 90.50%, validation loss: 0.365313, validation accuracy: 90.50%\n",
      "step: 32000, batch loss: 0.246217, train loss: 0.360512, train accuracy: 90.50%, validation loss: 0.366773, validation accuracy: 90.22%\n",
      "step: 33000, batch loss: 0.289707, train loss: 0.361918, train accuracy: 90.54%, validation loss: 0.370941, validation accuracy: 90.42%\n",
      "step: 34000, batch loss: 0.624133, train loss: 0.351815, train accuracy: 90.87%, validation loss: 0.365946, validation accuracy: 90.22%\n",
      "step: 35000, batch loss: 0.057858, train loss: 0.350378, train accuracy: 90.93%, validation loss: 0.359597, validation accuracy: 90.80%\n",
      "step: 36000, batch loss: 0.034793, train loss: 0.347336, train accuracy: 90.89%, validation loss: 0.355887, validation accuracy: 90.68%\n",
      "step: 37000, batch loss: 0.060765, train loss: 0.348835, train accuracy: 90.85%, validation loss: 0.360609, validation accuracy: 90.64%\n",
      "step: 38000, batch loss: 0.071522, train loss: 0.347659, train accuracy: 90.96%, validation loss: 0.351230, validation accuracy: 90.92%\n",
      "step: 39000, batch loss: 0.052015, train loss: 0.340829, train accuracy: 91.05%, validation loss: 0.351833, validation accuracy: 90.52%\n",
      "step: 40000, batch loss: 0.093606, train loss: 0.341176, train accuracy: 91.02%, validation loss: 0.351267, validation accuracy: 90.66%\n",
      "step: 41000, batch loss: 0.197340, train loss: 0.345368, train accuracy: 90.91%, validation loss: 0.360988, validation accuracy: 90.22%\n",
      "step: 42000, batch loss: 0.046640, train loss: 0.342334, train accuracy: 91.03%, validation loss: 0.353989, validation accuracy: 90.42%\n",
      "step: 43000, batch loss: 0.125518, train loss: 0.337417, train accuracy: 91.13%, validation loss: 0.352153, validation accuracy: 90.48%\n",
      "step: 44000, batch loss: 0.053315, train loss: 0.336511, train accuracy: 91.10%, validation loss: 0.350237, validation accuracy: 91.00%\n",
      "step: 45000, batch loss: 0.078711, train loss: 0.334169, train accuracy: 91.29%, validation loss: 0.348727, validation accuracy: 90.70%\n",
      "step: 46000, batch loss: 0.138104, train loss: 0.337843, train accuracy: 91.10%, validation loss: 0.348077, validation accuracy: 90.66%\n",
      "step: 47000, batch loss: 0.493800, train loss: 0.326849, train accuracy: 91.39%, validation loss: 0.345482, validation accuracy: 90.76%\n",
      "step: 48000, batch loss: 0.314079, train loss: 0.323421, train accuracy: 91.56%, validation loss: 0.339975, validation accuracy: 90.96%\n",
      "step: 49000, batch loss: 0.558318, train loss: 0.323272, train accuracy: 91.55%, validation loss: 0.333319, validation accuracy: 91.44%\n",
      "step: 50000, batch loss: 0.155940, train loss: 0.323177, train accuracy: 91.50%, validation loss: 0.339209, validation accuracy: 91.04%\n",
      "step: 51000, batch loss: 0.917280, train loss: 0.323305, train accuracy: 91.48%, validation loss: 0.337046, validation accuracy: 91.28%\n",
      "step: 52000, batch loss: 0.122254, train loss: 0.320197, train accuracy: 91.46%, validation loss: 0.338674, validation accuracy: 90.80%\n",
      "step: 53000, batch loss: 0.011465, train loss: 0.315867, train accuracy: 91.59%, validation loss: 0.330901, validation accuracy: 91.28%\n",
      "step: 54000, batch loss: 0.234118, train loss: 0.315608, train accuracy: 91.69%, validation loss: 0.330921, validation accuracy: 91.16%\n",
      "step: 55000, batch loss: 0.077448, train loss: 0.326890, train accuracy: 91.20%, validation loss: 0.342426, validation accuracy: 90.94%\n",
      "step: 56000, batch loss: 0.072255, train loss: 0.315785, train accuracy: 91.62%, validation loss: 0.333935, validation accuracy: 91.22%\n",
      "step: 57000, batch loss: 0.013942, train loss: 0.322886, train accuracy: 91.24%, validation loss: 0.340487, validation accuracy: 91.18%\n",
      "step: 58000, batch loss: 0.101793, train loss: 0.313880, train accuracy: 91.69%, validation loss: 0.328316, validation accuracy: 91.56%\n",
      "step: 59000, batch loss: 0.019594, train loss: 0.313106, train accuracy: 91.66%, validation loss: 0.330634, validation accuracy: 91.10%\n",
      "step: 60000, batch loss: 0.017899, train loss: 0.309905, train accuracy: 91.80%, validation loss: 0.327360, validation accuracy: 90.84%\n",
      "step: 61000, batch loss: 0.121176, train loss: 0.312886, train accuracy: 91.63%, validation loss: 0.334554, validation accuracy: 90.88%\n",
      "step: 62000, batch loss: 0.012930, train loss: 0.317947, train accuracy: 91.43%, validation loss: 0.333366, validation accuracy: 91.12%\n",
      "step: 63000, batch loss: 0.076381, train loss: 0.315237, train accuracy: 91.53%, validation loss: 0.336408, validation accuracy: 91.20%\n",
      "step: 64000, batch loss: 0.165439, train loss: 0.311211, train accuracy: 91.68%, validation loss: 0.324679, validation accuracy: 91.42%\n",
      "step: 65000, batch loss: 0.255653, train loss: 0.316167, train accuracy: 91.49%, validation loss: 0.330189, validation accuracy: 91.28%\n",
      "step: 66000, batch loss: 0.259922, train loss: 0.306572, train accuracy: 91.86%, validation loss: 0.322913, validation accuracy: 91.52%\n",
      "step: 67000, batch loss: 0.075373, train loss: 0.303766, train accuracy: 91.88%, validation loss: 0.319614, validation accuracy: 91.38%\n",
      "step: 68000, batch loss: 0.438680, train loss: 0.301244, train accuracy: 91.95%, validation loss: 0.318858, validation accuracy: 91.62%\n",
      "step: 69000, batch loss: 0.092922, train loss: 0.299911, train accuracy: 92.01%, validation loss: 0.323016, validation accuracy: 91.36%\n",
      "step: 70000, batch loss: 0.472187, train loss: 0.303352, train accuracy: 91.82%, validation loss: 0.326543, validation accuracy: 91.28%\n",
      "step: 71000, batch loss: 0.042409, train loss: 0.304615, train accuracy: 91.85%, validation loss: 0.323446, validation accuracy: 91.40%\n",
      "step: 72000, batch loss: 0.009432, train loss: 0.299205, train accuracy: 91.89%, validation loss: 0.320226, validation accuracy: 91.28%\n",
      "step: 73000, batch loss: 0.633133, train loss: 0.303062, train accuracy: 91.83%, validation loss: 0.322137, validation accuracy: 91.12%\n",
      "step: 74000, batch loss: 0.036856, train loss: 0.300998, train accuracy: 92.00%, validation loss: 0.317789, validation accuracy: 91.50%\n",
      "step: 75000, batch loss: 0.225884, train loss: 0.306910, train accuracy: 91.81%, validation loss: 0.324523, validation accuracy: 91.32%\n",
      "step: 76000, batch loss: 0.694269, train loss: 0.296622, train accuracy: 92.04%, validation loss: 0.313788, validation accuracy: 91.52%\n",
      "step: 77000, batch loss: 0.118218, train loss: 0.304555, train accuracy: 91.77%, validation loss: 0.326410, validation accuracy: 91.00%\n",
      "step: 78000, batch loss: 0.022690, train loss: 0.296764, train accuracy: 92.09%, validation loss: 0.317796, validation accuracy: 91.52%\n",
      "step: 79000, batch loss: 0.174713, train loss: 0.301629, train accuracy: 91.66%, validation loss: 0.323689, validation accuracy: 91.18%\n",
      "step: 80000, batch loss: 0.067385, train loss: 0.301302, train accuracy: 91.92%, validation loss: 0.318612, validation accuracy: 91.28%\n",
      "step: 81000, batch loss: 0.007294, train loss: 0.300893, train accuracy: 91.80%, validation loss: 0.323870, validation accuracy: 91.38%\n",
      "step: 82000, batch loss: 0.055709, train loss: 0.293079, train accuracy: 92.05%, validation loss: 0.313508, validation accuracy: 91.60%\n",
      "step: 83000, batch loss: 0.484183, train loss: 0.291792, train accuracy: 92.19%, validation loss: 0.316499, validation accuracy: 91.48%\n",
      "step: 84000, batch loss: 0.042417, train loss: 0.291929, train accuracy: 92.15%, validation loss: 0.313030, validation accuracy: 91.44%\n",
      "step: 85000, batch loss: 0.068286, train loss: 0.293032, train accuracy: 92.17%, validation loss: 0.318673, validation accuracy: 91.12%\n",
      "step: 86000, batch loss: 0.067448, train loss: 0.290376, train accuracy: 92.17%, validation loss: 0.312633, validation accuracy: 91.42%\n",
      "step: 87000, batch loss: 0.039180, train loss: 0.293423, train accuracy: 92.01%, validation loss: 0.313673, validation accuracy: 91.36%\n",
      "step: 88000, batch loss: 0.098090, train loss: 0.295831, train accuracy: 91.96%, validation loss: 0.318065, validation accuracy: 91.38%\n",
      "step: 89000, batch loss: 0.014993, train loss: 0.294838, train accuracy: 92.13%, validation loss: 0.313707, validation accuracy: 91.52%\n",
      "step: 90000, batch loss: 0.060929, train loss: 0.287184, train accuracy: 92.40%, validation loss: 0.308504, validation accuracy: 92.10%\n",
      "step: 91000, batch loss: 0.005583, train loss: 0.285972, train accuracy: 92.30%, validation loss: 0.307265, validation accuracy: 91.80%\n",
      "step: 92000, batch loss: 0.103097, train loss: 0.286706, train accuracy: 92.28%, validation loss: 0.307901, validation accuracy: 91.86%\n",
      "step: 93000, batch loss: 0.083106, train loss: 0.286129, train accuracy: 92.32%, validation loss: 0.307309, validation accuracy: 91.86%\n",
      "step: 94000, batch loss: 0.114631, train loss: 0.284109, train accuracy: 92.35%, validation loss: 0.305508, validation accuracy: 91.78%\n",
      "step: 95000, batch loss: 0.187966, train loss: 0.286859, train accuracy: 92.27%, validation loss: 0.311852, validation accuracy: 91.68%\n",
      "step: 96000, batch loss: 0.030470, train loss: 0.288871, train accuracy: 92.25%, validation loss: 0.309198, validation accuracy: 91.58%\n",
      "step: 97000, batch loss: 1.275023, train loss: 0.281286, train accuracy: 92.50%, validation loss: 0.303525, validation accuracy: 92.06%\n",
      "step: 98000, batch loss: 0.120136, train loss: 0.287255, train accuracy: 92.22%, validation loss: 0.312327, validation accuracy: 91.48%\n",
      "step: 99000, batch loss: 0.195859, train loss: 0.283326, train accuracy: 92.39%, validation loss: 0.306686, validation accuracy: 91.86%\n",
      "step: 100000, batch loss: 0.541802, train loss: 0.284278, train accuracy: 92.43%, validation loss: 0.311404, validation accuracy: 91.56%\n",
      "step: 101000, batch loss: 0.642690, train loss: 0.285439, train accuracy: 92.20%, validation loss: 0.303014, validation accuracy: 92.00%\n",
      "step: 102000, batch loss: 0.005702, train loss: 0.283624, train accuracy: 92.34%, validation loss: 0.304300, validation accuracy: 91.64%\n",
      "step: 103000, batch loss: 0.038385, train loss: 0.279962, train accuracy: 92.59%, validation loss: 0.304594, validation accuracy: 91.66%\n",
      "step: 104000, batch loss: 0.409718, train loss: 0.281421, train accuracy: 92.37%, validation loss: 0.305926, validation accuracy: 91.78%\n",
      "step: 105000, batch loss: 0.489154, train loss: 0.283879, train accuracy: 92.32%, validation loss: 0.312282, validation accuracy: 91.64%\n",
      "step: 106000, batch loss: 0.320891, train loss: 0.288781, train accuracy: 92.08%, validation loss: 0.311906, validation accuracy: 91.56%\n",
      "step: 107000, batch loss: 0.097331, train loss: 0.280798, train accuracy: 92.38%, validation loss: 0.308350, validation accuracy: 91.64%\n",
      "step: 108000, batch loss: 0.382515, train loss: 0.289873, train accuracy: 92.20%, validation loss: 0.314493, validation accuracy: 91.44%\n",
      "step: 109000, batch loss: 0.073838, train loss: 0.288488, train accuracy: 92.21%, validation loss: 0.306317, validation accuracy: 91.74%\n",
      "step: 110000, batch loss: 0.168962, train loss: 0.287581, train accuracy: 92.21%, validation loss: 0.304696, validation accuracy: 91.86%\n",
      "step: 111000, batch loss: 0.088263, train loss: 0.283355, train accuracy: 92.35%, validation loss: 0.307975, validation accuracy: 91.60%\n",
      "step: 112000, batch loss: 0.081773, train loss: 0.279792, train accuracy: 92.36%, validation loss: 0.301637, validation accuracy: 91.72%\n",
      "step: 113000, batch loss: 0.074218, train loss: 0.289058, train accuracy: 91.92%, validation loss: 0.310875, validation accuracy: 91.52%\n",
      "step: 114000, batch loss: 0.314746, train loss: 0.279419, train accuracy: 92.51%, validation loss: 0.305665, validation accuracy: 91.90%\n",
      "step: 115000, batch loss: 0.011540, train loss: 0.276788, train accuracy: 92.50%, validation loss: 0.299941, validation accuracy: 92.04%\n",
      "step: 116000, batch loss: 0.041995, train loss: 0.280385, train accuracy: 92.36%, validation loss: 0.308141, validation accuracy: 91.78%\n",
      "step: 117000, batch loss: 0.029875, train loss: 0.274657, train accuracy: 92.53%, validation loss: 0.298589, validation accuracy: 92.22%\n",
      "step: 118000, batch loss: 0.107201, train loss: 0.275269, train accuracy: 92.51%, validation loss: 0.298310, validation accuracy: 92.00%\n",
      "step: 119000, batch loss: 0.444612, train loss: 0.283862, train accuracy: 92.18%, validation loss: 0.309605, validation accuracy: 91.38%\n",
      "step: 120000, batch loss: 1.900410, train loss: 0.275768, train accuracy: 92.49%, validation loss: 0.300818, validation accuracy: 91.72%\n",
      "step: 121000, batch loss: 0.043076, train loss: 0.271180, train accuracy: 92.65%, validation loss: 0.297789, validation accuracy: 91.90%\n",
      "step: 122000, batch loss: 0.192980, train loss: 0.272120, train accuracy: 92.63%, validation loss: 0.294487, validation accuracy: 91.88%\n",
      "step: 123000, batch loss: 0.046942, train loss: 0.279821, train accuracy: 92.32%, validation loss: 0.300271, validation accuracy: 92.16%\n",
      "step: 124000, batch loss: 0.033757, train loss: 0.274732, train accuracy: 92.57%, validation loss: 0.300244, validation accuracy: 91.92%\n",
      "step: 125000, batch loss: 0.202310, train loss: 0.275099, train accuracy: 92.51%, validation loss: 0.301081, validation accuracy: 91.98%\n",
      "step: 126000, batch loss: 0.681615, train loss: 0.279943, train accuracy: 92.26%, validation loss: 0.304659, validation accuracy: 91.38%\n",
      "step: 127000, batch loss: 0.104418, train loss: 0.272425, train accuracy: 92.61%, validation loss: 0.298468, validation accuracy: 91.96%\n",
      "step: 128000, batch loss: 0.247312, train loss: 0.277487, train accuracy: 92.39%, validation loss: 0.307510, validation accuracy: 91.62%\n",
      "step: 129000, batch loss: 0.037899, train loss: 0.269682, train accuracy: 92.73%, validation loss: 0.296104, validation accuracy: 91.98%\n",
      "step: 130000, batch loss: 0.107210, train loss: 0.279622, train accuracy: 92.25%, validation loss: 0.307998, validation accuracy: 91.56%\n",
      "step: 131000, batch loss: 0.038455, train loss: 0.272981, train accuracy: 92.57%, validation loss: 0.299734, validation accuracy: 92.14%\n",
      "step: 132000, batch loss: 0.212433, train loss: 0.274069, train accuracy: 92.52%, validation loss: 0.302569, validation accuracy: 91.84%\n",
      "step: 133000, batch loss: 0.445238, train loss: 0.271844, train accuracy: 92.63%, validation loss: 0.298765, validation accuracy: 91.96%\n",
      "step: 134000, batch loss: 0.291908, train loss: 0.266973, train accuracy: 92.81%, validation loss: 0.295755, validation accuracy: 92.10%\n",
      "step: 135000, batch loss: 0.145961, train loss: 0.268348, train accuracy: 92.71%, validation loss: 0.294687, validation accuracy: 92.18%\n",
      "step: 136000, batch loss: 0.090484, train loss: 0.270837, train accuracy: 92.66%, validation loss: 0.299746, validation accuracy: 91.88%\n",
      "step: 137000, batch loss: 0.042956, train loss: 0.268239, train accuracy: 92.69%, validation loss: 0.297232, validation accuracy: 91.92%\n",
      "step: 138000, batch loss: 0.117267, train loss: 0.270089, train accuracy: 92.56%, validation loss: 0.294049, validation accuracy: 92.04%\n",
      "step: 139000, batch loss: 0.284569, train loss: 0.273307, train accuracy: 92.53%, validation loss: 0.302790, validation accuracy: 91.74%\n",
      "step: 140000, batch loss: 0.009862, train loss: 0.271258, train accuracy: 92.73%, validation loss: 0.300794, validation accuracy: 91.96%\n",
      "step: 141000, batch loss: 0.660137, train loss: 0.272321, train accuracy: 92.56%, validation loss: 0.297704, validation accuracy: 92.06%\n",
      "step: 142000, batch loss: 0.048862, train loss: 0.270695, train accuracy: 92.57%, validation loss: 0.299718, validation accuracy: 91.84%\n",
      "step: 143000, batch loss: 0.110349, train loss: 0.269794, train accuracy: 92.60%, validation loss: 0.299494, validation accuracy: 91.96%\n",
      "step: 144000, batch loss: 0.525755, train loss: 0.267013, train accuracy: 92.83%, validation loss: 0.293565, validation accuracy: 92.12%\n",
      "step: 145000, batch loss: 0.079897, train loss: 0.264160, train accuracy: 92.92%, validation loss: 0.294296, validation accuracy: 91.96%\n",
      "step: 146000, batch loss: 0.083108, train loss: 0.269963, train accuracy: 92.63%, validation loss: 0.295697, validation accuracy: 91.98%\n",
      "step: 147000, batch loss: 0.002016, train loss: 0.272992, train accuracy: 92.48%, validation loss: 0.302899, validation accuracy: 91.78%\n",
      "step: 148000, batch loss: 0.796219, train loss: 0.280754, train accuracy: 92.19%, validation loss: 0.307911, validation accuracy: 91.46%\n",
      "step: 149000, batch loss: 0.004300, train loss: 0.263457, train accuracy: 92.79%, validation loss: 0.288100, validation accuracy: 92.12%\n",
      "step: 150000, batch loss: 0.170315, train loss: 0.266307, train accuracy: 92.64%, validation loss: 0.293560, validation accuracy: 92.02%\n",
      "step: 151000, batch loss: 0.049159, train loss: 0.277344, train accuracy: 92.45%, validation loss: 0.310514, validation accuracy: 91.78%\n",
      "step: 152000, batch loss: 0.309343, train loss: 0.262847, train accuracy: 92.93%, validation loss: 0.294312, validation accuracy: 92.02%\n",
      "step: 153000, batch loss: 0.321168, train loss: 0.274142, train accuracy: 92.50%, validation loss: 0.299807, validation accuracy: 91.82%\n",
      "step: 154000, batch loss: 0.211879, train loss: 0.272952, train accuracy: 92.53%, validation loss: 0.301520, validation accuracy: 91.78%\n",
      "step: 155000, batch loss: 0.511421, train loss: 0.265004, train accuracy: 92.85%, validation loss: 0.296066, validation accuracy: 91.88%\n",
      "step: 156000, batch loss: 0.089492, train loss: 0.266817, train accuracy: 92.85%, validation loss: 0.291812, validation accuracy: 92.24%\n",
      "step: 157000, batch loss: 0.035311, train loss: 0.267781, train accuracy: 92.75%, validation loss: 0.297136, validation accuracy: 91.74%\n",
      "step: 158000, batch loss: 0.027185, train loss: 0.265388, train accuracy: 92.73%, validation loss: 0.295488, validation accuracy: 92.28%\n",
      "step: 159000, batch loss: 0.054027, train loss: 0.263284, train accuracy: 92.81%, validation loss: 0.291801, validation accuracy: 92.22%\n",
      "step: 160000, batch loss: 0.056165, train loss: 0.268482, train accuracy: 92.74%, validation loss: 0.298911, validation accuracy: 92.02%\n",
      "step: 161000, batch loss: 0.134034, train loss: 0.266984, train accuracy: 92.69%, validation loss: 0.293580, validation accuracy: 92.08%\n",
      "step: 162000, batch loss: 0.015845, train loss: 0.267893, train accuracy: 92.71%, validation loss: 0.294670, validation accuracy: 92.12%\n",
      "step: 163000, batch loss: 0.226563, train loss: 0.264627, train accuracy: 92.61%, validation loss: 0.295200, validation accuracy: 91.94%\n",
      "step: 164000, batch loss: 0.279859, train loss: 0.259637, train accuracy: 93.04%, validation loss: 0.289113, validation accuracy: 92.14%\n",
      "step: 165000, batch loss: 0.023436, train loss: 0.261610, train accuracy: 92.91%, validation loss: 0.294453, validation accuracy: 92.12%\n",
      "step: 166000, batch loss: 0.786859, train loss: 0.265481, train accuracy: 92.82%, validation loss: 0.289880, validation accuracy: 92.18%\n",
      "step: 167000, batch loss: 0.014959, train loss: 0.259643, train accuracy: 92.95%, validation loss: 0.291321, validation accuracy: 91.96%\n",
      "step: 168000, batch loss: 0.183622, train loss: 0.264039, train accuracy: 92.84%, validation loss: 0.292849, validation accuracy: 92.40%\n",
      "step: 169000, batch loss: 2.283840, train loss: 0.262690, train accuracy: 92.80%, validation loss: 0.293901, validation accuracy: 92.26%\n",
      "step: 170000, batch loss: 0.094380, train loss: 0.257628, train accuracy: 93.00%, validation loss: 0.292409, validation accuracy: 92.06%\n",
      "step: 171000, batch loss: 0.083227, train loss: 0.264543, train accuracy: 92.67%, validation loss: 0.298065, validation accuracy: 92.04%\n",
      "step: 172000, batch loss: 1.173567, train loss: 0.265641, train accuracy: 92.66%, validation loss: 0.298287, validation accuracy: 92.06%\n",
      "step: 173000, batch loss: 0.028042, train loss: 0.265032, train accuracy: 92.78%, validation loss: 0.294589, validation accuracy: 91.98%\n",
      "step: 174000, batch loss: 0.077527, train loss: 0.264382, train accuracy: 92.77%, validation loss: 0.297266, validation accuracy: 91.88%\n",
      "step: 175000, batch loss: 0.005332, train loss: 0.260196, train accuracy: 92.91%, validation loss: 0.291296, validation accuracy: 92.22%\n",
      "step: 176000, batch loss: 0.021567, train loss: 0.261340, train accuracy: 92.95%, validation loss: 0.289464, validation accuracy: 92.28%\n",
      "step: 177000, batch loss: 0.115111, train loss: 0.260472, train accuracy: 92.95%, validation loss: 0.288722, validation accuracy: 92.36%\n",
      "step: 178000, batch loss: 0.031614, train loss: 0.270536, train accuracy: 92.54%, validation loss: 0.300404, validation accuracy: 92.10%\n",
      "step: 179000, batch loss: 0.021231, train loss: 0.264179, train accuracy: 92.79%, validation loss: 0.298102, validation accuracy: 91.84%\n",
      "step: 180000, batch loss: 0.809086, train loss: 0.268224, train accuracy: 92.71%, validation loss: 0.298168, validation accuracy: 91.86%\n",
      "step: 181000, batch loss: 0.041321, train loss: 0.260402, train accuracy: 92.98%, validation loss: 0.293346, validation accuracy: 92.24%\n",
      "step: 182000, batch loss: 0.097231, train loss: 0.255951, train accuracy: 93.04%, validation loss: 0.286636, validation accuracy: 92.32%\n",
      "step: 183000, batch loss: 0.036740, train loss: 0.268277, train accuracy: 92.59%, validation loss: 0.298572, validation accuracy: 91.78%\n",
      "step: 184000, batch loss: 0.497135, train loss: 0.270165, train accuracy: 92.52%, validation loss: 0.297164, validation accuracy: 91.94%\n",
      "step: 185000, batch loss: 0.254292, train loss: 0.256127, train accuracy: 93.02%, validation loss: 0.286673, validation accuracy: 92.40%\n",
      "step: 186000, batch loss: 0.624231, train loss: 0.260709, train accuracy: 92.85%, validation loss: 0.292897, validation accuracy: 92.20%\n",
      "step: 187000, batch loss: 0.016255, train loss: 0.259727, train accuracy: 92.94%, validation loss: 0.293109, validation accuracy: 91.96%\n",
      "step: 188000, batch loss: 0.064813, train loss: 0.256777, train accuracy: 92.95%, validation loss: 0.293977, validation accuracy: 91.96%\n",
      "step: 189000, batch loss: 0.266506, train loss: 0.259288, train accuracy: 92.81%, validation loss: 0.288637, validation accuracy: 92.08%\n",
      "step: 190000, batch loss: 0.286358, train loss: 0.259747, train accuracy: 92.78%, validation loss: 0.289083, validation accuracy: 92.40%\n",
      "step: 191000, batch loss: 0.194585, train loss: 0.261743, train accuracy: 92.86%, validation loss: 0.292226, validation accuracy: 91.96%\n",
      "step: 192000, batch loss: 0.054614, train loss: 0.254302, train accuracy: 93.19%, validation loss: 0.283519, validation accuracy: 92.48%\n",
      "step: 193000, batch loss: 0.041276, train loss: 0.256486, train accuracy: 92.94%, validation loss: 0.289920, validation accuracy: 92.02%\n",
      "step: 194000, batch loss: 0.051850, train loss: 0.256459, train accuracy: 92.97%, validation loss: 0.289687, validation accuracy: 92.20%\n",
      "step: 195000, batch loss: 0.075491, train loss: 0.257849, train accuracy: 92.96%, validation loss: 0.292353, validation accuracy: 92.00%\n",
      "step: 196000, batch loss: 1.365106, train loss: 0.258578, train accuracy: 92.89%, validation loss: 0.290913, validation accuracy: 92.10%\n",
      "step: 197000, batch loss: 0.205657, train loss: 0.258659, train accuracy: 92.90%, validation loss: 0.288334, validation accuracy: 92.16%\n",
      "step: 198000, batch loss: 0.099885, train loss: 0.260936, train accuracy: 92.90%, validation loss: 0.295655, validation accuracy: 91.86%\n",
      "step: 199000, batch loss: 0.022069, train loss: 0.260666, train accuracy: 92.75%, validation loss: 0.294751, validation accuracy: 92.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103c66c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 13.805705, train loss: 10.258060, train accuracy: 15.14%, validation loss: 10.282709, validation accuracy: 14.36%\n",
      "step: 1000, batch loss: 1.089441, train loss: 1.937116, train accuracy: 61.77%, validation loss: 1.829677, validation accuracy: 63.04%\n",
      "step: 2000, batch loss: 1.398630, train loss: 1.307650, train accuracy: 72.18%, validation loss: 1.209321, validation accuracy: 73.58%\n",
      "step: 3000, batch loss: 0.755546, train loss: 1.053992, train accuracy: 76.98%, validation loss: 0.963534, validation accuracy: 78.34%\n",
      "step: 4000, batch loss: 3.035974, train loss: 0.932110, train accuracy: 79.33%, validation loss: 0.856052, validation accuracy: 80.24%\n",
      "step: 5000, batch loss: 0.743240, train loss: 0.849458, train accuracy: 81.05%, validation loss: 0.774065, validation accuracy: 82.04%\n",
      "step: 6000, batch loss: 0.884534, train loss: 0.790727, train accuracy: 82.09%, validation loss: 0.716525, validation accuracy: 83.28%\n",
      "step: 7000, batch loss: 0.662622, train loss: 0.740203, train accuracy: 83.15%, validation loss: 0.668769, validation accuracy: 84.26%\n",
      "step: 8000, batch loss: 1.202879, train loss: 0.705759, train accuracy: 83.94%, validation loss: 0.641101, validation accuracy: 84.80%\n",
      "step: 9000, batch loss: 0.100091, train loss: 0.685884, train accuracy: 84.26%, validation loss: 0.621398, validation accuracy: 85.30%\n",
      "step: 10000, batch loss: 0.094380, train loss: 0.654387, train accuracy: 84.98%, validation loss: 0.593177, validation accuracy: 86.04%\n",
      "step: 11000, batch loss: 0.184452, train loss: 0.635680, train accuracy: 85.22%, validation loss: 0.574955, validation accuracy: 86.28%\n",
      "step: 12000, batch loss: 0.094873, train loss: 0.611955, train accuracy: 85.61%, validation loss: 0.557088, validation accuracy: 86.88%\n",
      "step: 13000, batch loss: 0.372791, train loss: 0.599094, train accuracy: 85.81%, validation loss: 0.547458, validation accuracy: 86.84%\n",
      "step: 14000, batch loss: 0.332057, train loss: 0.582457, train accuracy: 86.22%, validation loss: 0.531352, validation accuracy: 87.12%\n",
      "step: 15000, batch loss: 0.034555, train loss: 0.566609, train accuracy: 86.47%, validation loss: 0.518001, validation accuracy: 87.30%\n",
      "step: 16000, batch loss: 0.849381, train loss: 0.558801, train accuracy: 86.67%, validation loss: 0.517184, validation accuracy: 87.54%\n",
      "step: 17000, batch loss: 0.024638, train loss: 0.546368, train accuracy: 86.82%, validation loss: 0.506522, validation accuracy: 87.64%\n",
      "step: 18000, batch loss: 0.129990, train loss: 0.534187, train accuracy: 87.05%, validation loss: 0.491984, validation accuracy: 87.70%\n",
      "step: 19000, batch loss: 0.004487, train loss: 0.527698, train accuracy: 87.20%, validation loss: 0.487043, validation accuracy: 87.84%\n",
      "step: 20000, batch loss: 0.352807, train loss: 0.521028, train accuracy: 87.38%, validation loss: 0.479283, validation accuracy: 88.30%\n",
      "step: 21000, batch loss: 0.464239, train loss: 0.511329, train accuracy: 87.57%, validation loss: 0.471692, validation accuracy: 88.56%\n",
      "step: 22000, batch loss: 0.374165, train loss: 0.505492, train accuracy: 87.69%, validation loss: 0.466428, validation accuracy: 88.50%\n",
      "step: 23000, batch loss: 1.575546, train loss: 0.495849, train accuracy: 87.81%, validation loss: 0.458465, validation accuracy: 88.88%\n",
      "step: 24000, batch loss: 0.670820, train loss: 0.491216, train accuracy: 87.82%, validation loss: 0.457995, validation accuracy: 88.54%\n",
      "step: 25000, batch loss: 1.466028, train loss: 0.483495, train accuracy: 88.03%, validation loss: 0.452367, validation accuracy: 89.02%\n",
      "step: 26000, batch loss: 1.779063, train loss: 0.481148, train accuracy: 88.09%, validation loss: 0.447823, validation accuracy: 88.94%\n",
      "step: 27000, batch loss: 1.297569, train loss: 0.473386, train accuracy: 88.21%, validation loss: 0.441485, validation accuracy: 88.88%\n",
      "step: 28000, batch loss: 0.628380, train loss: 0.469238, train accuracy: 88.30%, validation loss: 0.436675, validation accuracy: 89.18%\n",
      "step: 29000, batch loss: 0.702676, train loss: 0.462761, train accuracy: 88.53%, validation loss: 0.432698, validation accuracy: 89.18%\n",
      "step: 30000, batch loss: 0.220375, train loss: 0.459355, train accuracy: 88.52%, validation loss: 0.429776, validation accuracy: 89.02%\n",
      "step: 31000, batch loss: 0.008110, train loss: 0.452810, train accuracy: 88.69%, validation loss: 0.423225, validation accuracy: 89.30%\n",
      "step: 32000, batch loss: 1.624686, train loss: 0.449127, train accuracy: 88.72%, validation loss: 0.423269, validation accuracy: 89.68%\n",
      "step: 33000, batch loss: 0.042160, train loss: 0.448131, train accuracy: 88.63%, validation loss: 0.421268, validation accuracy: 89.56%\n",
      "step: 34000, batch loss: 0.023214, train loss: 0.444329, train accuracy: 88.65%, validation loss: 0.417510, validation accuracy: 89.54%\n",
      "step: 35000, batch loss: 0.218342, train loss: 0.436623, train accuracy: 88.89%, validation loss: 0.411621, validation accuracy: 89.76%\n",
      "step: 36000, batch loss: 0.301977, train loss: 0.432995, train accuracy: 89.09%, validation loss: 0.409336, validation accuracy: 89.84%\n",
      "step: 37000, batch loss: 0.782214, train loss: 0.429329, train accuracy: 89.14%, validation loss: 0.407373, validation accuracy: 89.68%\n",
      "step: 38000, batch loss: 0.910360, train loss: 0.425676, train accuracy: 89.21%, validation loss: 0.402961, validation accuracy: 89.86%\n",
      "step: 39000, batch loss: 0.570477, train loss: 0.425153, train accuracy: 89.20%, validation loss: 0.402219, validation accuracy: 89.92%\n",
      "step: 40000, batch loss: 0.041412, train loss: 0.424515, train accuracy: 89.27%, validation loss: 0.403237, validation accuracy: 89.98%\n",
      "step: 41000, batch loss: 0.013750, train loss: 0.415347, train accuracy: 89.35%, validation loss: 0.394714, validation accuracy: 90.10%\n",
      "step: 42000, batch loss: 0.543354, train loss: 0.416644, train accuracy: 89.44%, validation loss: 0.396123, validation accuracy: 90.30%\n",
      "step: 43000, batch loss: 0.168161, train loss: 0.413789, train accuracy: 89.38%, validation loss: 0.397678, validation accuracy: 90.12%\n",
      "step: 44000, batch loss: 1.088172, train loss: 0.408373, train accuracy: 89.53%, validation loss: 0.390704, validation accuracy: 90.28%\n",
      "step: 45000, batch loss: 1.312028, train loss: 0.407974, train accuracy: 89.58%, validation loss: 0.391171, validation accuracy: 90.12%\n",
      "step: 46000, batch loss: 0.090294, train loss: 0.403599, train accuracy: 89.61%, validation loss: 0.387300, validation accuracy: 90.26%\n",
      "step: 47000, batch loss: 0.055994, train loss: 0.403274, train accuracy: 89.67%, validation loss: 0.387706, validation accuracy: 90.26%\n",
      "step: 48000, batch loss: 0.025151, train loss: 0.398772, train accuracy: 89.72%, validation loss: 0.381558, validation accuracy: 90.28%\n",
      "step: 49000, batch loss: 0.393803, train loss: 0.396658, train accuracy: 89.76%, validation loss: 0.381293, validation accuracy: 90.48%\n",
      "step: 50000, batch loss: 0.014930, train loss: 0.393866, train accuracy: 89.87%, validation loss: 0.381665, validation accuracy: 90.36%\n",
      "step: 51000, batch loss: 2.697642, train loss: 0.393037, train accuracy: 89.76%, validation loss: 0.379183, validation accuracy: 90.36%\n",
      "step: 52000, batch loss: 0.042418, train loss: 0.390620, train accuracy: 89.84%, validation loss: 0.377951, validation accuracy: 90.48%\n",
      "step: 53000, batch loss: 0.009657, train loss: 0.387198, train accuracy: 89.95%, validation loss: 0.375232, validation accuracy: 90.66%\n",
      "step: 54000, batch loss: 0.012591, train loss: 0.385221, train accuracy: 89.94%, validation loss: 0.372222, validation accuracy: 90.58%\n",
      "step: 55000, batch loss: 0.004291, train loss: 0.388254, train accuracy: 89.88%, validation loss: 0.374463, validation accuracy: 90.76%\n",
      "step: 56000, batch loss: 2.271058, train loss: 0.382105, train accuracy: 90.09%, validation loss: 0.369727, validation accuracy: 90.78%\n",
      "step: 57000, batch loss: 0.006560, train loss: 0.379723, train accuracy: 90.15%, validation loss: 0.369182, validation accuracy: 90.80%\n",
      "step: 58000, batch loss: 0.115721, train loss: 0.377640, train accuracy: 90.20%, validation loss: 0.366463, validation accuracy: 91.00%\n",
      "step: 59000, batch loss: 0.050302, train loss: 0.381541, train accuracy: 90.02%, validation loss: 0.372092, validation accuracy: 90.36%\n",
      "step: 60000, batch loss: 0.461589, train loss: 0.375108, train accuracy: 90.21%, validation loss: 0.365746, validation accuracy: 90.68%\n",
      "step: 61000, batch loss: 0.114833, train loss: 0.375970, train accuracy: 90.20%, validation loss: 0.365337, validation accuracy: 90.58%\n",
      "step: 62000, batch loss: 0.612443, train loss: 0.375304, train accuracy: 90.21%, validation loss: 0.365217, validation accuracy: 90.76%\n",
      "step: 63000, batch loss: 0.572430, train loss: 0.372861, train accuracy: 90.24%, validation loss: 0.363136, validation accuracy: 90.84%\n",
      "step: 64000, batch loss: 0.035777, train loss: 0.370852, train accuracy: 90.22%, validation loss: 0.361877, validation accuracy: 90.76%\n",
      "step: 65000, batch loss: 0.406012, train loss: 0.372577, train accuracy: 90.14%, validation loss: 0.364845, validation accuracy: 90.96%\n",
      "step: 66000, batch loss: 0.085544, train loss: 0.366532, train accuracy: 90.35%, validation loss: 0.360580, validation accuracy: 90.86%\n",
      "step: 67000, batch loss: 0.635526, train loss: 0.363894, train accuracy: 90.48%, validation loss: 0.356350, validation accuracy: 91.02%\n",
      "step: 68000, batch loss: 0.108074, train loss: 0.368765, train accuracy: 90.38%, validation loss: 0.364262, validation accuracy: 90.82%\n",
      "step: 69000, batch loss: 0.590714, train loss: 0.362734, train accuracy: 90.52%, validation loss: 0.356819, validation accuracy: 90.80%\n",
      "step: 70000, batch loss: 0.007315, train loss: 0.361728, train accuracy: 90.51%, validation loss: 0.354551, validation accuracy: 90.80%\n",
      "step: 71000, batch loss: 0.554362, train loss: 0.359578, train accuracy: 90.54%, validation loss: 0.352875, validation accuracy: 91.34%\n",
      "step: 72000, batch loss: 0.557840, train loss: 0.358525, train accuracy: 90.69%, validation loss: 0.352310, validation accuracy: 91.10%\n",
      "step: 73000, batch loss: 0.317119, train loss: 0.357067, train accuracy: 90.59%, validation loss: 0.352534, validation accuracy: 90.96%\n",
      "step: 74000, batch loss: 0.056212, train loss: 0.356607, train accuracy: 90.67%, validation loss: 0.352902, validation accuracy: 91.02%\n",
      "step: 75000, batch loss: 2.333506, train loss: 0.353507, train accuracy: 90.71%, validation loss: 0.349359, validation accuracy: 91.06%\n",
      "step: 76000, batch loss: 0.559492, train loss: 0.355603, train accuracy: 90.58%, validation loss: 0.352581, validation accuracy: 90.86%\n",
      "step: 77000, batch loss: 0.022247, train loss: 0.353733, train accuracy: 90.69%, validation loss: 0.350975, validation accuracy: 90.86%\n",
      "step: 78000, batch loss: 0.070654, train loss: 0.348865, train accuracy: 90.84%, validation loss: 0.345899, validation accuracy: 90.90%\n",
      "step: 79000, batch loss: 1.272420, train loss: 0.349608, train accuracy: 90.79%, validation loss: 0.347295, validation accuracy: 90.92%\n",
      "step: 80000, batch loss: 0.124656, train loss: 0.352486, train accuracy: 90.70%, validation loss: 0.351491, validation accuracy: 90.88%\n",
      "step: 81000, batch loss: 0.022111, train loss: 0.345960, train accuracy: 90.86%, validation loss: 0.346253, validation accuracy: 91.20%\n",
      "step: 82000, batch loss: 0.262090, train loss: 0.344388, train accuracy: 90.90%, validation loss: 0.342945, validation accuracy: 91.20%\n",
      "step: 83000, batch loss: 0.263897, train loss: 0.346750, train accuracy: 90.78%, validation loss: 0.344640, validation accuracy: 91.10%\n",
      "step: 84000, batch loss: 0.034011, train loss: 0.344585, train accuracy: 90.88%, validation loss: 0.343152, validation accuracy: 91.20%\n",
      "step: 85000, batch loss: 0.048298, train loss: 0.345364, train accuracy: 90.82%, validation loss: 0.344934, validation accuracy: 91.20%\n",
      "step: 86000, batch loss: 0.133475, train loss: 0.343916, train accuracy: 90.92%, validation loss: 0.341212, validation accuracy: 91.38%\n",
      "step: 87000, batch loss: 0.390481, train loss: 0.340960, train accuracy: 91.00%, validation loss: 0.339391, validation accuracy: 91.26%\n",
      "step: 88000, batch loss: 0.070823, train loss: 0.340397, train accuracy: 91.02%, validation loss: 0.342781, validation accuracy: 90.98%\n",
      "step: 89000, batch loss: 0.018340, train loss: 0.337631, train accuracy: 91.09%, validation loss: 0.339255, validation accuracy: 91.24%\n",
      "step: 90000, batch loss: 0.015661, train loss: 0.341299, train accuracy: 90.92%, validation loss: 0.342017, validation accuracy: 91.06%\n",
      "step: 91000, batch loss: 0.323729, train loss: 0.338947, train accuracy: 90.96%, validation loss: 0.342780, validation accuracy: 90.96%\n",
      "step: 92000, batch loss: 0.387605, train loss: 0.341218, train accuracy: 90.86%, validation loss: 0.343399, validation accuracy: 90.86%\n",
      "step: 93000, batch loss: 0.455346, train loss: 0.334429, train accuracy: 91.08%, validation loss: 0.336952, validation accuracy: 91.16%\n",
      "step: 94000, batch loss: 0.129223, train loss: 0.333707, train accuracy: 91.14%, validation loss: 0.337697, validation accuracy: 91.26%\n",
      "step: 95000, batch loss: 0.008761, train loss: 0.334343, train accuracy: 91.05%, validation loss: 0.337691, validation accuracy: 91.20%\n",
      "step: 96000, batch loss: 0.600968, train loss: 0.337336, train accuracy: 91.04%, validation loss: 0.339383, validation accuracy: 91.08%\n",
      "step: 97000, batch loss: 0.556893, train loss: 0.333447, train accuracy: 91.21%, validation loss: 0.334527, validation accuracy: 91.42%\n",
      "step: 98000, batch loss: 0.922633, train loss: 0.332918, train accuracy: 91.31%, validation loss: 0.335106, validation accuracy: 91.24%\n",
      "step: 99000, batch loss: 0.974949, train loss: 0.329531, train accuracy: 91.37%, validation loss: 0.332103, validation accuracy: 91.34%\n",
      "step: 100000, batch loss: 1.392095, train loss: 0.332718, train accuracy: 91.08%, validation loss: 0.337009, validation accuracy: 91.16%\n",
      "step: 101000, batch loss: 0.249538, train loss: 0.330665, train accuracy: 91.29%, validation loss: 0.332570, validation accuracy: 91.34%\n",
      "step: 102000, batch loss: 0.075311, train loss: 0.327694, train accuracy: 91.22%, validation loss: 0.332667, validation accuracy: 91.34%\n",
      "step: 103000, batch loss: 0.206327, train loss: 0.329472, train accuracy: 91.21%, validation loss: 0.334427, validation accuracy: 91.18%\n",
      "step: 104000, batch loss: 0.017895, train loss: 0.328871, train accuracy: 91.23%, validation loss: 0.331373, validation accuracy: 91.26%\n",
      "step: 105000, batch loss: 0.023705, train loss: 0.333839, train accuracy: 91.01%, validation loss: 0.338933, validation accuracy: 91.12%\n",
      "step: 106000, batch loss: 0.798139, train loss: 0.325553, train accuracy: 91.29%, validation loss: 0.330301, validation accuracy: 91.36%\n",
      "step: 107000, batch loss: 0.274893, train loss: 0.325922, train accuracy: 91.25%, validation loss: 0.330744, validation accuracy: 91.36%\n",
      "step: 108000, batch loss: 0.099187, train loss: 0.325625, train accuracy: 91.34%, validation loss: 0.328708, validation accuracy: 91.42%\n",
      "step: 109000, batch loss: 0.118470, train loss: 0.323031, train accuracy: 91.42%, validation loss: 0.328899, validation accuracy: 91.30%\n",
      "step: 110000, batch loss: 0.034350, train loss: 0.325783, train accuracy: 91.20%, validation loss: 0.331795, validation accuracy: 90.82%\n",
      "step: 111000, batch loss: 0.057276, train loss: 0.323547, train accuracy: 91.50%, validation loss: 0.327346, validation accuracy: 91.28%\n",
      "step: 112000, batch loss: 0.701346, train loss: 0.320654, train accuracy: 91.51%, validation loss: 0.327591, validation accuracy: 91.38%\n",
      "step: 113000, batch loss: 0.073697, train loss: 0.323218, train accuracy: 91.35%, validation loss: 0.329977, validation accuracy: 91.26%\n",
      "step: 114000, batch loss: 0.028804, train loss: 0.325468, train accuracy: 91.23%, validation loss: 0.330683, validation accuracy: 91.02%\n",
      "step: 115000, batch loss: 0.057299, train loss: 0.320214, train accuracy: 91.53%, validation loss: 0.326296, validation accuracy: 91.70%\n",
      "step: 116000, batch loss: 0.756936, train loss: 0.318932, train accuracy: 91.46%, validation loss: 0.328373, validation accuracy: 91.26%\n",
      "step: 117000, batch loss: 0.075633, train loss: 0.321653, train accuracy: 91.46%, validation loss: 0.332183, validation accuracy: 91.40%\n",
      "step: 118000, batch loss: 0.158481, train loss: 0.317326, train accuracy: 91.57%, validation loss: 0.324648, validation accuracy: 91.60%\n",
      "step: 119000, batch loss: 1.036481, train loss: 0.323234, train accuracy: 91.30%, validation loss: 0.332050, validation accuracy: 91.26%\n",
      "step: 120000, batch loss: 0.033168, train loss: 0.315623, train accuracy: 91.55%, validation loss: 0.325806, validation accuracy: 91.38%\n",
      "step: 121000, batch loss: 0.187706, train loss: 0.321575, train accuracy: 91.39%, validation loss: 0.326837, validation accuracy: 91.44%\n",
      "step: 122000, batch loss: 0.007288, train loss: 0.315846, train accuracy: 91.61%, validation loss: 0.323457, validation accuracy: 91.60%\n",
      "step: 123000, batch loss: 0.017488, train loss: 0.316162, train accuracy: 91.59%, validation loss: 0.323766, validation accuracy: 91.46%\n",
      "step: 124000, batch loss: 0.015290, train loss: 0.316118, train accuracy: 91.50%, validation loss: 0.322757, validation accuracy: 91.64%\n",
      "step: 125000, batch loss: 0.095962, train loss: 0.316749, train accuracy: 91.48%, validation loss: 0.324554, validation accuracy: 91.66%\n",
      "step: 126000, batch loss: 0.215948, train loss: 0.313034, train accuracy: 91.63%, validation loss: 0.320098, validation accuracy: 91.52%\n",
      "step: 127000, batch loss: 0.042516, train loss: 0.313013, train accuracy: 91.58%, validation loss: 0.319285, validation accuracy: 91.72%\n",
      "step: 128000, batch loss: 0.041383, train loss: 0.318256, train accuracy: 91.41%, validation loss: 0.326700, validation accuracy: 91.40%\n",
      "step: 129000, batch loss: 0.017636, train loss: 0.313612, train accuracy: 91.55%, validation loss: 0.322756, validation accuracy: 91.46%\n",
      "step: 130000, batch loss: 2.311296, train loss: 0.311913, train accuracy: 91.64%, validation loss: 0.322824, validation accuracy: 91.54%\n",
      "step: 131000, batch loss: 0.038458, train loss: 0.314642, train accuracy: 91.57%, validation loss: 0.323991, validation accuracy: 91.62%\n",
      "step: 132000, batch loss: 0.897883, train loss: 0.310531, train accuracy: 91.69%, validation loss: 0.320785, validation accuracy: 91.60%\n",
      "step: 133000, batch loss: 0.181018, train loss: 0.312637, train accuracy: 91.59%, validation loss: 0.322263, validation accuracy: 91.42%\n",
      "step: 134000, batch loss: 0.162673, train loss: 0.310708, train accuracy: 91.73%, validation loss: 0.321602, validation accuracy: 91.74%\n",
      "step: 135000, batch loss: 0.027455, train loss: 0.307767, train accuracy: 91.81%, validation loss: 0.318427, validation accuracy: 91.66%\n",
      "step: 136000, batch loss: 0.041707, train loss: 0.309165, train accuracy: 91.63%, validation loss: 0.320277, validation accuracy: 91.62%\n",
      "step: 137000, batch loss: 0.702813, train loss: 0.313097, train accuracy: 91.59%, validation loss: 0.323623, validation accuracy: 91.58%\n",
      "step: 138000, batch loss: 1.447537, train loss: 0.308439, train accuracy: 91.77%, validation loss: 0.319696, validation accuracy: 91.54%\n",
      "step: 139000, batch loss: 0.279255, train loss: 0.306680, train accuracy: 91.74%, validation loss: 0.318881, validation accuracy: 91.56%\n",
      "step: 140000, batch loss: 0.625191, train loss: 0.306209, train accuracy: 91.86%, validation loss: 0.317919, validation accuracy: 91.60%\n",
      "step: 141000, batch loss: 0.376294, train loss: 0.305779, train accuracy: 91.86%, validation loss: 0.319940, validation accuracy: 91.62%\n",
      "step: 142000, batch loss: 0.022256, train loss: 0.305288, train accuracy: 91.84%, validation loss: 0.320185, validation accuracy: 91.66%\n",
      "step: 143000, batch loss: 1.472564, train loss: 0.305865, train accuracy: 91.78%, validation loss: 0.316055, validation accuracy: 91.54%\n",
      "step: 144000, batch loss: 0.679269, train loss: 0.305334, train accuracy: 91.84%, validation loss: 0.317535, validation accuracy: 91.86%\n",
      "step: 145000, batch loss: 0.209629, train loss: 0.304527, train accuracy: 91.78%, validation loss: 0.316956, validation accuracy: 91.56%\n",
      "step: 146000, batch loss: 0.180391, train loss: 0.302527, train accuracy: 91.85%, validation loss: 0.314569, validation accuracy: 91.86%\n",
      "step: 147000, batch loss: 0.242282, train loss: 0.302279, train accuracy: 91.89%, validation loss: 0.315431, validation accuracy: 91.70%\n",
      "step: 148000, batch loss: 0.608175, train loss: 0.301917, train accuracy: 91.93%, validation loss: 0.313432, validation accuracy: 91.70%\n",
      "step: 149000, batch loss: 0.015893, train loss: 0.303737, train accuracy: 91.87%, validation loss: 0.315372, validation accuracy: 91.84%\n",
      "step: 150000, batch loss: 0.007796, train loss: 0.303632, train accuracy: 91.87%, validation loss: 0.315361, validation accuracy: 91.62%\n",
      "step: 151000, batch loss: 0.051129, train loss: 0.300759, train accuracy: 91.95%, validation loss: 0.316356, validation accuracy: 91.70%\n",
      "step: 152000, batch loss: 0.016675, train loss: 0.302075, train accuracy: 91.92%, validation loss: 0.314079, validation accuracy: 91.94%\n",
      "step: 153000, batch loss: 0.127749, train loss: 0.300783, train accuracy: 91.97%, validation loss: 0.312476, validation accuracy: 91.74%\n",
      "step: 154000, batch loss: 0.206169, train loss: 0.301912, train accuracy: 91.88%, validation loss: 0.312589, validation accuracy: 91.90%\n",
      "step: 155000, batch loss: 0.027242, train loss: 0.299282, train accuracy: 92.00%, validation loss: 0.312242, validation accuracy: 91.70%\n",
      "step: 156000, batch loss: 0.327979, train loss: 0.302259, train accuracy: 91.90%, validation loss: 0.318590, validation accuracy: 91.78%\n",
      "step: 157000, batch loss: 0.423583, train loss: 0.300175, train accuracy: 91.93%, validation loss: 0.314776, validation accuracy: 91.70%\n",
      "step: 158000, batch loss: 0.053148, train loss: 0.302789, train accuracy: 91.78%, validation loss: 0.314424, validation accuracy: 91.80%\n",
      "step: 159000, batch loss: 0.454274, train loss: 0.300899, train accuracy: 91.92%, validation loss: 0.312755, validation accuracy: 91.80%\n",
      "step: 160000, batch loss: 0.104051, train loss: 0.298211, train accuracy: 91.99%, validation loss: 0.310825, validation accuracy: 91.78%\n",
      "step: 161000, batch loss: 0.085657, train loss: 0.300250, train accuracy: 91.86%, validation loss: 0.312618, validation accuracy: 91.92%\n",
      "step: 162000, batch loss: 0.082984, train loss: 0.297407, train accuracy: 92.02%, validation loss: 0.312849, validation accuracy: 91.88%\n",
      "step: 163000, batch loss: 0.108245, train loss: 0.302306, train accuracy: 91.89%, validation loss: 0.316843, validation accuracy: 91.52%\n",
      "step: 164000, batch loss: 0.128893, train loss: 0.296413, train accuracy: 92.03%, validation loss: 0.312938, validation accuracy: 91.80%\n",
      "step: 165000, batch loss: 0.535352, train loss: 0.301871, train accuracy: 91.73%, validation loss: 0.316532, validation accuracy: 91.46%\n",
      "step: 166000, batch loss: 0.114005, train loss: 0.299949, train accuracy: 91.92%, validation loss: 0.314851, validation accuracy: 91.76%\n",
      "step: 167000, batch loss: 0.457563, train loss: 0.297691, train accuracy: 91.94%, validation loss: 0.314050, validation accuracy: 91.74%\n",
      "step: 168000, batch loss: 0.017371, train loss: 0.296227, train accuracy: 92.07%, validation loss: 0.309575, validation accuracy: 92.08%\n",
      "step: 169000, batch loss: 0.248231, train loss: 0.294600, train accuracy: 92.04%, validation loss: 0.310049, validation accuracy: 91.70%\n",
      "step: 170000, batch loss: 0.143599, train loss: 0.294983, train accuracy: 92.09%, validation loss: 0.310436, validation accuracy: 91.82%\n",
      "step: 171000, batch loss: 0.015807, train loss: 0.294056, train accuracy: 92.12%, validation loss: 0.308543, validation accuracy: 92.02%\n",
      "step: 172000, batch loss: 0.113783, train loss: 0.297591, train accuracy: 91.99%, validation loss: 0.311016, validation accuracy: 91.88%\n",
      "step: 173000, batch loss: 0.030888, train loss: 0.297698, train accuracy: 91.93%, validation loss: 0.314257, validation accuracy: 91.78%\n",
      "step: 174000, batch loss: 0.433023, train loss: 0.295082, train accuracy: 92.01%, validation loss: 0.313149, validation accuracy: 91.78%\n",
      "step: 175000, batch loss: 0.515496, train loss: 0.293763, train accuracy: 92.03%, validation loss: 0.310573, validation accuracy: 91.82%\n",
      "step: 176000, batch loss: 0.119546, train loss: 0.294435, train accuracy: 92.09%, validation loss: 0.311366, validation accuracy: 91.92%\n",
      "step: 177000, batch loss: 0.599469, train loss: 0.295255, train accuracy: 91.95%, validation loss: 0.309997, validation accuracy: 91.88%\n",
      "step: 178000, batch loss: 0.265423, train loss: 0.295992, train accuracy: 91.98%, validation loss: 0.314338, validation accuracy: 91.68%\n",
      "step: 179000, batch loss: 0.754707, train loss: 0.291738, train accuracy: 92.19%, validation loss: 0.309838, validation accuracy: 92.14%\n",
      "step: 180000, batch loss: 0.347324, train loss: 0.291908, train accuracy: 92.11%, validation loss: 0.310168, validation accuracy: 92.06%\n",
      "step: 181000, batch loss: 0.269883, train loss: 0.291506, train accuracy: 92.10%, validation loss: 0.310683, validation accuracy: 91.78%\n",
      "step: 182000, batch loss: 0.026749, train loss: 0.290981, train accuracy: 92.19%, validation loss: 0.308327, validation accuracy: 91.80%\n",
      "step: 183000, batch loss: 0.428103, train loss: 0.291287, train accuracy: 92.14%, validation loss: 0.308333, validation accuracy: 91.94%\n",
      "step: 184000, batch loss: 0.720943, train loss: 0.294141, train accuracy: 92.07%, validation loss: 0.308856, validation accuracy: 91.86%\n",
      "step: 185000, batch loss: 0.007542, train loss: 0.289094, train accuracy: 92.21%, validation loss: 0.304988, validation accuracy: 92.02%\n",
      "step: 186000, batch loss: 1.506256, train loss: 0.291313, train accuracy: 92.14%, validation loss: 0.309433, validation accuracy: 91.88%\n",
      "step: 187000, batch loss: 0.148696, train loss: 0.290069, train accuracy: 92.05%, validation loss: 0.308974, validation accuracy: 91.80%\n",
      "step: 188000, batch loss: 0.030413, train loss: 0.292422, train accuracy: 91.99%, validation loss: 0.310283, validation accuracy: 91.60%\n",
      "step: 189000, batch loss: 0.349969, train loss: 0.288120, train accuracy: 92.24%, validation loss: 0.305523, validation accuracy: 91.72%\n",
      "step: 190000, batch loss: 0.009361, train loss: 0.290628, train accuracy: 92.16%, validation loss: 0.307312, validation accuracy: 91.94%\n",
      "step: 191000, batch loss: 0.183085, train loss: 0.287094, train accuracy: 92.29%, validation loss: 0.305530, validation accuracy: 92.22%\n",
      "step: 192000, batch loss: 0.255040, train loss: 0.290286, train accuracy: 92.13%, validation loss: 0.308852, validation accuracy: 91.92%\n",
      "step: 193000, batch loss: 0.041925, train loss: 0.287958, train accuracy: 92.17%, validation loss: 0.306411, validation accuracy: 91.96%\n",
      "step: 194000, batch loss: 0.225760, train loss: 0.288369, train accuracy: 92.15%, validation loss: 0.308001, validation accuracy: 92.04%\n",
      "step: 195000, batch loss: 0.016984, train loss: 0.291366, train accuracy: 91.99%, validation loss: 0.309237, validation accuracy: 91.58%\n",
      "step: 196000, batch loss: 0.571663, train loss: 0.285425, train accuracy: 92.27%, validation loss: 0.302257, validation accuracy: 92.06%\n",
      "step: 197000, batch loss: 0.028240, train loss: 0.287610, train accuracy: 92.10%, validation loss: 0.306836, validation accuracy: 91.58%\n",
      "step: 198000, batch loss: 0.509621, train loss: 0.285685, train accuracy: 92.22%, validation loss: 0.302913, validation accuracy: 91.96%\n",
      "step: 199000, batch loss: 0.037007, train loss: 0.284636, train accuracy: 92.38%, validation loss: 0.303104, validation accuracy: 92.24%\n",
      "step: 200000, batch loss: 0.040419, train loss: 0.287019, train accuracy: 92.11%, validation loss: 0.304574, validation accuracy: 91.96%\n",
      "step: 201000, batch loss: 0.086651, train loss: 0.287215, train accuracy: 92.13%, validation loss: 0.306072, validation accuracy: 91.88%\n",
      "step: 202000, batch loss: 0.062125, train loss: 0.286063, train accuracy: 92.21%, validation loss: 0.304844, validation accuracy: 91.66%\n",
      "step: 203000, batch loss: 0.651782, train loss: 0.285830, train accuracy: 92.29%, validation loss: 0.305113, validation accuracy: 92.06%\n",
      "step: 204000, batch loss: 0.087080, train loss: 0.284129, train accuracy: 92.33%, validation loss: 0.302196, validation accuracy: 92.08%\n",
      "step: 205000, batch loss: 0.293269, train loss: 0.282107, train accuracy: 92.39%, validation loss: 0.300922, validation accuracy: 92.02%\n",
      "step: 206000, batch loss: 0.622298, train loss: 0.285456, train accuracy: 92.29%, validation loss: 0.304522, validation accuracy: 91.98%\n",
      "step: 207000, batch loss: 0.537480, train loss: 0.284608, train accuracy: 92.34%, validation loss: 0.306920, validation accuracy: 92.00%\n",
      "step: 208000, batch loss: 0.442176, train loss: 0.281993, train accuracy: 92.36%, validation loss: 0.303768, validation accuracy: 92.00%\n",
      "step: 209000, batch loss: 0.749177, train loss: 0.284711, train accuracy: 92.33%, validation loss: 0.303824, validation accuracy: 92.26%\n",
      "step: 210000, batch loss: 0.017357, train loss: 0.287524, train accuracy: 92.26%, validation loss: 0.306255, validation accuracy: 92.18%\n",
      "step: 211000, batch loss: 0.189719, train loss: 0.283046, train accuracy: 92.32%, validation loss: 0.300704, validation accuracy: 91.96%\n",
      "step: 212000, batch loss: 0.217325, train loss: 0.282939, train accuracy: 92.31%, validation loss: 0.302464, validation accuracy: 92.04%\n",
      "step: 213000, batch loss: 0.396679, train loss: 0.283232, train accuracy: 92.30%, validation loss: 0.302672, validation accuracy: 92.02%\n",
      "step: 214000, batch loss: 0.169261, train loss: 0.281134, train accuracy: 92.39%, validation loss: 0.299368, validation accuracy: 92.24%\n",
      "step: 215000, batch loss: 0.144847, train loss: 0.281737, train accuracy: 92.39%, validation loss: 0.300071, validation accuracy: 92.00%\n",
      "step: 216000, batch loss: 1.364286, train loss: 0.283756, train accuracy: 92.32%, validation loss: 0.304596, validation accuracy: 92.04%\n",
      "step: 217000, batch loss: 0.396844, train loss: 0.281476, train accuracy: 92.43%, validation loss: 0.301564, validation accuracy: 92.12%\n",
      "step: 218000, batch loss: 0.675165, train loss: 0.282785, train accuracy: 92.43%, validation loss: 0.302965, validation accuracy: 92.18%\n",
      "step: 219000, batch loss: 0.588996, train loss: 0.281488, train accuracy: 92.36%, validation loss: 0.301351, validation accuracy: 91.78%\n",
      "step: 220000, batch loss: 0.292585, train loss: 0.281490, train accuracy: 92.41%, validation loss: 0.302000, validation accuracy: 92.24%\n",
      "step: 221000, batch loss: 0.129258, train loss: 0.284615, train accuracy: 92.23%, validation loss: 0.303264, validation accuracy: 91.70%\n",
      "step: 222000, batch loss: 0.041790, train loss: 0.278450, train accuracy: 92.47%, validation loss: 0.299390, validation accuracy: 91.96%\n",
      "step: 223000, batch loss: 0.114611, train loss: 0.281143, train accuracy: 92.45%, validation loss: 0.303471, validation accuracy: 91.96%\n",
      "step: 224000, batch loss: 0.062909, train loss: 0.280486, train accuracy: 92.35%, validation loss: 0.301321, validation accuracy: 91.96%\n",
      "step: 225000, batch loss: 0.147561, train loss: 0.278505, train accuracy: 92.48%, validation loss: 0.298679, validation accuracy: 92.26%\n",
      "step: 226000, batch loss: 0.045637, train loss: 0.280337, train accuracy: 92.38%, validation loss: 0.301925, validation accuracy: 91.94%\n",
      "step: 227000, batch loss: 0.242426, train loss: 0.278273, train accuracy: 92.47%, validation loss: 0.299203, validation accuracy: 92.10%\n",
      "step: 228000, batch loss: 0.190420, train loss: 0.280119, train accuracy: 92.35%, validation loss: 0.300723, validation accuracy: 91.90%\n",
      "step: 229000, batch loss: 0.012942, train loss: 0.278482, train accuracy: 92.48%, validation loss: 0.298674, validation accuracy: 92.22%\n",
      "step: 230000, batch loss: 0.086631, train loss: 0.278416, train accuracy: 92.40%, validation loss: 0.300088, validation accuracy: 91.96%\n",
      "step: 231000, batch loss: 0.137047, train loss: 0.282849, train accuracy: 92.34%, validation loss: 0.304685, validation accuracy: 92.04%\n",
      "step: 232000, batch loss: 0.592637, train loss: 0.279339, train accuracy: 92.47%, validation loss: 0.300877, validation accuracy: 92.16%\n",
      "step: 233000, batch loss: 0.124172, train loss: 0.277841, train accuracy: 92.45%, validation loss: 0.298920, validation accuracy: 92.26%\n",
      "step: 234000, batch loss: 0.139523, train loss: 0.278158, train accuracy: 92.45%, validation loss: 0.298904, validation accuracy: 92.24%\n",
      "step: 235000, batch loss: 0.236422, train loss: 0.278181, train accuracy: 92.44%, validation loss: 0.299586, validation accuracy: 92.26%\n",
      "step: 236000, batch loss: 0.010674, train loss: 0.275941, train accuracy: 92.53%, validation loss: 0.297836, validation accuracy: 92.20%\n",
      "step: 237000, batch loss: 0.079136, train loss: 0.276960, train accuracy: 92.45%, validation loss: 0.299715, validation accuracy: 92.08%\n",
      "step: 238000, batch loss: 0.056180, train loss: 0.276245, train accuracy: 92.59%, validation loss: 0.300680, validation accuracy: 92.28%\n",
      "step: 239000, batch loss: 0.159990, train loss: 0.278319, train accuracy: 92.37%, validation loss: 0.299350, validation accuracy: 92.10%\n",
      "step: 240000, batch loss: 0.176216, train loss: 0.276983, train accuracy: 92.39%, validation loss: 0.297405, validation accuracy: 92.04%\n",
      "step: 241000, batch loss: 0.063414, train loss: 0.277015, train accuracy: 92.46%, validation loss: 0.299999, validation accuracy: 92.08%\n",
      "step: 242000, batch loss: 0.140545, train loss: 0.278068, train accuracy: 92.47%, validation loss: 0.299501, validation accuracy: 92.50%\n",
      "step: 243000, batch loss: 0.157190, train loss: 0.277989, train accuracy: 92.44%, validation loss: 0.298749, validation accuracy: 92.10%\n",
      "step: 244000, batch loss: 0.221016, train loss: 0.275763, train accuracy: 92.51%, validation loss: 0.299429, validation accuracy: 92.14%\n",
      "step: 245000, batch loss: 0.169559, train loss: 0.276342, train accuracy: 92.44%, validation loss: 0.298179, validation accuracy: 91.96%\n",
      "step: 246000, batch loss: 0.069838, train loss: 0.275835, train accuracy: 92.49%, validation loss: 0.298541, validation accuracy: 92.04%\n",
      "step: 247000, batch loss: 0.005089, train loss: 0.274727, train accuracy: 92.61%, validation loss: 0.298288, validation accuracy: 92.16%\n",
      "step: 248000, batch loss: 0.407763, train loss: 0.277193, train accuracy: 92.40%, validation loss: 0.301377, validation accuracy: 92.12%\n",
      "step: 249000, batch loss: 0.172026, train loss: 0.274406, train accuracy: 92.61%, validation loss: 0.296963, validation accuracy: 92.04%\n",
      "step: 250000, batch loss: 0.856430, train loss: 0.275615, train accuracy: 92.50%, validation loss: 0.297950, validation accuracy: 92.12%\n",
      "step: 251000, batch loss: 0.548124, train loss: 0.276516, train accuracy: 92.56%, validation loss: 0.296805, validation accuracy: 92.32%\n",
      "step: 252000, batch loss: 0.018816, train loss: 0.275750, train accuracy: 92.44%, validation loss: 0.297795, validation accuracy: 91.82%\n",
      "step: 253000, batch loss: 0.106860, train loss: 0.274026, train accuracy: 92.60%, validation loss: 0.296328, validation accuracy: 92.26%\n",
      "step: 254000, batch loss: 0.878014, train loss: 0.274607, train accuracy: 92.49%, validation loss: 0.299232, validation accuracy: 92.32%\n",
      "step: 255000, batch loss: 0.099394, train loss: 0.276724, train accuracy: 92.52%, validation loss: 0.299844, validation accuracy: 92.14%\n",
      "step: 256000, batch loss: 0.084645, train loss: 0.275196, train accuracy: 92.53%, validation loss: 0.300080, validation accuracy: 92.36%\n",
      "step: 257000, batch loss: 0.216091, train loss: 0.273204, train accuracy: 92.52%, validation loss: 0.297606, validation accuracy: 92.30%\n",
      "step: 258000, batch loss: 0.187277, train loss: 0.274150, train accuracy: 92.61%, validation loss: 0.299820, validation accuracy: 92.38%\n",
      "step: 259000, batch loss: 0.013476, train loss: 0.273270, train accuracy: 92.53%, validation loss: 0.296353, validation accuracy: 92.26%\n",
      "step: 260000, batch loss: 0.811693, train loss: 0.274108, train accuracy: 92.47%, validation loss: 0.296931, validation accuracy: 91.92%\n",
      "step: 261000, batch loss: 1.159712, train loss: 0.274895, train accuracy: 92.42%, validation loss: 0.297482, validation accuracy: 92.04%\n",
      "step: 262000, batch loss: 0.277550, train loss: 0.273022, train accuracy: 92.62%, validation loss: 0.294505, validation accuracy: 92.32%\n",
      "step: 263000, batch loss: 0.297294, train loss: 0.273952, train accuracy: 92.57%, validation loss: 0.296127, validation accuracy: 92.10%\n",
      "step: 264000, batch loss: 0.072632, train loss: 0.270842, train accuracy: 92.67%, validation loss: 0.294179, validation accuracy: 92.12%\n",
      "step: 265000, batch loss: 0.126634, train loss: 0.271865, train accuracy: 92.55%, validation loss: 0.296058, validation accuracy: 92.02%\n",
      "step: 266000, batch loss: 0.083861, train loss: 0.272270, train accuracy: 92.59%, validation loss: 0.296502, validation accuracy: 92.32%\n",
      "step: 267000, batch loss: 0.596951, train loss: 0.271753, train accuracy: 92.61%, validation loss: 0.294737, validation accuracy: 91.98%\n",
      "step: 268000, batch loss: 0.520956, train loss: 0.272971, train accuracy: 92.67%, validation loss: 0.296683, validation accuracy: 92.28%\n",
      "step: 269000, batch loss: 0.264349, train loss: 0.271104, train accuracy: 92.71%, validation loss: 0.293954, validation accuracy: 92.32%\n",
      "step: 270000, batch loss: 0.124346, train loss: 0.272856, train accuracy: 92.53%, validation loss: 0.298202, validation accuracy: 91.88%\n",
      "step: 271000, batch loss: 0.205594, train loss: 0.274204, train accuracy: 92.51%, validation loss: 0.298636, validation accuracy: 92.10%\n",
      "step: 272000, batch loss: 0.040532, train loss: 0.272427, train accuracy: 92.53%, validation loss: 0.295032, validation accuracy: 92.30%\n",
      "step: 273000, batch loss: 0.200301, train loss: 0.271690, train accuracy: 92.61%, validation loss: 0.295767, validation accuracy: 92.10%\n",
      "step: 274000, batch loss: 0.077415, train loss: 0.270092, train accuracy: 92.72%, validation loss: 0.295093, validation accuracy: 92.42%\n",
      "step: 275000, batch loss: 0.031799, train loss: 0.270968, train accuracy: 92.66%, validation loss: 0.296365, validation accuracy: 92.14%\n",
      "step: 276000, batch loss: 0.232828, train loss: 0.270027, train accuracy: 92.68%, validation loss: 0.294416, validation accuracy: 92.14%\n",
      "step: 277000, batch loss: 0.103111, train loss: 0.272504, train accuracy: 92.59%, validation loss: 0.296057, validation accuracy: 92.40%\n",
      "step: 278000, batch loss: 0.052355, train loss: 0.271690, train accuracy: 92.66%, validation loss: 0.295481, validation accuracy: 92.42%\n",
      "step: 279000, batch loss: 0.031934, train loss: 0.270060, train accuracy: 92.65%, validation loss: 0.293170, validation accuracy: 92.14%\n",
      "step: 280000, batch loss: 0.152183, train loss: 0.271763, train accuracy: 92.61%, validation loss: 0.294737, validation accuracy: 92.26%\n",
      "step: 281000, batch loss: 0.096702, train loss: 0.272353, train accuracy: 92.44%, validation loss: 0.296138, validation accuracy: 92.30%\n",
      "step: 282000, batch loss: 0.534543, train loss: 0.269167, train accuracy: 92.70%, validation loss: 0.294371, validation accuracy: 92.16%\n",
      "step: 283000, batch loss: 0.050004, train loss: 0.268037, train accuracy: 92.71%, validation loss: 0.292698, validation accuracy: 92.30%\n",
      "step: 284000, batch loss: 0.166851, train loss: 0.269451, train accuracy: 92.65%, validation loss: 0.294370, validation accuracy: 92.16%\n",
      "step: 285000, batch loss: 0.230058, train loss: 0.268586, train accuracy: 92.64%, validation loss: 0.294181, validation accuracy: 92.34%\n",
      "step: 286000, batch loss: 0.025230, train loss: 0.270858, train accuracy: 92.61%, validation loss: 0.297141, validation accuracy: 92.16%\n",
      "step: 287000, batch loss: 0.345933, train loss: 0.267744, train accuracy: 92.80%, validation loss: 0.292942, validation accuracy: 92.40%\n",
      "step: 288000, batch loss: 0.006352, train loss: 0.269080, train accuracy: 92.68%, validation loss: 0.293147, validation accuracy: 92.34%\n",
      "step: 289000, batch loss: 0.066230, train loss: 0.269280, train accuracy: 92.62%, validation loss: 0.292615, validation accuracy: 92.28%\n",
      "step: 290000, batch loss: 0.050290, train loss: 0.269770, train accuracy: 92.64%, validation loss: 0.294934, validation accuracy: 92.16%\n",
      "step: 291000, batch loss: 0.227934, train loss: 0.270544, train accuracy: 92.62%, validation loss: 0.294316, validation accuracy: 92.42%\n",
      "step: 292000, batch loss: 1.581879, train loss: 0.267118, train accuracy: 92.73%, validation loss: 0.293143, validation accuracy: 92.36%\n",
      "step: 293000, batch loss: 0.821616, train loss: 0.270483, train accuracy: 92.74%, validation loss: 0.294555, validation accuracy: 92.42%\n",
      "step: 294000, batch loss: 0.810758, train loss: 0.267378, train accuracy: 92.67%, validation loss: 0.292633, validation accuracy: 92.12%\n",
      "step: 295000, batch loss: 0.919000, train loss: 0.265976, train accuracy: 92.76%, validation loss: 0.293098, validation accuracy: 92.44%\n",
      "step: 296000, batch loss: 0.254199, train loss: 0.267425, train accuracy: 92.67%, validation loss: 0.293146, validation accuracy: 92.40%\n",
      "step: 297000, batch loss: 0.222123, train loss: 0.268485, train accuracy: 92.65%, validation loss: 0.295670, validation accuracy: 92.04%\n",
      "step: 298000, batch loss: 0.041367, train loss: 0.269673, train accuracy: 92.65%, validation loss: 0.296946, validation accuracy: 92.12%\n",
      "step: 299000, batch loss: 0.668027, train loss: 0.265796, train accuracy: 92.78%, validation loss: 0.291902, validation accuracy: 92.26%\n",
      "step: 300000, batch loss: 0.482662, train loss: 0.270987, train accuracy: 92.54%, validation loss: 0.297812, validation accuracy: 92.04%\n",
      "step: 301000, batch loss: 0.440154, train loss: 0.267946, train accuracy: 92.68%, validation loss: 0.294061, validation accuracy: 92.28%\n",
      "step: 302000, batch loss: 0.024567, train loss: 0.269323, train accuracy: 92.57%, validation loss: 0.295290, validation accuracy: 91.88%\n",
      "step: 303000, batch loss: 0.209459, train loss: 0.267321, train accuracy: 92.67%, validation loss: 0.294364, validation accuracy: 92.12%\n",
      "step: 304000, batch loss: 0.086287, train loss: 0.267113, train accuracy: 92.74%, validation loss: 0.292397, validation accuracy: 92.12%\n",
      "step: 305000, batch loss: 0.032983, train loss: 0.270566, train accuracy: 92.55%, validation loss: 0.296142, validation accuracy: 92.24%\n",
      "step: 306000, batch loss: 0.052511, train loss: 0.266904, train accuracy: 92.72%, validation loss: 0.291781, validation accuracy: 92.46%\n",
      "step: 307000, batch loss: 0.095579, train loss: 0.275252, train accuracy: 92.41%, validation loss: 0.298183, validation accuracy: 91.98%\n",
      "step: 308000, batch loss: 1.186134, train loss: 0.265347, train accuracy: 92.79%, validation loss: 0.291485, validation accuracy: 92.44%\n",
      "step: 309000, batch loss: 0.288803, train loss: 0.264473, train accuracy: 92.81%, validation loss: 0.292140, validation accuracy: 92.50%\n",
      "step: 310000, batch loss: 0.078765, train loss: 0.264896, train accuracy: 92.75%, validation loss: 0.292715, validation accuracy: 92.32%\n",
      "step: 311000, batch loss: 0.024367, train loss: 0.267350, train accuracy: 92.69%, validation loss: 0.295121, validation accuracy: 92.14%\n",
      "step: 312000, batch loss: 0.101329, train loss: 0.266428, train accuracy: 92.78%, validation loss: 0.293982, validation accuracy: 92.26%\n",
      "step: 313000, batch loss: 0.593372, train loss: 0.267838, train accuracy: 92.73%, validation loss: 0.293619, validation accuracy: 92.40%\n",
      "step: 314000, batch loss: 0.200812, train loss: 0.265593, train accuracy: 92.76%, validation loss: 0.291146, validation accuracy: 92.20%\n",
      "step: 315000, batch loss: 1.577806, train loss: 0.266296, train accuracy: 92.78%, validation loss: 0.293063, validation accuracy: 92.36%\n",
      "step: 316000, batch loss: 0.730453, train loss: 0.265784, train accuracy: 92.75%, validation loss: 0.291687, validation accuracy: 92.38%\n",
      "step: 317000, batch loss: 0.072938, train loss: 0.264721, train accuracy: 92.75%, validation loss: 0.288542, validation accuracy: 92.60%\n",
      "step: 318000, batch loss: 0.743984, train loss: 0.265871, train accuracy: 92.73%, validation loss: 0.292196, validation accuracy: 92.18%\n",
      "step: 319000, batch loss: 0.080153, train loss: 0.263514, train accuracy: 92.84%, validation loss: 0.289459, validation accuracy: 92.44%\n",
      "step: 320000, batch loss: 0.040012, train loss: 0.264082, train accuracy: 92.76%, validation loss: 0.290523, validation accuracy: 92.18%\n",
      "step: 321000, batch loss: 0.065008, train loss: 0.267171, train accuracy: 92.74%, validation loss: 0.292201, validation accuracy: 92.38%\n",
      "step: 322000, batch loss: 0.275059, train loss: 0.265946, train accuracy: 92.70%, validation loss: 0.294970, validation accuracy: 92.22%\n",
      "step: 323000, batch loss: 0.998199, train loss: 0.263193, train accuracy: 92.76%, validation loss: 0.292180, validation accuracy: 92.18%\n",
      "step: 324000, batch loss: 0.297782, train loss: 0.265786, train accuracy: 92.72%, validation loss: 0.292291, validation accuracy: 92.12%\n",
      "step: 325000, batch loss: 0.274017, train loss: 0.264266, train accuracy: 92.80%, validation loss: 0.291354, validation accuracy: 92.30%\n",
      "step: 326000, batch loss: 0.184966, train loss: 0.263313, train accuracy: 92.81%, validation loss: 0.291324, validation accuracy: 92.44%\n",
      "step: 327000, batch loss: 0.339621, train loss: 0.264906, train accuracy: 92.79%, validation loss: 0.293129, validation accuracy: 92.34%\n",
      "step: 328000, batch loss: 0.796552, train loss: 0.264753, train accuracy: 92.71%, validation loss: 0.293453, validation accuracy: 92.10%\n",
      "step: 329000, batch loss: 0.422215, train loss: 0.264540, train accuracy: 92.82%, validation loss: 0.292713, validation accuracy: 92.46%\n",
      "step: 330000, batch loss: 0.038156, train loss: 0.263838, train accuracy: 92.77%, validation loss: 0.290623, validation accuracy: 92.34%\n",
      "step: 331000, batch loss: 0.204790, train loss: 0.265238, train accuracy: 92.76%, validation loss: 0.291670, validation accuracy: 92.32%\n",
      "step: 332000, batch loss: 0.188352, train loss: 0.262192, train accuracy: 92.90%, validation loss: 0.289080, validation accuracy: 92.44%\n",
      "step: 333000, batch loss: 0.093666, train loss: 0.264557, train accuracy: 92.77%, validation loss: 0.293798, validation accuracy: 92.20%\n",
      "step: 334000, batch loss: 0.130942, train loss: 0.262463, train accuracy: 92.83%, validation loss: 0.293216, validation accuracy: 92.24%\n",
      "step: 335000, batch loss: 0.234155, train loss: 0.266053, train accuracy: 92.70%, validation loss: 0.296412, validation accuracy: 92.26%\n",
      "step: 336000, batch loss: 0.551852, train loss: 0.262690, train accuracy: 92.85%, validation loss: 0.291368, validation accuracy: 92.48%\n",
      "step: 337000, batch loss: 0.280725, train loss: 0.264552, train accuracy: 92.72%, validation loss: 0.292281, validation accuracy: 92.28%\n",
      "step: 338000, batch loss: 0.013818, train loss: 0.266664, train accuracy: 92.68%, validation loss: 0.294418, validation accuracy: 92.04%\n",
      "step: 339000, batch loss: 0.047469, train loss: 0.262294, train accuracy: 92.83%, validation loss: 0.290951, validation accuracy: 92.20%\n",
      "step: 340000, batch loss: 0.154756, train loss: 0.261386, train accuracy: 92.83%, validation loss: 0.290665, validation accuracy: 92.28%\n",
      "step: 341000, batch loss: 0.036801, train loss: 0.264082, train accuracy: 92.71%, validation loss: 0.293546, validation accuracy: 92.12%\n",
      "step: 342000, batch loss: 0.131698, train loss: 0.263431, train accuracy: 92.79%, validation loss: 0.288461, validation accuracy: 92.08%\n",
      "step: 343000, batch loss: 0.154391, train loss: 0.263048, train accuracy: 92.83%, validation loss: 0.290272, validation accuracy: 92.30%\n",
      "step: 344000, batch loss: 0.500700, train loss: 0.262647, train accuracy: 92.84%, validation loss: 0.289940, validation accuracy: 92.44%\n",
      "step: 345000, batch loss: 0.693136, train loss: 0.261368, train accuracy: 92.89%, validation loss: 0.290450, validation accuracy: 92.42%\n",
      "step: 346000, batch loss: 0.048159, train loss: 0.262197, train accuracy: 92.86%, validation loss: 0.292522, validation accuracy: 92.42%\n",
      "step: 347000, batch loss: 0.463626, train loss: 0.262418, train accuracy: 92.87%, validation loss: 0.290758, validation accuracy: 92.48%\n",
      "step: 348000, batch loss: 0.132924, train loss: 0.260384, train accuracy: 92.89%, validation loss: 0.289231, validation accuracy: 92.32%\n",
      "step: 349000, batch loss: 0.288759, train loss: 0.260574, train accuracy: 92.90%, validation loss: 0.289004, validation accuracy: 92.38%\n",
      "step: 350000, batch loss: 0.515903, train loss: 0.259616, train accuracy: 92.95%, validation loss: 0.287839, validation accuracy: 92.26%\n",
      "step: 351000, batch loss: 0.310235, train loss: 0.260087, train accuracy: 92.86%, validation loss: 0.290792, validation accuracy: 92.36%\n",
      "step: 352000, batch loss: 0.633723, train loss: 0.259835, train accuracy: 92.92%, validation loss: 0.289581, validation accuracy: 92.38%\n",
      "step: 353000, batch loss: 0.197538, train loss: 0.260396, train accuracy: 92.90%, validation loss: 0.289917, validation accuracy: 92.22%\n",
      "step: 354000, batch loss: 0.108249, train loss: 0.259811, train accuracy: 92.90%, validation loss: 0.288965, validation accuracy: 92.46%\n",
      "step: 355000, batch loss: 0.473923, train loss: 0.261249, train accuracy: 92.86%, validation loss: 0.287544, validation accuracy: 92.42%\n",
      "step: 356000, batch loss: 0.364316, train loss: 0.264330, train accuracy: 92.71%, validation loss: 0.294899, validation accuracy: 92.00%\n",
      "step: 357000, batch loss: 0.055884, train loss: 0.264031, train accuracy: 92.76%, validation loss: 0.294668, validation accuracy: 92.16%\n",
      "step: 358000, batch loss: 0.028578, train loss: 0.261213, train accuracy: 92.92%, validation loss: 0.291254, validation accuracy: 92.32%\n",
      "step: 359000, batch loss: 0.006618, train loss: 0.260467, train accuracy: 92.92%, validation loss: 0.291845, validation accuracy: 92.26%\n",
      "step: 360000, batch loss: 0.149797, train loss: 0.263742, train accuracy: 92.76%, validation loss: 0.295742, validation accuracy: 92.04%\n",
      "step: 361000, batch loss: 0.167302, train loss: 0.259555, train accuracy: 92.90%, validation loss: 0.287697, validation accuracy: 92.44%\n",
      "step: 362000, batch loss: 0.991162, train loss: 0.261298, train accuracy: 92.89%, validation loss: 0.289678, validation accuracy: 92.46%\n",
      "step: 363000, batch loss: 0.470844, train loss: 0.261142, train accuracy: 92.91%, validation loss: 0.288564, validation accuracy: 92.58%\n",
      "step: 364000, batch loss: 0.224745, train loss: 0.259672, train accuracy: 92.84%, validation loss: 0.288799, validation accuracy: 92.26%\n",
      "step: 365000, batch loss: 0.036835, train loss: 0.261057, train accuracy: 92.80%, validation loss: 0.291009, validation accuracy: 92.24%\n",
      "step: 366000, batch loss: 0.096228, train loss: 0.260880, train accuracy: 92.87%, validation loss: 0.288479, validation accuracy: 92.18%\n",
      "step: 367000, batch loss: 0.406052, train loss: 0.264006, train accuracy: 92.66%, validation loss: 0.292108, validation accuracy: 92.08%\n",
      "step: 368000, batch loss: 0.075229, train loss: 0.259129, train accuracy: 92.90%, validation loss: 0.289289, validation accuracy: 92.34%\n",
      "step: 369000, batch loss: 0.977855, train loss: 0.258302, train accuracy: 92.93%, validation loss: 0.288341, validation accuracy: 92.42%\n",
      "step: 370000, batch loss: 0.137203, train loss: 0.258988, train accuracy: 92.97%, validation loss: 0.289178, validation accuracy: 92.42%\n",
      "step: 371000, batch loss: 0.305079, train loss: 0.258853, train accuracy: 92.96%, validation loss: 0.288793, validation accuracy: 92.44%\n",
      "step: 372000, batch loss: 0.419700, train loss: 0.257629, train accuracy: 92.95%, validation loss: 0.287868, validation accuracy: 92.22%\n",
      "step: 373000, batch loss: 0.027713, train loss: 0.258332, train accuracy: 92.90%, validation loss: 0.289178, validation accuracy: 92.34%\n",
      "step: 374000, batch loss: 0.097966, train loss: 0.257221, train accuracy: 93.03%, validation loss: 0.287070, validation accuracy: 92.44%\n",
      "step: 375000, batch loss: 0.179205, train loss: 0.258599, train accuracy: 92.95%, validation loss: 0.288031, validation accuracy: 92.66%\n",
      "step: 376000, batch loss: 0.010655, train loss: 0.260955, train accuracy: 92.84%, validation loss: 0.293077, validation accuracy: 92.14%\n",
      "step: 377000, batch loss: 0.027507, train loss: 0.259764, train accuracy: 92.84%, validation loss: 0.287710, validation accuracy: 92.52%\n",
      "step: 378000, batch loss: 0.423838, train loss: 0.258323, train accuracy: 92.82%, validation loss: 0.288657, validation accuracy: 92.38%\n",
      "step: 379000, batch loss: 0.123548, train loss: 0.259974, train accuracy: 92.92%, validation loss: 0.291299, validation accuracy: 92.52%\n",
      "step: 380000, batch loss: 0.065467, train loss: 0.260119, train accuracy: 92.79%, validation loss: 0.290760, validation accuracy: 92.30%\n",
      "step: 381000, batch loss: 0.102024, train loss: 0.258712, train accuracy: 92.86%, validation loss: 0.287681, validation accuracy: 92.32%\n",
      "step: 382000, batch loss: 0.052346, train loss: 0.259120, train accuracy: 92.87%, validation loss: 0.290431, validation accuracy: 92.42%\n",
      "step: 383000, batch loss: 0.023939, train loss: 0.256221, train accuracy: 92.95%, validation loss: 0.286291, validation accuracy: 92.34%\n",
      "step: 384000, batch loss: 0.267799, train loss: 0.257467, train accuracy: 93.01%, validation loss: 0.287396, validation accuracy: 92.36%\n",
      "step: 385000, batch loss: 0.851122, train loss: 0.259436, train accuracy: 92.84%, validation loss: 0.288952, validation accuracy: 92.40%\n",
      "step: 386000, batch loss: 0.034285, train loss: 0.258183, train accuracy: 92.97%, validation loss: 0.289716, validation accuracy: 92.36%\n",
      "step: 387000, batch loss: 0.026755, train loss: 0.259681, train accuracy: 92.93%, validation loss: 0.289136, validation accuracy: 92.56%\n",
      "step: 388000, batch loss: 0.033132, train loss: 0.256477, train accuracy: 92.99%, validation loss: 0.285870, validation accuracy: 92.40%\n",
      "step: 389000, batch loss: 0.126750, train loss: 0.256049, train accuracy: 93.01%, validation loss: 0.287836, validation accuracy: 92.56%\n",
      "step: 390000, batch loss: 0.036876, train loss: 0.257983, train accuracy: 92.86%, validation loss: 0.289734, validation accuracy: 92.28%\n",
      "step: 391000, batch loss: 0.058798, train loss: 0.255887, train accuracy: 93.05%, validation loss: 0.285319, validation accuracy: 92.50%\n",
      "step: 392000, batch loss: 0.379041, train loss: 0.259677, train accuracy: 92.90%, validation loss: 0.291592, validation accuracy: 92.38%\n",
      "step: 393000, batch loss: 0.249431, train loss: 0.259762, train accuracy: 92.95%, validation loss: 0.287641, validation accuracy: 92.50%\n",
      "step: 394000, batch loss: 0.101599, train loss: 0.256163, train accuracy: 92.98%, validation loss: 0.285661, validation accuracy: 92.46%\n",
      "step: 395000, batch loss: 0.156759, train loss: 0.257660, train accuracy: 92.97%, validation loss: 0.288985, validation accuracy: 92.20%\n",
      "step: 396000, batch loss: 0.007029, train loss: 0.257107, train accuracy: 92.97%, validation loss: 0.289433, validation accuracy: 92.30%\n",
      "step: 397000, batch loss: 0.170349, train loss: 0.256823, train accuracy: 92.97%, validation loss: 0.288975, validation accuracy: 92.16%\n",
      "step: 398000, batch loss: 0.053335, train loss: 0.254749, train accuracy: 93.07%, validation loss: 0.284904, validation accuracy: 92.44%\n",
      "step: 399000, batch loss: 0.056743, train loss: 0.260728, train accuracy: 92.86%, validation loss: 0.290216, validation accuracy: 92.46%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6/8B1zNiYHS+ElEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d3fed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunLogsticRegression(\n",
    "    learning_rate=0.02,\n",
    "    steps = 400 * 1000,\n",
    "    l2_regularization_strength=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with LeRU and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RunNeuralNetwork(learning_rate = 0.01,\n",
    "                     batch_size = 16,\n",
    "                     steps = 100 * 1000,\n",
    "                     sample = 5000,\n",
    "                     dropout_keep_prob = 1.0,\n",
    "                     hidden_sizes = [100, 75, 50]):\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer = inputs\n",
    "        for hidden_size in hidden_sizes:\n",
    "            input_size = layer.get_shape()[1].value\n",
    "            weights = tf.Variable(tf.truncated_normal([input_size, hidden_size], stddev=0.1))\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[hidden_size]))\n",
    "            layer = tf.nn.relu(tf.matmul(layer, weights) + biases)\n",
    "            if dropout_keep_prob < 1.0:\n",
    "                layer = tf.nn.dropout(layer, keep_prob)\n",
    "\n",
    "        input_size = layer.get_shape()[1].value\n",
    "        weights = tf.Variable(tf.truncated_normal([input_size, num_classes], stddev=0.1))\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(layer, weights) + biases\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "        # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)    \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # This must be called after all Variable definitions.\n",
    "        init_variables = tf.initialize_all_variables()\n",
    "\n",
    "    step_records = []\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label, keep_prob: dropout_keep_prob})\n",
    "            if step % sample == 0 or step == steps - 1:\n",
    "                batch_entropy = sess.run(\n",
    "                    cross_entropy,\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                train_entropy, train_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.train.images, labels: mnist.train.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                validation_entropy, validation_accuracy = sess.run(\n",
    "                    (cross_entropy, accuracy),\n",
    "                    {inputs: mnist.validation.images, labels: mnist.validation.labels, keep_prob: 1.0}\n",
    "                )\n",
    "                print 'step: %d, batch loss: %.4f, train loss: %f, train accuracy: %.2f%%, validation loss: %.4f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if train_entropy < 0.5:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.3066, train loss: 2.308120, train accuracy: 10.55%, validation loss: 2.3066, validation accuracy: 10.64%\n",
      "step: 5000, batch loss: 0.2587, train loss: 0.274207, train accuracy: 92.09%, validation loss: 0.2587, validation accuracy: 92.80%\n",
      "step: 10000, batch loss: 0.1978, train loss: 0.203664, train accuracy: 94.11%, validation loss: 0.1978, validation accuracy: 94.40%\n",
      "step: 15000, batch loss: 0.1509, train loss: 0.149528, train accuracy: 95.72%, validation loss: 0.1509, validation accuracy: 95.84%\n",
      "step: 20000, batch loss: 0.1302, train loss: 0.119249, train accuracy: 96.56%, validation loss: 0.1302, validation accuracy: 96.50%\n",
      "step: 25000, batch loss: 0.1253, train loss: 0.106434, train accuracy: 96.85%, validation loss: 0.1253, validation accuracy: 96.72%\n",
      "step: 30000, batch loss: 0.1060, train loss: 0.082389, train accuracy: 97.65%, validation loss: 0.1060, validation accuracy: 97.28%\n",
      "step: 35000, batch loss: 0.1032, train loss: 0.072574, train accuracy: 97.92%, validation loss: 0.1032, validation accuracy: 97.34%\n",
      "step: 40000, batch loss: 0.1030, train loss: 0.065279, train accuracy: 98.20%, validation loss: 0.1030, validation accuracy: 97.24%\n",
      "step: 45000, batch loss: 0.0995, train loss: 0.056159, train accuracy: 98.41%, validation loss: 0.0995, validation accuracy: 97.38%\n",
      "step: 50000, batch loss: 0.0887, train loss: 0.044721, train accuracy: 98.74%, validation loss: 0.0887, validation accuracy: 97.60%\n",
      "step: 55000, batch loss: 0.0904, train loss: 0.040034, train accuracy: 98.89%, validation loss: 0.0904, validation accuracy: 97.44%\n",
      "step: 60000, batch loss: 0.0902, train loss: 0.034832, train accuracy: 99.05%, validation loss: 0.0902, validation accuracy: 97.52%\n",
      "step: 65000, batch loss: 0.0877, train loss: 0.028998, train accuracy: 99.21%, validation loss: 0.0877, validation accuracy: 97.64%\n",
      "step: 70000, batch loss: 0.0881, train loss: 0.026896, train accuracy: 99.30%, validation loss: 0.0881, validation accuracy: 97.68%\n",
      "step: 75000, batch loss: 0.0886, train loss: 0.022576, train accuracy: 99.45%, validation loss: 0.0886, validation accuracy: 97.72%\n",
      "step: 80000, batch loss: 0.0955, train loss: 0.020307, train accuracy: 99.53%, validation loss: 0.0955, validation accuracy: 97.54%\n",
      "step: 85000, batch loss: 0.0935, train loss: 0.017934, train accuracy: 99.58%, validation loss: 0.0935, validation accuracy: 97.56%\n",
      "step: 90000, batch loss: 0.0952, train loss: 0.020039, train accuracy: 99.42%, validation loss: 0.0952, validation accuracy: 97.40%\n",
      "step: 95000, batch loss: 0.0961, train loss: 0.011658, train accuracy: 99.78%, validation loss: 0.0961, validation accuracy: 97.74%\n",
      "step: 99999, batch loss: 0.1046, train loss: 0.015137, train accuracy: 99.58%, validation loss: 0.1046, validation accuracy: 97.54%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d6ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 2.4424, train loss: 2.445468, train accuracy: 7.24%, validation loss: 2.4424, validation accuracy: 6.62%\n",
      "step: 5000, batch loss: 0.3394, train loss: 0.352611, train accuracy: 90.11%, validation loss: 0.3394, validation accuracy: 90.66%\n",
      "step: 10000, batch loss: 0.2209, train loss: 0.232395, train accuracy: 93.10%, validation loss: 0.2209, validation accuracy: 93.46%\n",
      "step: 15000, batch loss: 0.1780, train loss: 0.183670, train accuracy: 94.62%, validation loss: 0.1780, validation accuracy: 94.98%\n",
      "step: 20000, batch loss: 0.1508, train loss: 0.152575, train accuracy: 95.44%, validation loss: 0.1508, validation accuracy: 95.86%\n",
      "step: 25000, batch loss: 0.1375, train loss: 0.133160, train accuracy: 96.03%, validation loss: 0.1375, validation accuracy: 96.20%\n",
      "step: 30000, batch loss: 0.1205, train loss: 0.114255, train accuracy: 96.61%, validation loss: 0.1205, validation accuracy: 96.60%\n",
      "step: 35000, batch loss: 0.1165, train loss: 0.101848, train accuracy: 97.02%, validation loss: 0.1165, validation accuracy: 96.76%\n",
      "step: 40000, batch loss: 0.1052, train loss: 0.089015, train accuracy: 97.40%, validation loss: 0.1052, validation accuracy: 97.04%\n",
      "step: 45000, batch loss: 0.0996, train loss: 0.082571, train accuracy: 97.58%, validation loss: 0.0996, validation accuracy: 97.32%\n",
      "step: 50000, batch loss: 0.0977, train loss: 0.074562, train accuracy: 97.78%, validation loss: 0.0977, validation accuracy: 97.40%\n",
      "step: 55000, batch loss: 0.0949, train loss: 0.070546, train accuracy: 97.90%, validation loss: 0.0949, validation accuracy: 97.40%\n",
      "step: 60000, batch loss: 0.0910, train loss: 0.063173, train accuracy: 98.14%, validation loss: 0.0910, validation accuracy: 97.54%\n",
      "step: 65000, batch loss: 0.0855, train loss: 0.057770, train accuracy: 98.31%, validation loss: 0.0855, validation accuracy: 97.70%\n",
      "step: 70000, batch loss: 0.0840, train loss: 0.054660, train accuracy: 98.38%, validation loss: 0.0840, validation accuracy: 97.68%\n",
      "step: 75000, batch loss: 0.0861, train loss: 0.050314, train accuracy: 98.49%, validation loss: 0.0861, validation accuracy: 97.60%\n",
      "step: 80000, batch loss: 0.0842, train loss: 0.047510, train accuracy: 98.59%, validation loss: 0.0842, validation accuracy: 97.72%\n",
      "step: 85000, batch loss: 0.0829, train loss: 0.044131, train accuracy: 98.71%, validation loss: 0.0829, validation accuracy: 97.56%\n",
      "step: 90000, batch loss: 0.0823, train loss: 0.043364, train accuracy: 98.71%, validation loss: 0.0823, validation accuracy: 97.66%\n",
      "step: 95000, batch loss: 0.0789, train loss: 0.039200, train accuracy: 98.85%, validation loss: 0.0789, validation accuracy: 97.78%\n",
      "step: 100000, batch loss: 0.0776, train loss: 0.037974, train accuracy: 98.89%, validation loss: 0.0776, validation accuracy: 97.94%\n",
      "step: 105000, batch loss: 0.0812, train loss: 0.036564, train accuracy: 98.91%, validation loss: 0.0812, validation accuracy: 97.76%\n",
      "step: 110000, batch loss: 0.0789, train loss: 0.034005, train accuracy: 98.99%, validation loss: 0.0789, validation accuracy: 97.72%\n",
      "step: 115000, batch loss: 0.0776, train loss: 0.031722, train accuracy: 99.05%, validation loss: 0.0776, validation accuracy: 97.86%\n",
      "step: 120000, batch loss: 0.0763, train loss: 0.030185, train accuracy: 99.09%, validation loss: 0.0763, validation accuracy: 97.92%\n",
      "step: 125000, batch loss: 0.0753, train loss: 0.028042, train accuracy: 99.19%, validation loss: 0.0753, validation accuracy: 97.82%\n",
      "step: 130000, batch loss: 0.0785, train loss: 0.027489, train accuracy: 99.21%, validation loss: 0.0785, validation accuracy: 97.82%\n",
      "step: 135000, batch loss: 0.0777, train loss: 0.026062, train accuracy: 99.24%, validation loss: 0.0777, validation accuracy: 97.80%\n",
      "step: 140000, batch loss: 0.0783, train loss: 0.024813, train accuracy: 99.27%, validation loss: 0.0783, validation accuracy: 97.88%\n",
      "step: 145000, batch loss: 0.0775, train loss: 0.023827, train accuracy: 99.30%, validation loss: 0.0775, validation accuracy: 97.98%\n",
      "step: 150000, batch loss: 0.0800, train loss: 0.022579, train accuracy: 99.36%, validation loss: 0.0800, validation accuracy: 97.92%\n",
      "step: 155000, batch loss: 0.0730, train loss: 0.020663, train accuracy: 99.40%, validation loss: 0.0730, validation accuracy: 98.02%\n",
      "step: 160000, batch loss: 0.0789, train loss: 0.020628, train accuracy: 99.41%, validation loss: 0.0789, validation accuracy: 97.82%\n",
      "step: 165000, batch loss: 0.0775, train loss: 0.020001, train accuracy: 99.40%, validation loss: 0.0775, validation accuracy: 97.94%\n",
      "step: 170000, batch loss: 0.0804, train loss: 0.018946, train accuracy: 99.45%, validation loss: 0.0804, validation accuracy: 97.84%\n",
      "step: 175000, batch loss: 0.0762, train loss: 0.017892, train accuracy: 99.47%, validation loss: 0.0762, validation accuracy: 98.00%\n",
      "step: 180000, batch loss: 0.0784, train loss: 0.017291, train accuracy: 99.50%, validation loss: 0.0784, validation accuracy: 98.06%\n",
      "step: 185000, batch loss: 0.0790, train loss: 0.016487, train accuracy: 99.52%, validation loss: 0.0790, validation accuracy: 97.94%\n",
      "step: 190000, batch loss: 0.0781, train loss: 0.015546, train accuracy: 99.55%, validation loss: 0.0781, validation accuracy: 98.04%\n",
      "step: 195000, batch loss: 0.0795, train loss: 0.015543, train accuracy: 99.54%, validation loss: 0.0795, validation accuracy: 98.08%\n",
      "step: 199999, batch loss: 0.0804, train loss: 0.014833, train accuracy: 99.56%, validation loss: 0.0804, validation accuracy: 97.94%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nISIizVJQiIhIsxQUIiLSLAWFiIg06/8Dk5hS/MEGR64AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a5fc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunNeuralNetwork(\n",
    "    dropout_keep_prob=0.5,\n",
    "    steps = 200 * 1000,\n",
    "    hidden_sizes = [200, 150, 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network\n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html)\n",
    "- [An example in tensorflow: convolutional.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/mnist/convolutional.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConvConfig = collections.namedtuple('ConvConfig', ['patch', 'channel'])\n",
    "\n",
    "default_conv_configs = [\n",
    "    ConvConfig(patch=5, channel=32),\n",
    "    ConvConfig(patch=5, channel=64),\n",
    "]\n",
    "\n",
    "image_width = 28\n",
    "assert image_width * image_width == image_size\n",
    "\n",
    "def RunConvolutionalNN(\n",
    "    batch_size = 16,\n",
    "    learning_rate = 0.01,\n",
    "    steps = 50 * 1000,\n",
    "    sample = 5000,\n",
    "    conv_configs=default_conv_configs,\n",
    "    fully_connect_sizes = [512],\n",
    "    dropout_keep_prob = 1.0,\n",
    "):\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(graph=graph)\n",
    "\n",
    "    with graph.as_default():\n",
    "        images = tf.placeholder(tf.float32, [None, image_size])\n",
    "        labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        # keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        layer = tf.reshape(images, [-1, image_width, image_width, 1], name='reshaped')\n",
    "        print 'reshaped input shape:', layer.get_shape()\n",
    "        for i in xrange(len(conv_configs)):\n",
    "            config = conv_configs[i]\n",
    "            weight_shape = [config.patch, config.patch, layer.get_shape()[-1].value, config.channel]\n",
    "            weight = tf.Variable(tf.truncated_normal(weight_shape, stddev=0.1))\n",
    "            conv = tf.nn.conv2d(layer, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            bias = tf.Variable(tf.zeros([config.channel]))\n",
    "            conv = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "\n",
    "            # Is the order of relu and max_pool important?\n",
    "            layer = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            print 'conv layer.shape (%d):' % i, layer.get_shape()\n",
    "\n",
    "        layer = tf.reshape(layer, [-1, 7 * 7 * 64])\n",
    "        for i in xrange(len(fully_connect_sizes)):\n",
    "          size = fully_connect_sizes[i]\n",
    "          weight = tf.Variable(tf.truncated_normal([layer.get_shape()[1].value, size], stddev=0.1))\n",
    "          bias = tf.Variable(tf.constant(0.1, shape=[size]))\n",
    "          layer = tf.nn.relu(tf.matmul(layer, weight) + bias)\n",
    "          print 'fully-connected.shape (%d):' % i, layer.get_shape()\n",
    "\n",
    "        weight = tf.Variable(tf.truncated_normal([layer.get_shape()[1].value, num_classes], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "        logits = tf.matmul(layer, weight) + bias\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "\n",
    "        # train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)    \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        \n",
    "        correct_count = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.int32))\n",
    "\n",
    "        init_variables = tf.initialize_all_variables()\n",
    "        \n",
    "    step_records = []\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    @contextlib.contextmanager\n",
    "    def show_graph():\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            plt.plot(step_records, train_losses)\n",
    "            plt.plot(step_records, validation_losses)\n",
    "            plt.show()\n",
    "\n",
    "    # This is important tosave RAM.\n",
    "    def evaluate_in_batches(dataset):\n",
    "        EVAL_BATCH_SIZE = 128\n",
    "        size = dataset.images.shape[0]\n",
    "        losses = []\n",
    "        correct_sum = 0\n",
    "        for start in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "            end = start + EVAL_BATCH_SIZE\n",
    "            if end > size:\n",
    "                end = size\n",
    "            loss, correct = sess.run([cross_entropy, correct_count], {images: dataset.images[start:end], labels:dataset.labels[start:end]})\n",
    "            losses.append(loss)\n",
    "            correct_sum += correct\n",
    "        return np.mean(losses), 1.0 * correct_sum / size\n",
    "\n",
    "    with show_graph(), sess.as_default():\n",
    "        init_variables.run()\n",
    "\n",
    "        for step in xrange(steps):\n",
    "            batch_images, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {images: batch_images, labels: batch_labels})\n",
    "            if step % sample == 0:\n",
    "                batch_loss = cross_entropy.eval({images: batch_images, labels: batch_labels})\n",
    "                print 'step: %d, batch loss: %f' % (step, batch_loss)\n",
    "                train_loss, train_accuracy = evaluate_in_batches(mnist.train)\n",
    "                validation_loss, validation_accuracy = evaluate_in_batches(mnist.validation)\n",
    "                print 'step: %d, train loss: %f, train accuracy: %.2f%%, validation loss: %.4f, validation accuracy: %.2f%%' % (\n",
    "                    step, train_loss, 100. * train_accuracy, validation_loss, 100. * validation_accuracy)\n",
    "                if train_loss < 0.5:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_loss)\n",
    "                    validation_losses.append(validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped input shape: (?, 28, 28, 1)\n",
      "conv layer.shape (0): (?, 14, 14, 32)\n",
      "conv layer.shape (1): (?, 7, 7, 64)\n",
      "fully-connected.shape (0): (?, 512)\n",
      "step: 0, batch loss: 3.187042\n",
      "step: 0, train loss: 3.203293, train accuracy: 10.78%, validation loss: 3.2104, validation accuracy: 11.06%\n",
      "step: 5000, batch loss: 0.014325\n",
      "step: 5000, train loss: 0.040486, train accuracy: 98.75%, validation loss: 0.0469, validation accuracy: 98.66%\n",
      "step: 10000, batch loss: 0.009346\n",
      "step: 10000, train loss: 0.026283, train accuracy: 99.20%, validation loss: 0.0407, validation accuracy: 98.90%\n",
      "step: 15000, batch loss: 0.005426\n",
      "step: 15000, train loss: 0.016398, train accuracy: 99.52%, validation loss: 0.0362, validation accuracy: 99.00%\n",
      "step: 20000, batch loss: 0.001204\n",
      "step: 20000, train loss: 0.007087, train accuracy: 99.81%, validation loss: 0.0316, validation accuracy: 99.12%\n",
      "step: 25000, batch loss: 0.066502\n",
      "step: 25000, train loss: 0.006818, train accuracy: 99.79%, validation loss: 0.0382, validation accuracy: 98.92%\n",
      "step: 30000, batch loss: 0.001059\n",
      "step: 30000, train loss: 0.002259, train accuracy: 99.95%, validation loss: 0.0297, validation accuracy: 99.30%\n",
      "step: 35000, batch loss: 0.001469\n",
      "step: 35000, train loss: 0.002943, train accuracy: 99.91%, validation loss: 0.0359, validation accuracy: 99.08%\n",
      "step: 40000, batch loss: 0.000232\n",
      "step: 40000, train loss: 0.001171, train accuracy: 99.98%, validation loss: 0.0321, validation accuracy: 99.10%\n",
      "step: 45000, batch loss: 0.000013\n",
      "step: 45000, train loss: 0.005571, train accuracy: 99.81%, validation loss: 0.0437, validation accuracy: 98.86%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njLnK/wONJRszzwFyvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110ef3cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunConvolutionalNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
