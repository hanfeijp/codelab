{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('mnist', one_hot=True)\n",
    "image_size = 28 * 28\n",
    "num_classes = 10\n",
    "assert mnist.train.images.shape[1] == image_size\n",
    "assert mnist.train.labels.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, batch loss: 10.315619, train loss: 12.232466, train accuracy: 7.45%, validation loss: 12.376769, validation accuracy: 7.48%\n",
      "step: 1000, batch loss: 0.398619, train loss: 1.324386, train accuracy: 72.95%, validation loss: 1.281298, validation accuracy: 75.60%\n",
      "step: 2000, batch loss: 1.028669, train loss: 0.934611, train accuracy: 79.87%, validation loss: 0.914769, validation accuracy: 81.32%\n",
      "step: 3000, batch loss: 0.586172, train loss: 0.786268, train accuracy: 82.79%, validation loss: 0.768607, validation accuracy: 83.70%\n",
      "step: 4000, batch loss: 0.380030, train loss: 0.717456, train accuracy: 84.01%, validation loss: 0.709984, validation accuracy: 84.84%\n",
      "step: 5000, batch loss: 0.585375, train loss: 0.648079, train accuracy: 85.17%, validation loss: 0.632494, validation accuracy: 86.16%\n",
      "step: 6000, batch loss: 0.365921, train loss: 0.612381, train accuracy: 85.69%, validation loss: 0.603611, validation accuracy: 86.22%\n",
      "step: 7000, batch loss: 0.105980, train loss: 0.574791, train accuracy: 86.42%, validation loss: 0.571843, validation accuracy: 86.98%\n",
      "step: 8000, batch loss: 0.021492, train loss: 0.541212, train accuracy: 87.09%, validation loss: 0.534764, validation accuracy: 88.06%\n",
      "step: 9000, batch loss: 0.266055, train loss: 0.526106, train accuracy: 87.40%, validation loss: 0.530966, validation accuracy: 87.82%\n",
      "step: 10000, batch loss: 0.040239, train loss: 0.500304, train accuracy: 87.73%, validation loss: 0.493277, validation accuracy: 88.60%\n",
      "step: 11000, batch loss: 0.031686, train loss: 0.487383, train accuracy: 87.89%, validation loss: 0.489008, validation accuracy: 88.54%\n",
      "step: 12000, batch loss: 0.851616, train loss: 0.467428, train accuracy: 88.41%, validation loss: 0.468748, validation accuracy: 88.78%\n",
      "step: 13000, batch loss: 0.083847, train loss: 0.455736, train accuracy: 88.77%, validation loss: 0.461711, validation accuracy: 89.12%\n",
      "step: 14000, batch loss: 0.679243, train loss: 0.454929, train accuracy: 88.61%, validation loss: 0.454442, validation accuracy: 89.20%\n",
      "step: 15000, batch loss: 0.438960, train loss: 0.450466, train accuracy: 88.61%, validation loss: 0.449916, validation accuracy: 89.38%\n",
      "step: 16000, batch loss: 0.629555, train loss: 0.441383, train accuracy: 88.78%, validation loss: 0.444363, validation accuracy: 89.30%\n",
      "step: 17000, batch loss: 0.757836, train loss: 0.423531, train accuracy: 89.23%, validation loss: 0.426791, validation accuracy: 89.94%\n",
      "step: 18000, batch loss: 0.039433, train loss: 0.420775, train accuracy: 89.22%, validation loss: 0.427668, validation accuracy: 89.94%\n",
      "step: 19000, batch loss: 0.024773, train loss: 0.412281, train accuracy: 89.48%, validation loss: 0.420154, validation accuracy: 89.86%\n",
      "step: 20000, batch loss: 0.199742, train loss: 0.403492, train accuracy: 89.62%, validation loss: 0.410961, validation accuracy: 89.86%\n",
      "step: 21000, batch loss: 0.060561, train loss: 0.399942, train accuracy: 89.72%, validation loss: 0.403939, validation accuracy: 90.12%\n",
      "step: 22000, batch loss: 0.078289, train loss: 0.394502, train accuracy: 89.84%, validation loss: 0.408670, validation accuracy: 90.02%\n",
      "step: 23000, batch loss: 0.181766, train loss: 0.393653, train accuracy: 89.85%, validation loss: 0.398837, validation accuracy: 90.12%\n",
      "step: 24000, batch loss: 0.065694, train loss: 0.383298, train accuracy: 89.96%, validation loss: 0.394136, validation accuracy: 90.20%\n",
      "step: 25000, batch loss: 0.091653, train loss: 0.376877, train accuracy: 90.29%, validation loss: 0.386286, validation accuracy: 90.44%\n",
      "step: 26000, batch loss: 0.113835, train loss: 0.372010, train accuracy: 90.30%, validation loss: 0.379337, validation accuracy: 90.56%\n",
      "step: 27000, batch loss: 0.314741, train loss: 0.372608, train accuracy: 90.31%, validation loss: 0.384500, validation accuracy: 90.68%\n",
      "step: 28000, batch loss: 0.585565, train loss: 0.365840, train accuracy: 90.41%, validation loss: 0.380017, validation accuracy: 90.62%\n",
      "step: 29000, batch loss: 0.900997, train loss: 0.367852, train accuracy: 90.27%, validation loss: 0.376446, validation accuracy: 90.32%\n",
      "step: 30000, batch loss: 0.021327, train loss: 0.360908, train accuracy: 90.53%, validation loss: 0.374891, validation accuracy: 90.46%\n",
      "step: 31000, batch loss: 0.083092, train loss: 0.364192, train accuracy: 90.45%, validation loss: 0.374616, validation accuracy: 90.40%\n",
      "step: 32000, batch loss: 0.014458, train loss: 0.354326, train accuracy: 90.73%, validation loss: 0.368173, validation accuracy: 90.60%\n",
      "step: 33000, batch loss: 0.064707, train loss: 0.360313, train accuracy: 90.42%, validation loss: 0.372248, validation accuracy: 90.32%\n",
      "step: 34000, batch loss: 0.445821, train loss: 0.351900, train accuracy: 90.64%, validation loss: 0.363975, validation accuracy: 90.60%\n",
      "step: 35000, batch loss: 0.058556, train loss: 0.347554, train accuracy: 90.84%, validation loss: 0.356757, validation accuracy: 90.76%\n",
      "step: 36000, batch loss: 0.018404, train loss: 0.346086, train accuracy: 90.83%, validation loss: 0.359746, validation accuracy: 90.82%\n",
      "step: 37000, batch loss: 0.938500, train loss: 0.352822, train accuracy: 90.59%, validation loss: 0.366668, validation accuracy: 90.56%\n",
      "step: 38000, batch loss: 0.126090, train loss: 0.358092, train accuracy: 90.31%, validation loss: 0.374566, validation accuracy: 90.42%\n",
      "step: 39000, batch loss: 0.736637, train loss: 0.339287, train accuracy: 91.00%, validation loss: 0.356162, validation accuracy: 90.72%\n",
      "step: 40000, batch loss: 0.163276, train loss: 0.344386, train accuracy: 90.78%, validation loss: 0.361590, validation accuracy: 90.36%\n",
      "step: 41000, batch loss: 0.030620, train loss: 0.335817, train accuracy: 91.07%, validation loss: 0.352193, validation accuracy: 90.74%\n",
      "step: 42000, batch loss: 0.820319, train loss: 0.334679, train accuracy: 91.12%, validation loss: 0.353897, validation accuracy: 91.00%\n",
      "step: 43000, batch loss: 0.024085, train loss: 0.337608, train accuracy: 91.00%, validation loss: 0.353283, validation accuracy: 91.12%\n",
      "step: 44000, batch loss: 0.015755, train loss: 0.344269, train accuracy: 90.78%, validation loss: 0.361066, validation accuracy: 90.78%\n",
      "step: 45000, batch loss: 0.019731, train loss: 0.329092, train accuracy: 91.19%, validation loss: 0.343184, validation accuracy: 90.76%\n",
      "step: 46000, batch loss: 0.116836, train loss: 0.330886, train accuracy: 91.21%, validation loss: 0.353154, validation accuracy: 91.08%\n",
      "step: 47000, batch loss: 0.012289, train loss: 0.322515, train accuracy: 91.36%, validation loss: 0.339812, validation accuracy: 90.86%\n",
      "step: 48000, batch loss: 0.231323, train loss: 0.331104, train accuracy: 91.03%, validation loss: 0.347890, validation accuracy: 91.00%\n",
      "step: 49000, batch loss: 0.294248, train loss: 0.329522, train accuracy: 91.23%, validation loss: 0.351384, validation accuracy: 91.16%\n",
      "step: 50000, batch loss: 0.335925, train loss: 0.321571, train accuracy: 91.40%, validation loss: 0.339528, validation accuracy: 91.34%\n",
      "step: 51000, batch loss: 0.119039, train loss: 0.326977, train accuracy: 91.14%, validation loss: 0.342996, validation accuracy: 90.94%\n",
      "step: 52000, batch loss: 0.644314, train loss: 0.322040, train accuracy: 91.34%, validation loss: 0.339244, validation accuracy: 91.30%\n",
      "step: 53000, batch loss: 0.455682, train loss: 0.313437, train accuracy: 91.56%, validation loss: 0.332820, validation accuracy: 91.16%\n",
      "step: 54000, batch loss: 0.067731, train loss: 0.321980, train accuracy: 91.37%, validation loss: 0.343548, validation accuracy: 91.10%\n",
      "step: 55000, batch loss: 0.144732, train loss: 0.315274, train accuracy: 91.49%, validation loss: 0.336260, validation accuracy: 91.16%\n",
      "step: 56000, batch loss: 0.008469, train loss: 0.316499, train accuracy: 91.35%, validation loss: 0.341148, validation accuracy: 90.94%\n",
      "step: 57000, batch loss: 0.145192, train loss: 0.310496, train accuracy: 91.61%, validation loss: 0.332166, validation accuracy: 91.34%\n",
      "step: 58000, batch loss: 0.131688, train loss: 0.308106, train accuracy: 91.65%, validation loss: 0.326610, validation accuracy: 91.34%\n",
      "step: 59000, batch loss: 0.807177, train loss: 0.319748, train accuracy: 91.23%, validation loss: 0.335632, validation accuracy: 91.28%\n",
      "step: 60000, batch loss: 0.257427, train loss: 0.309503, train accuracy: 91.60%, validation loss: 0.333399, validation accuracy: 90.84%\n",
      "step: 61000, batch loss: 0.052815, train loss: 0.310886, train accuracy: 91.53%, validation loss: 0.329494, validation accuracy: 91.12%\n",
      "step: 62000, batch loss: 0.129870, train loss: 0.309939, train accuracy: 91.51%, validation loss: 0.334447, validation accuracy: 91.24%\n",
      "step: 63000, batch loss: 0.089658, train loss: 0.313148, train accuracy: 91.43%, validation loss: 0.335249, validation accuracy: 91.36%\n",
      "step: 64000, batch loss: 0.057120, train loss: 0.311807, train accuracy: 91.61%, validation loss: 0.332262, validation accuracy: 91.22%\n",
      "step: 65000, batch loss: 0.201349, train loss: 0.303935, train accuracy: 91.84%, validation loss: 0.328245, validation accuracy: 91.36%\n",
      "step: 66000, batch loss: 0.157631, train loss: 0.309198, train accuracy: 91.54%, validation loss: 0.332361, validation accuracy: 91.36%\n",
      "step: 67000, batch loss: 0.040992, train loss: 0.310020, train accuracy: 91.64%, validation loss: 0.332014, validation accuracy: 91.52%\n",
      "step: 68000, batch loss: 0.094685, train loss: 0.303840, train accuracy: 91.70%, validation loss: 0.329642, validation accuracy: 91.14%\n",
      "step: 69000, batch loss: 0.026753, train loss: 0.301182, train accuracy: 91.80%, validation loss: 0.326286, validation accuracy: 91.38%\n",
      "step: 70000, batch loss: 0.026239, train loss: 0.305985, train accuracy: 91.58%, validation loss: 0.326127, validation accuracy: 91.38%\n",
      "step: 71000, batch loss: 0.389758, train loss: 0.303644, train accuracy: 91.59%, validation loss: 0.327004, validation accuracy: 91.30%\n",
      "step: 72000, batch loss: 0.252058, train loss: 0.303734, train accuracy: 91.74%, validation loss: 0.328686, validation accuracy: 91.28%\n",
      "step: 73000, batch loss: 0.399573, train loss: 0.309377, train accuracy: 91.46%, validation loss: 0.334662, validation accuracy: 91.16%\n",
      "step: 74000, batch loss: 0.354951, train loss: 0.293839, train accuracy: 91.94%, validation loss: 0.320689, validation accuracy: 91.50%\n",
      "step: 75000, batch loss: 0.080668, train loss: 0.293863, train accuracy: 91.95%, validation loss: 0.318444, validation accuracy: 91.40%\n",
      "step: 76000, batch loss: 0.164915, train loss: 0.293560, train accuracy: 91.93%, validation loss: 0.320498, validation accuracy: 91.30%\n",
      "step: 77000, batch loss: 0.421105, train loss: 0.294414, train accuracy: 92.00%, validation loss: 0.321238, validation accuracy: 91.42%\n",
      "step: 78000, batch loss: 1.014642, train loss: 0.294354, train accuracy: 91.92%, validation loss: 0.319312, validation accuracy: 91.44%\n",
      "step: 79000, batch loss: 0.059581, train loss: 0.290100, train accuracy: 92.13%, validation loss: 0.313541, validation accuracy: 91.70%\n",
      "step: 80000, batch loss: 0.294088, train loss: 0.290225, train accuracy: 92.00%, validation loss: 0.316710, validation accuracy: 91.70%\n",
      "step: 81000, batch loss: 0.240907, train loss: 0.289326, train accuracy: 92.13%, validation loss: 0.315068, validation accuracy: 91.72%\n",
      "step: 82000, batch loss: 0.173685, train loss: 0.295967, train accuracy: 91.83%, validation loss: 0.323684, validation accuracy: 91.22%\n",
      "step: 83000, batch loss: 0.074906, train loss: 0.288582, train accuracy: 92.06%, validation loss: 0.311751, validation accuracy: 91.70%\n",
      "step: 84000, batch loss: 0.211877, train loss: 0.290608, train accuracy: 92.08%, validation loss: 0.314540, validation accuracy: 91.16%\n",
      "step: 85000, batch loss: 0.015019, train loss: 0.293405, train accuracy: 91.79%, validation loss: 0.315390, validation accuracy: 91.38%\n",
      "step: 86000, batch loss: 0.056876, train loss: 0.291379, train accuracy: 91.94%, validation loss: 0.315550, validation accuracy: 91.34%\n",
      "step: 87000, batch loss: 0.012988, train loss: 0.285569, train accuracy: 92.26%, validation loss: 0.310953, validation accuracy: 91.64%\n",
      "step: 88000, batch loss: 0.031024, train loss: 0.286947, train accuracy: 92.14%, validation loss: 0.316194, validation accuracy: 91.44%\n",
      "step: 89000, batch loss: 0.142300, train loss: 0.291048, train accuracy: 91.92%, validation loss: 0.315194, validation accuracy: 91.24%\n",
      "step: 90000, batch loss: 0.708775, train loss: 0.289431, train accuracy: 92.03%, validation loss: 0.316270, validation accuracy: 91.52%\n",
      "step: 91000, batch loss: 0.520591, train loss: 0.285160, train accuracy: 92.21%, validation loss: 0.314751, validation accuracy: 91.58%\n",
      "step: 92000, batch loss: 0.091002, train loss: 0.282595, train accuracy: 92.19%, validation loss: 0.310439, validation accuracy: 91.58%\n",
      "step: 93000, batch loss: 0.062687, train loss: 0.282760, train accuracy: 92.22%, validation loss: 0.310259, validation accuracy: 91.74%\n",
      "step: 94000, batch loss: 0.368998, train loss: 0.286270, train accuracy: 92.11%, validation loss: 0.312926, validation accuracy: 91.40%\n",
      "step: 95000, batch loss: 0.102775, train loss: 0.280703, train accuracy: 92.34%, validation loss: 0.309616, validation accuracy: 91.62%\n",
      "step: 96000, batch loss: 0.145864, train loss: 0.284974, train accuracy: 92.15%, validation loss: 0.307097, validation accuracy: 91.88%\n",
      "step: 97000, batch loss: 0.119787, train loss: 0.290412, train accuracy: 92.03%, validation loss: 0.317476, validation accuracy: 91.38%\n",
      "step: 98000, batch loss: 0.029483, train loss: 0.283620, train accuracy: 92.33%, validation loss: 0.311632, validation accuracy: 91.64%\n",
      "step: 99000, batch loss: 0.042419, train loss: 0.294015, train accuracy: 91.91%, validation loss: 0.321598, validation accuracy: 91.10%\n",
      "step: 100000, batch loss: 0.154891, train loss: 0.278319, train accuracy: 92.43%, validation loss: 0.304752, validation accuracy: 91.86%\n",
      "step: 101000, batch loss: 1.473445, train loss: 0.282076, train accuracy: 92.28%, validation loss: 0.308323, validation accuracy: 91.62%\n",
      "step: 102000, batch loss: 0.266631, train loss: 0.279586, train accuracy: 92.29%, validation loss: 0.306856, validation accuracy: 91.54%\n",
      "step: 103000, batch loss: 0.115208, train loss: 0.290344, train accuracy: 91.86%, validation loss: 0.321579, validation accuracy: 91.02%\n",
      "step: 104000, batch loss: 0.128595, train loss: 0.277867, train accuracy: 92.49%, validation loss: 0.308982, validation accuracy: 91.74%\n",
      "step: 105000, batch loss: 0.034878, train loss: 0.276839, train accuracy: 92.49%, validation loss: 0.305535, validation accuracy: 91.84%\n",
      "step: 106000, batch loss: 0.063157, train loss: 0.282361, train accuracy: 92.21%, validation loss: 0.310766, validation accuracy: 91.78%\n",
      "step: 107000, batch loss: 0.012763, train loss: 0.280948, train accuracy: 92.33%, validation loss: 0.309549, validation accuracy: 91.84%\n",
      "step: 108000, batch loss: 0.152843, train loss: 0.278432, train accuracy: 92.30%, validation loss: 0.308167, validation accuracy: 91.52%\n",
      "step: 109000, batch loss: 0.411255, train loss: 0.278399, train accuracy: 92.39%, validation loss: 0.310252, validation accuracy: 91.88%\n",
      "step: 110000, batch loss: 0.323467, train loss: 0.281907, train accuracy: 92.15%, validation loss: 0.315378, validation accuracy: 91.50%\n",
      "step: 111000, batch loss: 0.771127, train loss: 0.274919, train accuracy: 92.51%, validation loss: 0.304817, validation accuracy: 91.68%\n",
      "step: 112000, batch loss: 0.166416, train loss: 0.278342, train accuracy: 92.43%, validation loss: 0.307770, validation accuracy: 91.78%\n",
      "step: 113000, batch loss: 0.201026, train loss: 0.275448, train accuracy: 92.46%, validation loss: 0.302862, validation accuracy: 91.92%\n",
      "step: 114000, batch loss: 0.418198, train loss: 0.272077, train accuracy: 92.62%, validation loss: 0.303925, validation accuracy: 91.62%\n",
      "step: 115000, batch loss: 0.102948, train loss: 0.281007, train accuracy: 92.25%, validation loss: 0.309113, validation accuracy: 91.54%\n",
      "step: 116000, batch loss: 0.858824, train loss: 0.278422, train accuracy: 92.37%, validation loss: 0.313366, validation accuracy: 91.46%\n",
      "step: 117000, batch loss: 0.997578, train loss: 0.277635, train accuracy: 92.36%, validation loss: 0.306252, validation accuracy: 91.82%\n",
      "step: 118000, batch loss: 0.033906, train loss: 0.272644, train accuracy: 92.51%, validation loss: 0.304186, validation accuracy: 91.68%\n",
      "step: 119000, batch loss: 0.095903, train loss: 0.278594, train accuracy: 92.33%, validation loss: 0.311404, validation accuracy: 91.62%\n",
      "step: 120000, batch loss: 0.135420, train loss: 0.272888, train accuracy: 92.32%, validation loss: 0.300703, validation accuracy: 91.94%\n",
      "step: 121000, batch loss: 0.157932, train loss: 0.275105, train accuracy: 92.46%, validation loss: 0.305126, validation accuracy: 91.96%\n",
      "step: 122000, batch loss: 0.264308, train loss: 0.274971, train accuracy: 92.35%, validation loss: 0.304920, validation accuracy: 91.84%\n",
      "step: 123000, batch loss: 0.041283, train loss: 0.267615, train accuracy: 92.66%, validation loss: 0.297783, validation accuracy: 91.92%\n",
      "step: 124000, batch loss: 1.364383, train loss: 0.273630, train accuracy: 92.46%, validation loss: 0.301744, validation accuracy: 92.04%\n",
      "step: 125000, batch loss: 0.127066, train loss: 0.268478, train accuracy: 92.67%, validation loss: 0.301698, validation accuracy: 91.70%\n",
      "step: 126000, batch loss: 0.081494, train loss: 0.269371, train accuracy: 92.58%, validation loss: 0.301134, validation accuracy: 91.94%\n",
      "step: 127000, batch loss: 2.107178, train loss: 0.271594, train accuracy: 92.48%, validation loss: 0.301995, validation accuracy: 91.76%\n",
      "step: 128000, batch loss: 0.011665, train loss: 0.271417, train accuracy: 92.44%, validation loss: 0.301067, validation accuracy: 91.88%\n",
      "step: 129000, batch loss: 0.299739, train loss: 0.272238, train accuracy: 92.48%, validation loss: 0.304182, validation accuracy: 91.78%\n",
      "step: 130000, batch loss: 0.007840, train loss: 0.268742, train accuracy: 92.58%, validation loss: 0.302519, validation accuracy: 91.80%\n",
      "step: 131000, batch loss: 0.070522, train loss: 0.274955, train accuracy: 92.42%, validation loss: 0.302927, validation accuracy: 91.66%\n",
      "step: 132000, batch loss: 0.199234, train loss: 0.272961, train accuracy: 92.56%, validation loss: 0.306948, validation accuracy: 91.76%\n",
      "step: 133000, batch loss: 0.051950, train loss: 0.271993, train accuracy: 92.58%, validation loss: 0.306083, validation accuracy: 91.74%\n",
      "step: 134000, batch loss: 0.023755, train loss: 0.268802, train accuracy: 92.52%, validation loss: 0.298403, validation accuracy: 91.90%\n",
      "step: 135000, batch loss: 0.039369, train loss: 0.266698, train accuracy: 92.72%, validation loss: 0.299181, validation accuracy: 91.94%\n",
      "step: 136000, batch loss: 0.017914, train loss: 0.265316, train accuracy: 92.81%, validation loss: 0.298259, validation accuracy: 91.92%\n",
      "step: 137000, batch loss: 0.638034, train loss: 0.267351, train accuracy: 92.70%, validation loss: 0.299309, validation accuracy: 91.58%\n",
      "step: 138000, batch loss: 0.102812, train loss: 0.268306, train accuracy: 92.60%, validation loss: 0.303006, validation accuracy: 92.02%\n",
      "step: 139000, batch loss: 0.068522, train loss: 0.262838, train accuracy: 92.73%, validation loss: 0.298640, validation accuracy: 91.84%\n",
      "step: 140000, batch loss: 0.012137, train loss: 0.265106, train accuracy: 92.80%, validation loss: 0.300656, validation accuracy: 92.02%\n",
      "step: 141000, batch loss: 0.092531, train loss: 0.266807, train accuracy: 92.68%, validation loss: 0.302291, validation accuracy: 91.60%\n",
      "step: 142000, batch loss: 0.137061, train loss: 0.263636, train accuracy: 92.79%, validation loss: 0.295497, validation accuracy: 92.02%\n",
      "step: 143000, batch loss: 3.337349, train loss: 0.266896, train accuracy: 92.68%, validation loss: 0.304026, validation accuracy: 91.70%\n",
      "step: 144000, batch loss: 0.053331, train loss: 0.265718, train accuracy: 92.65%, validation loss: 0.297043, validation accuracy: 91.82%\n",
      "step: 145000, batch loss: 0.018920, train loss: 0.274662, train accuracy: 92.43%, validation loss: 0.301867, validation accuracy: 92.12%\n",
      "step: 146000, batch loss: 0.675669, train loss: 0.267708, train accuracy: 92.59%, validation loss: 0.298944, validation accuracy: 91.72%\n",
      "step: 147000, batch loss: 0.010728, train loss: 0.269498, train accuracy: 92.40%, validation loss: 0.302462, validation accuracy: 91.74%\n",
      "step: 148000, batch loss: 2.198852, train loss: 0.262675, train accuracy: 92.85%, validation loss: 0.297946, validation accuracy: 91.92%\n",
      "step: 149000, batch loss: 0.071812, train loss: 0.268400, train accuracy: 92.58%, validation loss: 0.302635, validation accuracy: 91.66%\n",
      "step: 150000, batch loss: 0.247511, train loss: 0.266837, train accuracy: 92.53%, validation loss: 0.299996, validation accuracy: 91.66%\n",
      "step: 151000, batch loss: 0.050465, train loss: 0.266018, train accuracy: 92.70%, validation loss: 0.297019, validation accuracy: 91.98%\n",
      "step: 152000, batch loss: 0.577861, train loss: 0.269549, train accuracy: 92.48%, validation loss: 0.304057, validation accuracy: 91.44%\n",
      "step: 153000, batch loss: 0.112059, train loss: 0.264156, train accuracy: 92.68%, validation loss: 0.302948, validation accuracy: 91.60%\n",
      "step: 154000, batch loss: 0.401760, train loss: 0.263472, train accuracy: 92.77%, validation loss: 0.298320, validation accuracy: 91.96%\n",
      "step: 155000, batch loss: 0.319741, train loss: 0.260805, train accuracy: 92.87%, validation loss: 0.295304, validation accuracy: 92.04%\n",
      "step: 156000, batch loss: 0.592103, train loss: 0.272096, train accuracy: 92.49%, validation loss: 0.305210, validation accuracy: 91.80%\n",
      "step: 157000, batch loss: 0.042288, train loss: 0.268085, train accuracy: 92.65%, validation loss: 0.302315, validation accuracy: 91.68%\n",
      "step: 158000, batch loss: 0.007190, train loss: 0.274369, train accuracy: 92.40%, validation loss: 0.304894, validation accuracy: 91.64%\n",
      "step: 159000, batch loss: 1.363637, train loss: 0.263256, train accuracy: 92.73%, validation loss: 0.297715, validation accuracy: 91.92%\n",
      "step: 160000, batch loss: 0.239029, train loss: 0.261451, train accuracy: 92.82%, validation loss: 0.295352, validation accuracy: 92.06%\n",
      "step: 161000, batch loss: 0.027503, train loss: 0.265265, train accuracy: 92.60%, validation loss: 0.299294, validation accuracy: 91.90%\n",
      "step: 162000, batch loss: 0.047385, train loss: 0.257425, train accuracy: 92.89%, validation loss: 0.294711, validation accuracy: 92.16%\n",
      "step: 163000, batch loss: 0.084723, train loss: 0.264862, train accuracy: 92.71%, validation loss: 0.301394, validation accuracy: 91.82%\n",
      "step: 164000, batch loss: 0.024742, train loss: 0.261279, train accuracy: 92.83%, validation loss: 0.293521, validation accuracy: 92.14%\n",
      "step: 165000, batch loss: 0.005050, train loss: 0.261298, train accuracy: 92.82%, validation loss: 0.297520, validation accuracy: 92.02%\n",
      "step: 166000, batch loss: 0.137626, train loss: 0.265116, train accuracy: 92.57%, validation loss: 0.300967, validation accuracy: 91.70%\n",
      "step: 167000, batch loss: 0.835616, train loss: 0.260978, train accuracy: 92.90%, validation loss: 0.294354, validation accuracy: 91.80%\n",
      "step: 168000, batch loss: 0.806057, train loss: 0.258331, train accuracy: 92.92%, validation loss: 0.296012, validation accuracy: 91.90%\n",
      "step: 169000, batch loss: 0.121903, train loss: 0.264681, train accuracy: 92.70%, validation loss: 0.300160, validation accuracy: 91.68%\n",
      "step: 170000, batch loss: 1.050885, train loss: 0.257528, train accuracy: 92.82%, validation loss: 0.294912, validation accuracy: 92.06%\n",
      "step: 171000, batch loss: 0.223307, train loss: 0.262083, train accuracy: 92.86%, validation loss: 0.297501, validation accuracy: 91.94%\n",
      "step: 172000, batch loss: 1.049877, train loss: 0.265369, train accuracy: 92.65%, validation loss: 0.298509, validation accuracy: 91.74%\n",
      "step: 173000, batch loss: 0.034483, train loss: 0.260553, train accuracy: 92.71%, validation loss: 0.292362, validation accuracy: 92.08%\n",
      "step: 174000, batch loss: 0.028558, train loss: 0.260700, train accuracy: 92.82%, validation loss: 0.295125, validation accuracy: 91.92%\n",
      "step: 175000, batch loss: 0.048844, train loss: 0.260886, train accuracy: 92.74%, validation loss: 0.296233, validation accuracy: 91.98%\n",
      "step: 176000, batch loss: 0.116613, train loss: 0.259756, train accuracy: 92.87%, validation loss: 0.297309, validation accuracy: 91.86%\n",
      "step: 177000, batch loss: 0.031406, train loss: 0.265399, train accuracy: 92.77%, validation loss: 0.306986, validation accuracy: 91.64%\n",
      "step: 178000, batch loss: 0.107554, train loss: 0.254960, train accuracy: 92.99%, validation loss: 0.291831, validation accuracy: 92.14%\n",
      "step: 179000, batch loss: 0.319670, train loss: 0.268172, train accuracy: 92.50%, validation loss: 0.302068, validation accuracy: 92.08%\n",
      "step: 180000, batch loss: 0.522155, train loss: 0.259928, train accuracy: 92.89%, validation loss: 0.297604, validation accuracy: 91.94%\n",
      "step: 181000, batch loss: 0.219882, train loss: 0.261111, train accuracy: 92.78%, validation loss: 0.301875, validation accuracy: 92.00%\n",
      "step: 182000, batch loss: 0.107440, train loss: 0.261074, train accuracy: 92.88%, validation loss: 0.301609, validation accuracy: 91.54%\n",
      "step: 183000, batch loss: 0.078157, train loss: 0.257194, train accuracy: 92.90%, validation loss: 0.294967, validation accuracy: 92.10%\n",
      "step: 184000, batch loss: 0.031616, train loss: 0.256870, train accuracy: 92.85%, validation loss: 0.297101, validation accuracy: 91.80%\n",
      "step: 185000, batch loss: 0.230460, train loss: 0.260868, train accuracy: 92.78%, validation loss: 0.294750, validation accuracy: 92.04%\n",
      "step: 186000, batch loss: 0.173556, train loss: 0.263223, train accuracy: 92.61%, validation loss: 0.298120, validation accuracy: 91.94%\n",
      "step: 187000, batch loss: 0.172437, train loss: 0.261558, train accuracy: 92.75%, validation loss: 0.296645, validation accuracy: 91.94%\n",
      "step: 188000, batch loss: 0.035768, train loss: 0.257897, train accuracy: 92.84%, validation loss: 0.294664, validation accuracy: 91.94%\n",
      "step: 189000, batch loss: 0.091194, train loss: 0.260211, train accuracy: 92.90%, validation loss: 0.294617, validation accuracy: 91.84%\n",
      "step: 190000, batch loss: 0.062037, train loss: 0.256057, train accuracy: 92.89%, validation loss: 0.294695, validation accuracy: 91.78%\n",
      "step: 191000, batch loss: 0.019448, train loss: 0.272812, train accuracy: 92.09%, validation loss: 0.309043, validation accuracy: 91.46%\n",
      "step: 192000, batch loss: 1.353569, train loss: 0.257454, train accuracy: 92.87%, validation loss: 0.296147, validation accuracy: 92.02%\n",
      "step: 193000, batch loss: 0.022200, train loss: 0.261067, train accuracy: 92.79%, validation loss: 0.302511, validation accuracy: 91.94%\n",
      "step: 194000, batch loss: 0.003980, train loss: 0.260743, train accuracy: 92.72%, validation loss: 0.298229, validation accuracy: 91.70%\n",
      "step: 195000, batch loss: 1.497911, train loss: 0.255675, train accuracy: 93.08%, validation loss: 0.295927, validation accuracy: 91.90%\n",
      "step: 196000, batch loss: 0.094750, train loss: 0.253419, train accuracy: 92.95%, validation loss: 0.290322, validation accuracy: 92.30%\n",
      "step: 197000, batch loss: 0.033300, train loss: 0.256088, train accuracy: 92.97%, validation loss: 0.289407, validation accuracy: 92.12%\n",
      "step: 198000, batch loss: 0.320902, train loss: 0.255128, train accuracy: 93.02%, validation loss: 0.292365, validation accuracy: 91.88%\n",
      "step: 199000, batch loss: 0.326912, train loss: 0.255490, train accuracy: 92.93%, validation loss: 0.295906, validation accuracy: 91.98%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/Users/yunabe/local/homebrew/lib/python2.7/site-packages/matplotlib/pyplot.pyc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "steps = 200 * 1000\n",
    "sample = 1000\n",
    "batch_size = 8\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, [None, image_size])\n",
    "    labels = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([image_size, num_classes], stddev=1), name='a')\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='bias')\n",
    "    logits = tf.matmul(inputs, weight) + bias\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels))\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    init_variables = tf.initialize_all_variables()\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    step_records = []\n",
    "    with sess.as_default():\n",
    "        init_variables.run()\n",
    "        for step in xrange(steps):\n",
    "            batch_input, batch_label = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, {inputs: batch_input, labels: batch_label})\n",
    "            if step % sample == 0:\n",
    "                batch_entropy, batch_accuracy = sess.run((cross_entropy, accuracy), {inputs: batch_input, labels: batch_label})\n",
    "                train_entropy, train_accuracy = sess.run((cross_entropy, accuracy), {inputs: mnist.train.images, labels: mnist.train.labels})\n",
    "                validation_entropy, validation_accuracy = sess.run((cross_entropy, accuracy), {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "                print 'step: %d, batch loss: %f, train loss: %f, train accuracy: %.2f%%, validation loss: %f, validation accuracy: %.2f%%' % (\n",
    "                    step, batch_entropy, train_entropy, 100. * train_accuracy, validation_entropy, 100. * validation_accuracy)\n",
    "                if step > 0:\n",
    "                    step_records.append(step)\n",
    "                    train_losses.append(train_entropy)\n",
    "                    validation_losses.append(validation_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wczYAI5Hqne8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154e2b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(step_records, train_losses)\n",
    "plt.plot(step_records, validation_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
